# -*- coding: utf-8 -*-
"""RAG basics.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Z12ut9aBUNCZllzlFd68ZryQ6fM2f9zZ
"""

from glob import glob
from langchain_community.document_loaders import PyPDFLoader

# Import Libraries
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_groq import ChatGroq
from langchain.vectorstores import FAISS  # <-- added import

from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.document_loaders import TextLoader
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate

import getpass
import os
import sys

# old:
# os.environ["GROQ_API_KEY"] = getpass.getpass()
# os.environ["HF_TOKEN"] = getpass.getpass()

# new: prefer local_secrets.py (for local dev) then environment (do NOT commit secrets)
try:
    import local_secrets  # local file created by the user
    groq_key = getattr(local_secrets, "GROQ_API_KEY", "").strip()
    hf_token = getattr(local_secrets, "HF_TOKEN", "").strip()
    if groq_key:
        os.environ["GROQ_API_KEY"] = groq_key
    else:
        os.environ["GROQ_API_KEY"] = os.environ.get("GROQ_API_KEY", "")
    if hf_token:
        os.environ["HF_TOKEN"] = hf_token
    else:
        os.environ["HF_TOKEN"] = os.environ.get("HF_TOKEN", "")
except ImportError:
    # local_secrets not provided — use environment variables
    os.environ["GROQ_API_KEY"] = os.environ.get("GROQ_API_KEY", "")
    os.environ["HF_TOKEN"] = os.environ.get("HF_TOKEN", "")

# Helper function for printing docs
def pretty_print_docs(docs):
    # Iterate through each document and format the output
    for i, d in enumerate(docs):
        print(f"{'-' * 50}\nDocument {i + 1}:")
        print(f"Content:\n{d.page_content}\n")
        print("Metadata:")
        for key, value in d.metadata.items():
            print(f"  {key}: {value}")
    print(f"{'-' * 50}")  # Final separator for clarity

# Example usage
# Assuming `docs` is a list of Document objects

# Commented out IPython magic to ensure Python compatibility.
# %pip install -qU langchain-community pypdf

# Removed Colab magic (!pip install ...) — install packages manually in your environment.

# Replace the Colab PDF folder path with a local-relative path
pdf_folder = "./documents/*.pdf"   # <-- put your PDFs in c:\Users\saish\OneDrive\Desktop\RAG\documents\

# Helper: print venv/activation guidance and common mistake
def print_env_help():
    print("\nVirtualenv creation and activation examples (Windows):")
    print("  Create venv (correct): python -m venv .venv")
    print("  Common mistake (wrong): python -m venv.venv   <-- missing space")
    print("  Activate (PowerShell): .\\.venv\\Scripts\\Activate")
    print("  Activate (CMD):        .\\.venv\\Scripts\\activate.bat")
    print("  Or with conda:         conda create -n rag python=3.10 && conda activate rag\n")

# Helper: check for common required packages and print install hint
def check_required_packages():
    missing = []
    try:
        import langchain  # noqa: F401
    except Exception:
        missing.append("langchain")
    try:
        import langchain_huggingface  # noqa: F401
    except Exception:
        missing.append("langchain-huggingface")
    try:
        import langchain_groq  # noqa: F401
    except Exception:
        missing.append("langchain-groq")
    try:
        import sentence_transformers  # noqa: F401
    except Exception:
        missing.append("sentence-transformers")
    # FAISS can be named faiss or faiss_cpu depending on package
    try:
        import faiss  # noqa: F401
    except Exception:
        try:
            import faiss_cpu  # noqa: F401
        except Exception:
            missing.append("faiss-cpu (or faiss)")

    if missing:
        print("Missing Python packages detected:", ", ".join(missing))
        print("Install them inside your virtualenv, for example:")
        print("  pip install sentence-transformers langchain langchain-groq langchain-huggingface einops faiss-cpu")
        print_env_help()
        sys.exit(1)

# wrap execution in main guard so script can be imported or run
def main():
    # Early environment checks to make terminal errors actionable
    check_required_packages()

    # Auto-load PDFs from Documents folder
    pdf_files = glob(pdf_folder)

    if not pdf_files:
        print("No PDF files found in the documents folder:", os.path.abspath(os.path.dirname(pdf_folder)))
        print("Put PDFs in the documents folder and re-run. Example path:")
        print(os.path.join(os.getcwd(), "documents"))
        print_env_help()  # show venv/help when user gets this far
        return

    documents = []
    for file in pdf_files:
        loader = PyPDFLoader(file)
        documents.extend(loader.load())

    print(f"Loaded {len(pdf_files)} PDF files ")

    # Split documents into chunks of 500 characters with 100 characters overlap
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
    texts = text_splitter.split_documents(documents)
    for idx, text in enumerate(texts):
        text.metadata["id"] = idx

    print(f"Created {len(texts)} text chunks")

    # Create embeddings for the text chunks
    embedding = HuggingFaceEmbeddings(model_name="nomic-ai/nomic-embed-text-v1.5", model_kwargs={'trust_remote_code': True})

    # Initialize a FAISS (Vector Store) retriever with the text embeddings
    retriever = FAISS.from_documents(texts, embedding).as_retriever(search_kwargs={"k": 20})

    # Define LLM and chain
    llm = ChatGroq(model="openai/gpt-oss-120b", temperature=0.9)
    chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)

    # Example queries
    query = "give me summary of this?"
    result = chain.run(query)   # <-- use run for terminal script
    print(result)

    query = "Explain the role fo infoedge _data_scientist"
    result = chain.run(query)
    print(result)

if __name__ == "__main__":
    main()

cd "C:\Users\saish\OneDrive\Desktop\RAG"

# create venv (note the space)
python -m venv .venv

# activate (PowerShell)
.\.venv\Scripts\Activate

# upgrade pip and install deps
python -m pip install --upgrade pip
pip install sentence-transformers langchain langchain-groq langchain-community langchain-huggingface einops faiss-cpu

# run the script
python rag_basics.py

