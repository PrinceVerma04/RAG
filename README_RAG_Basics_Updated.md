# ğŸ§  RAG Basics â€” Multi-PDF Question Answering using LangChain, Groq, and FAISS

This repository demonstrates a **Retrieval-Augmented Generation (RAG)** pipeline for reading and querying multiple PDFs locally using **LangChain**, **HuggingFace embeddings**, and **Groq LLMs**.

You can use this code to load PDFs, chunk them into embeddings, store them in a FAISS vector database, and query them using natural language.

---

## ğŸ“š Overview

**Goal:** Build a local RAG pipeline capable of understanding multiple PDFs and answering questions contextually.

### âš™ï¸ Main Components
| Component | Description |
|------------|-------------|
| **LangChain** | Framework to manage document loaders, retrievers, and chains |
| **HuggingFace Embeddings** | Converts text chunks into numerical vectors |
| **FAISS** | Vector store for fast semantic search |
| **Groq LLM (openai/gpt-oss-120b)** | Model that generates final responses |
| **RAG Pipeline** | Combines retrieval (from FAISS) and generation (from LLM) |

---

## ğŸ› ï¸ Setup Instructions

### ğŸ§© Folder Structure

```
RAG/
â”‚
â”œâ”€â”€ rag_basics.py              # Main Python script
â”œâ”€â”€ documents/                 # Folder containing your PDFs
â”œâ”€â”€ local_secrets.py (optional)# File to safely store your API keys locally
â””â”€â”€ README.md
```

### ğŸ“¦ Step 1 â€” Create Virtual Environment

```bash
# Navigate to your project folder
cd "C:\Users\saish\OneDrive\Desktop\RAG"

# Create venv (note the space)
python -m venv .venv

# Activate (PowerShell)
.\.venv\Scripts\Activate

# Activate (CMD)
.\.venv\Scripts\activate.bat
```

> âš ï¸ **Common mistake:** Don't forget the space in `python -m venv .venv`  
> âŒ Wrong: `python -m venv.venv`

---

### ğŸ§° Step 2 â€” Install Dependencies

```bash
python -m pip install --upgrade pip

pip install sentence-transformers langchain langchain-groq langchain-community langchain-huggingface einops faiss-cpu
```

---

### ğŸ”‘ Step 3 â€” Set API Keys

You need two API keys:

1. **Groq API Key** â€” For using the `ChatGroq` LLM.  
   ğŸ‘‰ Create one for free at: [https://console.groq.com/keys](https://console.groq.com/keys)

2. **HuggingFace API Token** â€” For downloading embedding models.  
   ğŸ‘‰ Generate your token here: [https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)

#### Option A: Add them manually via environment variables

```bash
setx GROQ_API_KEY "your_groq_api_key_here"
setx HF_TOKEN "your_huggingface_token_here"
```

#### Option B: Store them in a local file (`local_secrets.py`)

Create a file named `local_secrets.py` inside your project folder:

```python
# local_secrets.py
GROQ_API_KEY = "your_groq_api_key_here"
HF_TOKEN = "your_huggingface_token_here"
```

This method keeps your credentials safe from version control.

---

### ğŸ“„ Step 4 â€” Add PDFs

Place your PDFs in the `documents/` folder:

```
RAG/
â””â”€â”€ documents/
    â”œâ”€â”€ ECAPA-TDNN.pdf
    â”œâ”€â”€ SKA-TDNN.pdf
    â”œâ”€â”€ InfoEdge_Data_Scientist_2025.pdf
    â””â”€â”€ YourPaper.pdf
```

---

### ğŸš€ Step 5 â€” Run the Script

```bash
python rag_basics.py
```

Expected output:

```
Loaded 4 PDF files
Created 152 text chunks
<AI-generated answer summarizing your PDFs>
```

---

## ğŸ§  Code Explanation

### 1ï¸âƒ£ PDF Loading

```python
pdf_files = glob("./documents/*.pdf")
for file in pdf_files:
    loader = PyPDFLoader(file)
    documents.extend(loader.load())
```
Loads all PDFs from the `documents/` folder.

---

### 2ï¸âƒ£ Text Chunking

```python
text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
texts = text_splitter.split_documents(documents)
```
Splits large texts into small overlapping chunks to maintain context during embedding.

---

### 3ï¸âƒ£ Embeddings

```python
embedding = HuggingFaceEmbeddings(model_name="nomic-ai/nomic-embed-text-v1.5")
```
Converts each text chunk into a high-dimensional vector using a pre-trained model.

---

### 4ï¸âƒ£ FAISS Vector Store

```python
retriever = FAISS.from_documents(texts, embedding).as_retriever(search_kwargs={"k": 20})
```
Stores embeddings in a FAISS database for efficient vector similarity search.

---

### 5ï¸âƒ£ LLM and RAG Chain

```python
llm = ChatGroq(model="openai/gpt-oss-120b", temperature=0.9)
chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)
```

The RetrievalQA chain combines:
- **Retriever:** Fetches top relevant chunks.
- **LLM:** Generates final answers grounded in retrieved content.

---

### 6ï¸âƒ£ Query and Output

```python
query = "Give me summary of this?"
result = chain.run(query)
print(result)
```
Sends the question to the RAG system and prints the LLMâ€™s contextual answer.

---

## ğŸ’¬ Example Queries

```text
"Summarize ECAPA-TDNN paper."
"Explain InfoEdge Data Scientist report."
"Compare SKA-TDNN and ECAPA-TDNN models."
"List all datasets mentioned across papers."
```

---

## ğŸ§© Common Issues

| Issue | Cause | Fix |
|-------|--------|-----|
| `No module named faiss` | Missing FAISS | Install `faiss-cpu` |
| `Invalid API key` | Missing Groq/HF keys | Add them to `local_secrets.py` |
| `No PDF files found` | Wrong folder path | Ensure PDFs in `/documents` |

---

## ğŸ§¾ Output Example

```
Loaded 5 PDF files
Created 215 text chunks
Answer:
The ECAPA-TDNN architecture enhances speaker verification by integrating channel attention and residual layers...
```

---

## ğŸ§± Future Enhancements

- Gradio/Streamlit UI for interactive Q&A  
- PDF upload via web interface  
- Support for YouTube transcripts or DOCX files  
- Local persistence of FAISS index for faster reloads  

---

## ğŸ§‘â€ğŸ’» Author

Developed by **Prince Verma**  
RAG System using LangChain + HuggingFace + Groq

---

## ğŸ“œ License

This project is for **academic and educational purposes** only.

---
