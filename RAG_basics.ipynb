{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOChPrzN6A7iaaE2PdoC1Ma",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f824d00eb0274e29b99616d1866eec73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3d286dd471804058a8ec92ec3eb4a9a7",
              "IPY_MODEL_1c4ce3fa903c48919d2ba1f204094e4f",
              "IPY_MODEL_62cec4a858e64b0ea5ff2f66000509b0"
            ],
            "layout": "IPY_MODEL_a92f080a7c49485ab02cb4160eb4cb28"
          }
        },
        "3d286dd471804058a8ec92ec3eb4a9a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4c2cdfc51e6a4a9485275f81b84fac31",
            "placeholder": "​",
            "style": "IPY_MODEL_b48f89bfa20f442d87eb4729f7f81bdc",
            "value": "modules.json: 100%"
          }
        },
        "1c4ce3fa903c48919d2ba1f204094e4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_403bee13a3b94d2ab553c48640fee635",
            "max": 255,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_af656d5a6adb45c7a3b9fc3fd9b1ba9e",
            "value": 255
          }
        },
        "62cec4a858e64b0ea5ff2f66000509b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4385dd78e1e940a4859ca4d976a919fe",
            "placeholder": "​",
            "style": "IPY_MODEL_5a4d88a8d41949148b414b92ef5f7b62",
            "value": " 255/255 [00:00&lt;00:00, 11.7kB/s]"
          }
        },
        "a92f080a7c49485ab02cb4160eb4cb28": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c2cdfc51e6a4a9485275f81b84fac31": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b48f89bfa20f442d87eb4729f7f81bdc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "403bee13a3b94d2ab553c48640fee635": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af656d5a6adb45c7a3b9fc3fd9b1ba9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4385dd78e1e940a4859ca4d976a919fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a4d88a8d41949148b414b92ef5f7b62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a0400180db5c4415afa51a31eacd0fd6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_25c01da6ea004ace81bdadc19248d745",
              "IPY_MODEL_899f222440d44e629b08dba2743de590",
              "IPY_MODEL_52d32bf370f04b239948b648acb50ae6"
            ],
            "layout": "IPY_MODEL_153347bc34324bbcaf156044233d51e4"
          }
        },
        "25c01da6ea004ace81bdadc19248d745": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9bd678c150524691b05fac9194eba6fa",
            "placeholder": "​",
            "style": "IPY_MODEL_3ca8cd8126cc408f9c4136536962af2b",
            "value": "config_sentence_transformers.json: 100%"
          }
        },
        "899f222440d44e629b08dba2743de590": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_827154e17fc14be3a47944712bcbc640",
            "max": 140,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0318ad513b834e0490a190ad5024807d",
            "value": 140
          }
        },
        "52d32bf370f04b239948b648acb50ae6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2c7be21e2b6d41ec8a2974edc4ad8602",
            "placeholder": "​",
            "style": "IPY_MODEL_b3b8dc34d40b4dd2a89190635b8caea5",
            "value": " 140/140 [00:00&lt;00:00, 7.11kB/s]"
          }
        },
        "153347bc34324bbcaf156044233d51e4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9bd678c150524691b05fac9194eba6fa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ca8cd8126cc408f9c4136536962af2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "827154e17fc14be3a47944712bcbc640": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0318ad513b834e0490a190ad5024807d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2c7be21e2b6d41ec8a2974edc4ad8602": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b3b8dc34d40b4dd2a89190635b8caea5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1cdd64fe5e4c4721a9f7b864eeb359b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7a37a20adf094c9980a2068aa9645e49",
              "IPY_MODEL_2086f6e4b1b1440ea46785f7aa4c2cdf",
              "IPY_MODEL_a403ee6cf9ea4cf79d7267de2f4ee042"
            ],
            "layout": "IPY_MODEL_fa936923a58046a8abcb3fbf11b33c9e"
          }
        },
        "7a37a20adf094c9980a2068aa9645e49": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f1c318375fbc42ee8fcee3b8bec48404",
            "placeholder": "​",
            "style": "IPY_MODEL_f72af3f02e4d4b1c960d3a7c739561d3",
            "value": "README.md: "
          }
        },
        "2086f6e4b1b1440ea46785f7aa4c2cdf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eeea5d041a674d57a9ead96e58f488f0",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c19b620ab80b46e7b410e36fcf7269ee",
            "value": 1
          }
        },
        "a403ee6cf9ea4cf79d7267de2f4ee042": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2e994a89ad7845adaee92184cf6b370e",
            "placeholder": "​",
            "style": "IPY_MODEL_6adac25fec014e079c0b9f6c056c6ae0",
            "value": " 71.8k/? [00:00&lt;00:00, 3.33MB/s]"
          }
        },
        "fa936923a58046a8abcb3fbf11b33c9e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f1c318375fbc42ee8fcee3b8bec48404": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f72af3f02e4d4b1c960d3a7c739561d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eeea5d041a674d57a9ead96e58f488f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "c19b620ab80b46e7b410e36fcf7269ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2e994a89ad7845adaee92184cf6b370e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6adac25fec014e079c0b9f6c056c6ae0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c93487298a764d9f841ffc9aa84db7e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b0859c7f7c8248a7b4c50b23beb6866a",
              "IPY_MODEL_ad6d9ca3c7d44a63a789c62b516a2eea",
              "IPY_MODEL_8a44ae29e4d4499da456da70039a2e7c"
            ],
            "layout": "IPY_MODEL_bacf3bf1b1f94e4f94ecd07323c3cfbd"
          }
        },
        "b0859c7f7c8248a7b4c50b23beb6866a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c42c0ea0998449399f27fe6a770aca89",
            "placeholder": "​",
            "style": "IPY_MODEL_b215aded4035426ebb36e1c4245fe69b",
            "value": "sentence_bert_config.json: 100%"
          }
        },
        "ad6d9ca3c7d44a63a789c62b516a2eea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cb16ee3ce40243ac9e4baca9a87a45af",
            "max": 120,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7de66071e42f498e86d67d83745bb74d",
            "value": 120
          }
        },
        "8a44ae29e4d4499da456da70039a2e7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b064165dbebf4e828995162e7c337fa7",
            "placeholder": "​",
            "style": "IPY_MODEL_cc298aeb72cc4e6ea4a3fb4f5f1b6498",
            "value": " 120/120 [00:00&lt;00:00, 6.31kB/s]"
          }
        },
        "bacf3bf1b1f94e4f94ecd07323c3cfbd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c42c0ea0998449399f27fe6a770aca89": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b215aded4035426ebb36e1c4245fe69b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cb16ee3ce40243ac9e4baca9a87a45af": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7de66071e42f498e86d67d83745bb74d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b064165dbebf4e828995162e7c337fa7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc298aeb72cc4e6ea4a3fb4f5f1b6498": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a4b7ec057bce41529558411a2f2e3de2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_96f00a020b594331b7151cbb7b7adf80",
              "IPY_MODEL_628f48973f4c497d961b4bb4351b0d56",
              "IPY_MODEL_4e0af9f6b7e841b0818d82ed2f33fb5b"
            ],
            "layout": "IPY_MODEL_fc3afdbcd59e470e9106c4b86a4ff23b"
          }
        },
        "96f00a020b594331b7151cbb7b7adf80": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bdb1bfcec5954e4cbaf38fd712f8f430",
            "placeholder": "​",
            "style": "IPY_MODEL_a617692c9ebc497db11bc708e3e409ed",
            "value": "config.json: "
          }
        },
        "628f48973f4c497d961b4bb4351b0d56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_19e443e0d3c44a1e8137454af48131cc",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_16a0d57ea4044878a171e6c0d4512d1e",
            "value": 1
          }
        },
        "4e0af9f6b7e841b0818d82ed2f33fb5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0267a25f378f44589042a674873ae185",
            "placeholder": "​",
            "style": "IPY_MODEL_f70c7a69bf89437a8253c914e09b1cbd",
            "value": " 2.33k/? [00:00&lt;00:00, 164kB/s]"
          }
        },
        "fc3afdbcd59e470e9106c4b86a4ff23b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bdb1bfcec5954e4cbaf38fd712f8f430": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a617692c9ebc497db11bc708e3e409ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "19e443e0d3c44a1e8137454af48131cc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "16a0d57ea4044878a171e6c0d4512d1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0267a25f378f44589042a674873ae185": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f70c7a69bf89437a8253c914e09b1cbd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "016f071b914a41c9a9b108f4df63b90e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_39ae7e24d84345528d42fc42458b50f9",
              "IPY_MODEL_0a2868f6fa9242ea857953e28d947be6",
              "IPY_MODEL_16d27ad3860a4943b2c945078b881cf2"
            ],
            "layout": "IPY_MODEL_00b5a8bb7baa44e983bc3563439b8d3e"
          }
        },
        "39ae7e24d84345528d42fc42458b50f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c60904536f134aeaa972bada6c634602",
            "placeholder": "​",
            "style": "IPY_MODEL_184db2a996514115b8a52dfa130adcff",
            "value": "configuration_hf_nomic_bert.py: "
          }
        },
        "0a2868f6fa9242ea857953e28d947be6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0de0e480e539402c9374f32a2fd9cef7",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7b8d346ee9ca452c8b66b5a300db47e5",
            "value": 1
          }
        },
        "16d27ad3860a4943b2c945078b881cf2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_96a2580b6eb64f858bcba70fa54ab485",
            "placeholder": "​",
            "style": "IPY_MODEL_930bdd694a7f452c8c7b97d9b1bebe5e",
            "value": " 1.96k/? [00:00&lt;00:00, 140kB/s]"
          }
        },
        "00b5a8bb7baa44e983bc3563439b8d3e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c60904536f134aeaa972bada6c634602": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "184db2a996514115b8a52dfa130adcff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0de0e480e539402c9374f32a2fd9cef7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "7b8d346ee9ca452c8b66b5a300db47e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "96a2580b6eb64f858bcba70fa54ab485": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "930bdd694a7f452c8c7b97d9b1bebe5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f071c9dcbfe14a8e8603165e48e0791f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_880d23d0845741098b4189bca5414759",
              "IPY_MODEL_068901942c30418aae76e5ac65595dbb",
              "IPY_MODEL_721ab22448b94230b18cf579b767d3af"
            ],
            "layout": "IPY_MODEL_653badcba90f474a8ee5d57eaf028ac4"
          }
        },
        "880d23d0845741098b4189bca5414759": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_80e554035bfc4628b0deddabcb1d1739",
            "placeholder": "​",
            "style": "IPY_MODEL_fcaf72e3159e4325b1452f1506ad29b9",
            "value": "modeling_hf_nomic_bert.py: "
          }
        },
        "068901942c30418aae76e5ac65595dbb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ebb614b9e9a44885a40dabbd6c808c7b",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_992e8f7513d844009638c8862de40a10",
            "value": 1
          }
        },
        "721ab22448b94230b18cf579b767d3af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d3a2eda188b440a1806ef9881e6ecb35",
            "placeholder": "​",
            "style": "IPY_MODEL_efe7533e63774879a1259e0eccde647f",
            "value": " 104k/? [00:00&lt;00:00, 7.59MB/s]"
          }
        },
        "653badcba90f474a8ee5d57eaf028ac4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "80e554035bfc4628b0deddabcb1d1739": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fcaf72e3159e4325b1452f1506ad29b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ebb614b9e9a44885a40dabbd6c808c7b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "992e8f7513d844009638c8862de40a10": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d3a2eda188b440a1806ef9881e6ecb35": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "efe7533e63774879a1259e0eccde647f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1e8bbbcec5684c3fb88b46dfee733d54": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dc4ebff916f84bba98c84de7afbee852",
              "IPY_MODEL_277d8207ff5f49178cbee2a3363b97df",
              "IPY_MODEL_f2d021e50e5f4aaca9907fb6a82d23bc"
            ],
            "layout": "IPY_MODEL_b1da9bb3292c4b41b13dca5faaf0f89e"
          }
        },
        "dc4ebff916f84bba98c84de7afbee852": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_59a2282225eb43e4a5134c7f28e0f0ff",
            "placeholder": "​",
            "style": "IPY_MODEL_fa924fbf3f7f4f4dbcd6a43674562440",
            "value": "model.safetensors: 100%"
          }
        },
        "277d8207ff5f49178cbee2a3363b97df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ea090cde7e7540218187c4308ab464aa",
            "max": 546938168,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2c137a3fb71e4ff39d8644f9577a0a4a",
            "value": 546938168
          }
        },
        "f2d021e50e5f4aaca9907fb6a82d23bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b9c594dbd4b44fd6a0cea5cb187cdb00",
            "placeholder": "​",
            "style": "IPY_MODEL_613ee9bee03e4ac9b5c87e382925cc02",
            "value": " 547M/547M [00:09&lt;00:00, 36.9MB/s]"
          }
        },
        "b1da9bb3292c4b41b13dca5faaf0f89e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "59a2282225eb43e4a5134c7f28e0f0ff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa924fbf3f7f4f4dbcd6a43674562440": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ea090cde7e7540218187c4308ab464aa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c137a3fb71e4ff39d8644f9577a0a4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b9c594dbd4b44fd6a0cea5cb187cdb00": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "613ee9bee03e4ac9b5c87e382925cc02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3c311f0293004dd1a19b6c73b581d68d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_99e75e7bbbd34471880b17b18ba7fc20",
              "IPY_MODEL_2235b116df1f494c9fce73a7a38a19a6",
              "IPY_MODEL_83c28460467c433fbd8eb5ceb67743a8"
            ],
            "layout": "IPY_MODEL_3ed99862a3a048e1a3e8544bf4fe0cdc"
          }
        },
        "99e75e7bbbd34471880b17b18ba7fc20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e48f1097ff414f3794008996669439f7",
            "placeholder": "​",
            "style": "IPY_MODEL_5b03c9a460294597a58e44433588392b",
            "value": "tokenizer_config.json: "
          }
        },
        "2235b116df1f494c9fce73a7a38a19a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3855765c295148a89cc56e101557e860",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_82b5ea5c95794b8d98aafcc65b619bf1",
            "value": 1
          }
        },
        "83c28460467c433fbd8eb5ceb67743a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0e2c290792754c658b3d45199f8b2333",
            "placeholder": "​",
            "style": "IPY_MODEL_ce47345ba07d42718d0280d3ebdcd655",
            "value": " 1.19k/? [00:00&lt;00:00, 88.0kB/s]"
          }
        },
        "3ed99862a3a048e1a3e8544bf4fe0cdc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e48f1097ff414f3794008996669439f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b03c9a460294597a58e44433588392b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3855765c295148a89cc56e101557e860": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "82b5ea5c95794b8d98aafcc65b619bf1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0e2c290792754c658b3d45199f8b2333": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce47345ba07d42718d0280d3ebdcd655": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5b9bff89d2234c78b4656e31cc6ed354": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_61de5aa2756343eea6f0caa9aed67dc9",
              "IPY_MODEL_ff48fa24654649be910302be350005a5",
              "IPY_MODEL_e33a748795bb4868b61863dad673e67e"
            ],
            "layout": "IPY_MODEL_a1cd1d028992418e9939cf2bebb14bae"
          }
        },
        "61de5aa2756343eea6f0caa9aed67dc9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_259fe087fb9742cfa2f3d4885329eb6d",
            "placeholder": "​",
            "style": "IPY_MODEL_0bb60e26366d4e048ade4189782a3527",
            "value": "vocab.txt: "
          }
        },
        "ff48fa24654649be910302be350005a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9e90cf5a553c46fdaa1e2e8d9b2d6d67",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d2ce16d7a2a443d1ac6771cd8a39b27e",
            "value": 1
          }
        },
        "e33a748795bb4868b61863dad673e67e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_64c16938a20a4ba08f85a96b428350e2",
            "placeholder": "​",
            "style": "IPY_MODEL_100a11b0c270445683b89a995de361e9",
            "value": " 232k/? [00:00&lt;00:00, 7.68MB/s]"
          }
        },
        "a1cd1d028992418e9939cf2bebb14bae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "259fe087fb9742cfa2f3d4885329eb6d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0bb60e26366d4e048ade4189782a3527": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9e90cf5a553c46fdaa1e2e8d9b2d6d67": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "d2ce16d7a2a443d1ac6771cd8a39b27e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "64c16938a20a4ba08f85a96b428350e2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "100a11b0c270445683b89a995de361e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a2cd8444035c4eddb710ec217df3b7a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_eb2a5521a47a447790ce7dfc069a14f8",
              "IPY_MODEL_e189fe1bf2f04198a411213fa01114f8",
              "IPY_MODEL_fc2998fbc37647459d29dc8dee9bf445"
            ],
            "layout": "IPY_MODEL_1fe209a0b91845cca00840dc98dbef12"
          }
        },
        "eb2a5521a47a447790ce7dfc069a14f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2c712478359c4334bd1b846427a71ab5",
            "placeholder": "​",
            "style": "IPY_MODEL_ed2a766723c6482ea2ca06046cf51c51",
            "value": "tokenizer.json: "
          }
        },
        "e189fe1bf2f04198a411213fa01114f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2d52815bb96741fa893e760adb8abe31",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f9ac908adbd549dca2e8282bfdc6dab9",
            "value": 1
          }
        },
        "fc2998fbc37647459d29dc8dee9bf445": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4cc8270b531742678db83a4851ef8289",
            "placeholder": "​",
            "style": "IPY_MODEL_a79a353c7bac411c9e481cce7c98c345",
            "value": " 711k/? [00:00&lt;00:00, 14.7MB/s]"
          }
        },
        "1fe209a0b91845cca00840dc98dbef12": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c712478359c4334bd1b846427a71ab5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed2a766723c6482ea2ca06046cf51c51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2d52815bb96741fa893e760adb8abe31": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "f9ac908adbd549dca2e8282bfdc6dab9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4cc8270b531742678db83a4851ef8289": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a79a353c7bac411c9e481cce7c98c345": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ae80e3224eea44c2a3b51f084d42efe8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_de5fd94152d844e3bb13a0c1c58caca1",
              "IPY_MODEL_de3cd9ee49cf4b86b4d3cf1ec085c4ff",
              "IPY_MODEL_f18e44d69bf24c64965d2a22473cb05e"
            ],
            "layout": "IPY_MODEL_7c466d1b4cfd4d55892390a204085695"
          }
        },
        "de5fd94152d844e3bb13a0c1c58caca1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0099939376db41018f5b6d2c0264b8b7",
            "placeholder": "​",
            "style": "IPY_MODEL_c9e2673428ca4d3992b36d89e9171d42",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "de3cd9ee49cf4b86b4d3cf1ec085c4ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_626e753e3d394ad5a8aaf3538e56fba2",
            "max": 695,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b9085a75972b46c885975523f12c8365",
            "value": 695
          }
        },
        "f18e44d69bf24c64965d2a22473cb05e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_55e677ac79ae4584bd075a1b9a10d342",
            "placeholder": "​",
            "style": "IPY_MODEL_981dad94e9cc4e20a6dff47cc22a7346",
            "value": " 695/695 [00:00&lt;00:00, 53.6kB/s]"
          }
        },
        "7c466d1b4cfd4d55892390a204085695": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0099939376db41018f5b6d2c0264b8b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c9e2673428ca4d3992b36d89e9171d42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "626e753e3d394ad5a8aaf3538e56fba2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b9085a75972b46c885975523f12c8365": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "55e677ac79ae4584bd075a1b9a10d342": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "981dad94e9cc4e20a6dff47cc22a7346": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "160a740f12f143d1a29aec4dbb6196b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a5b9510692314346858d2a59d2636694",
              "IPY_MODEL_bf5b10dcec4c475e9455aa6e6fe19eef",
              "IPY_MODEL_20b01b9183d54e288b4c7c5152479730"
            ],
            "layout": "IPY_MODEL_546edc99466c4592a41ccf129cd1d2eb"
          }
        },
        "a5b9510692314346858d2a59d2636694": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c24853989d5d46eca8a115ba09129275",
            "placeholder": "​",
            "style": "IPY_MODEL_293f5ff4498e4e00a12a06eb40f5fe47",
            "value": "config.json: 100%"
          }
        },
        "bf5b10dcec4c475e9455aa6e6fe19eef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d795a28b1ca5458a9c62afb9b2e93f1d",
            "max": 286,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f767a22290e94c08a31992617af258c1",
            "value": 286
          }
        },
        "20b01b9183d54e288b4c7c5152479730": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d920b9419375496ebf12c54f9025d1ca",
            "placeholder": "​",
            "style": "IPY_MODEL_e79d768fed964348b139fd7759790a8b",
            "value": " 286/286 [00:00&lt;00:00, 28.3kB/s]"
          }
        },
        "546edc99466c4592a41ccf129cd1d2eb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c24853989d5d46eca8a115ba09129275": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "293f5ff4498e4e00a12a06eb40f5fe47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d795a28b1ca5458a9c62afb9b2e93f1d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f767a22290e94c08a31992617af258c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d920b9419375496ebf12c54f9025d1ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e79d768fed964348b139fd7759790a8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PrinceVerma04/RAG/blob/main/RAG_basics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5CQLMh14gs9Y",
        "outputId": "4c9e54e0-a657-4b23-d69d-70007c09a59c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.8/135.8 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.0/363.0 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m964.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q -U \\\n",
        "     Sentence-transformers==3.0.1 \\\n",
        "     langchain==0.3.19 \\\n",
        "     langchain-groq==0.2.4 \\\n",
        "     langchain-community==0.3.18 \\\n",
        "     langchain-huggingface==0.1.2 \\\n",
        "     einops==0.8.1 \\\n",
        "     faiss_cpu==1.10.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from glob import glob\n",
        "from langchain_community.document_loaders import PyPDFLoader"
      ],
      "metadata": {
        "id": "pzKQgBchm55p"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_groq import ChatGroq\n",
        "\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n"
      ],
      "metadata": {
        "id": "1RZQo7z7gxeW"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os"
      ],
      "metadata": {
        "id": "tOlQ1L8sgz6P"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lm99z7mWg74K",
        "outputId": "ea77f070-4dd6-41aa-f237-3bbe58294f90"
      },
      "execution_count": 5,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"HF_TOKEN\"] = getpass.getpass()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NyjbPho3hT7S",
        "outputId": "209dd20e-bba2-4fa5-fa17-6a7f45790d4d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Helper function for printing docs\n",
        "def pretty_print_docs(docs):\n",
        "    # Iterate through each document and format the output\n",
        "    for i, d in enumerate(docs):\n",
        "        print(f\"{'-' * 50}\\nDocument {i + 1}:\")\n",
        "        print(f\"Content:\\n{d.page_content}\\n\")\n",
        "        print(\"Metadata:\")\n",
        "        for key, value in d.metadata.items():\n",
        "            print(f\"  {key}: {value}\")\n",
        "    print(f\"{'-' * 50}\")  # Final separator for clarity\n",
        "\n",
        "# Example usage\n",
        "# Assuming `docs` is a list of Document objects"
      ],
      "metadata": {
        "id": "2yqzPSPfhwau"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -qU langchain-community pypdf\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tvtx8rB7hp-a",
        "outputId": "a84a2616-ffa3-4770-e6b9-e07bad2269ec"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.8/2.5 MB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/326.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m326.6/326.6 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m40.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.2/471.2 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain 0.3.19 requires langchain-core<1.0.0,>=0.3.35, but you have langchain-core 1.0.4 which is incompatible.\n",
            "langchain 0.3.19 requires langchain-text-splitters<1.0.0,>=0.3.6, but you have langchain-text-splitters 1.0.0 which is incompatible.\n",
            "langchain-huggingface 0.1.2 requires langchain-core<0.4.0,>=0.3.15, but you have langchain-core 1.0.4 which is incompatible.\n",
            "langchain-groq 0.2.4 requires langchain-core<0.4.0,>=0.3.33, but you have langchain-core 1.0.4 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ✅ Auto-load PDFs from Documents folder\n",
        "pdf_folder = \"/content/Documents/*.pdf\"\n",
        "pdf_files = glob(pdf_folder)"
      ],
      "metadata": {
        "id": "0WY3npyGnIvD"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "documents = []\n",
        "for file in pdf_files:\n",
        "    loader = PyPDFLoader(file)\n",
        "    documents.extend(loader.load())\n",
        "\n",
        "print(f\"Loaded {len(pdf_files)} PDF files ✅\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j19BAEshnMBK",
        "outputId": "be16a6f5-03f7-45ed-fafa-3b838bd7881e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 5 PDF files ✅\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from langchain_community.document_loaders import WebBaseLoader, PyPDFLoader\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "# The PDF files are already loaded into the 'documents' variable from a previous cell.\n",
        "# No need to load them again here with a directory path, so we remove the following lines:\n",
        "# file_path = \"/content/Documents\"\n",
        "# loader = PyPDFLoader(file_path)\n",
        "# documents = loader.load()  # <-- load the PDF into documents\n",
        "\n",
        "# Split documents into chunks of 500 characters with 100 characters overlap\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
        "texts = text_splitter.split_documents(documents)\n",
        "# Add unique IDs to each text chunk\n",
        "for idx, text in enumerate(texts):\n",
        "    text.metadata[\"id\"] = idx\n",
        "\n",
        "\n",
        "print(documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "USY7msvEiUUf",
        "outputId": "218a96b2-732c-42f7-b1e6-a5a4fd94ccc4"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Document(metadata={'producer': 'Acrobat Distiller 11.0 (Windows); modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref package', 'creationdate': '2025-02-08T10:21:28+05:30', 'moddate': '2025-02-12T12:23:54-05:00', 'ieee article id': '10839331', 'ieee issue id': '10834412', 'subject': 'IEEE Open Journal of Signal Processing;2025;6; ;10.1109/OJSP.2025.3529377', 'ieee publication id': '8782710', 'title': 'SpoofCeleb: Speech Deepfake Detection and SASV in the Wild', 'source': '/content/Documents/SpoofCeleb_Speech_Deepfake_Detection_and_SASV_in_the_Wild (1).pdf', 'total_pages': 10, 'page': 0, 'page_label': '1'}, page_content='Received 17 September 2024; revised 3 December 2024; accepted 24 December 2024. Date of publication 13 January 2025;\\ndate of current version 12 February 2025. The review of this article was arranged by Associate Editor Zafar Raﬁi.\\nDigital Object Identiﬁer 10.1109/OJSP .2025.3529377\\nSpoofCeleb: Speech Deepfake Detection and\\nSASV in the Wild\\nJEE-WEON JUNG (Member, IEEE), YIHAN WU (Member, IEEE), XIN WANG(Member, IEEE),\\nJI-HOON KIM (Member, IEEE), SOUMI MAITI (Member, IEEE), YUTA MATSUNAGA,\\nHYE-JIN SHIM (Member, IEEE), JINCHUAN TIAN, NICHOLAS EVANS (Member, IEEE),\\nJOON SON CHUNG (Member, IEEE), WANGYOU ZHANG (Member, IEEE), SEYUN UM (Member, IEEE),\\nSHINNOSUKE TAKAMICHI (Member, IEEE), AND SHINJI WATANABE (Fellow, IEEE)\\n1Carnegie Mellon University, Pittsburgh, PA 15213 USA\\n2Renmin University of China, Beijing 100872, China\\n3National Institute of Informatics, Chiyoda, Tokyo 101-8430, Japan\\n4Korea Advanced Institute of Science and Technology, Daejeon 34141, South Korea\\n5Meta, Menlo Park, CA 94025 USA\\n6University of Tokyo, Bunkyo, Tokyo 1130033, Japan\\n7EURECOM, 06410 Biot, France\\n8Shanghai Jiao Tong University, Shanghai 200240, China\\n9Yonsei University, Seoul 03722, South Korea\\n10Keio University, Kanagawa 2238521, Japan\\nCORRESPONDING AUTHOR: JEE-WEON JUNG (email: jeeweonj@ieee.org).\\nThe work of Jee-weon Jung, Hye-Jin Shim, Jinchuan Tian, and Shinji Watanabe was supported in part by Bridges2 System at PSC and the Delta System at NCSA\\nthrough an Allocation under Grant CIS210014 from the Advanced Cyberinfrastructure Coordination Ecosystem: Services and Support (ACCESS) Programand in\\npart by the National Science Foundation under Grant 2138259, Grant 2138286, Grant 2138307, Grant 2137603, and Grant 2138296. The work of Xin Wang was\\nsupported in part by JST and in part by PRESTO under Grant JPMJPR23P9, Japan. This work was supported by ONR under Grant N00014-231-2086.\\nABSTRACT This paper introduces SpoofCeleb, a dataset designed for Speech Deepfake Detection (SDD)\\nand Spooﬁng-robust Automatic Speaker Veriﬁcation (SASV), utilizing source data from real-world condi-\\ntions and spooﬁng attacks generated by Text-To-Speech (TTS) systems also trained on the same real-world\\ndata. Robust recognition systems require speech data recorded in varied acoustic environments with different\\nlevels of noise to be trained. However, current datasets typically include clean, high-quality recordings\\n(bona ﬁde data) due to the requirements for TTS training; studio-quality or well-recorded read speech is\\ntypically necessary to train TTS models. Current SDD datasets also have limited usefulness for training\\nSASV models due to insufﬁcient speaker diversity. SpoofCeleb leverages a fully automated pipeline we\\ndeveloped that processes the V oxCeleb1 dataset, transforming it into a suitable form for TTS training.\\nWe subsequently train 23 contemporary TTS systems. SpoofCeleb comprises over 2.5 million utterances\\nfrom 1,251 unique speakers, collected under natural, real-world conditions. The dataset includes carefully\\npartitioned training, validation, and evaluation sets with well-controlled experimental protocols. We present\\nthe baseline results for both SDD and SASV tasks. All data, protocols, and baselines are publicly available\\nat https://jungjee.github.io/spoofceleb.\\nINDEX TERMS In the wild, speech deepfake detection, spooﬁng-robust automatic speaker veriﬁcation.\\nI. INTRODUCTION\\nThe quality of synthetic speech has improved rapidly, driven\\nby advancements in technologies such as ﬂow matching, neu-\\nral codecs, and speech-language modeling[1], [2], [3]. These\\ninnovations have signiﬁcantly enhanced the naturalness and\\nintelligibility of generated speech. The increasing availability\\nof open sources and APIs for Text-To-Speech (TTS) systems\\nhas made high-quality synthetic speech more accessible to the\\ngeneral public[4], [5].\\nAlthough originally developed for positive applications,\\nthis technology is increasingly being exploited for malicious\\npurposes [6], [7]. Synthetic speech generated with harmful\\n© 2025 The Authors. This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\\n68 VOLUME 6, 2025'), Document(metadata={'producer': 'Acrobat Distiller 11.0 (Windows); modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref package', 'creationdate': '2025-02-08T10:21:28+05:30', 'moddate': '2025-02-12T12:23:54-05:00', 'ieee article id': '10839331', 'ieee issue id': '10834412', 'subject': 'IEEE Open Journal of Signal Processing;2025;6; ;10.1109/OJSP.2025.3529377', 'ieee publication id': '8782710', 'title': 'SpoofCeleb: Speech Deepfake Detection and SASV in the Wild', 'source': '/content/Documents/SpoofCeleb_Speech_Deepfake_Detection_and_SASV_in_the_Wild (1).pdf', 'total_pages': 10, 'page': 1, 'page_label': '2'}, page_content='intent, often referred to as spooﬁng, is being used to deceive\\nindividuals in scenarios such as voice phishing (or vishing).\\nSpooﬁng also undermines the reliability of speech biometric\\nsystems, including Automatic Speaker Veriﬁcation (ASV),\\nmany of which remain highly vulnerable to such attacks\\n[8], [9].\\nIn response to these challenges, several datasets have been\\ndeveloped to advance research in Speech Deepfake Detection\\n(SDD) [10], [11], [12]. For robust recognition systems, it is\\nessential to have training data that cover a wide range of real-\\nworld acoustic environments and speaker diversity. However,\\nspeech generation systems, such as TTS and V oice Conversion\\n(VC), typically require studio-quality or clean, read speech\\nfor training. Therefore, current datasets tend to feature clean,\\nmonotonic bona ﬁde speech, with spoofed samples also being\\nclean, as they are synthesized using TTS and VC systems\\ntrained on such data. The emerging task of Spooﬁng-robust\\nAutomatic Speaker Veriﬁcation (SASV)[13] lacks dedicated\\ndatasets. Many SDD datasets also suffer from limited speaker\\ndiversity, which hinders research on SASV systems that re-\\nquire training with data from hundreds or even thousands of\\nspeakers.\\nTo this end, we introduce SpoofCeleb, a dataset built upon\\nV oxCeleb1[14], a widely used ASV dataset consisting of\\nthe voices of 1,251 celebrities recorded under real-world\\nconditions. We also develop a fully automated pipeline that\\nprocesses V oxCeleb1 to produce in-the-wild bona ﬁde speech\\nsamples that can be used for training TTS systems.1 From\\nthe two available TTS training sets in TITW, we use TITW-\\nEasy as the source dataset to generate 23 spooﬁng attacks.\\nSpoofCeleb is the ﬁrst dataset explicitly designed for both\\nSDD and SASV , where the bona ﬁde speech is real-world,\\nnoisy speech. The dataset is divided into three subsets for\\ntraining, validation, and evaluation, accompanied by evalu-\\nation protocols. Baseline systems trained on SpoofCeleb’s\\ntraining set are also presented, demonstrating SpoofCeleb’s\\neffectiveness in and potential for future research in SDD and\\nSASV .\\nII. RELATED WORKS\\nDatasets for SDD and the generation-recognition trade-off:\\nTo safeguard the authenticity of speech, several datasets have\\nbeen published to support research in SDD[9], [10], [11],\\n[12], [18], [21], [23], [26]. One of the most critical decisions\\nwhen creating these datasets is the selection of the source data\\n(i.e., bona ﬁde speech). This decision involves a trade-off,\\nwhich we refer to as the “generation-recognition trade-off.”\\nFor both SDD and SASV on the recognition side, incor-\\nporating data with diverse noise, reverberation, and varied\\ndomains is essential for training robust models. It is well\\nknown that recognition models trained solely on clean speech\\n1The development of this pipeline is extensive, and the resulting bona ﬁde\\nspeech data can serve other purposes, such as advancing research on TTS\\nsystems trained on noisy, in-the-wild data. We detail this aspect in a separate\\nwork, referring to the dataset as TTS In The Wild (TITW)[15].\\noften struggle to effectively generalize to noisy environments\\nduring inference [18]. While data augmentation techniques\\ncan help mitigate this issue[31], the most effective solution\\nis to use training data drawn from a wide range of real-world\\nsources.\\nConversely, traditional TTS training requires a carefully\\ncurated and recorded dataset. Sentence prompts must be\\nselected to ensure comprehensive phonetic coverage [32],\\nand recordings are typically made by voice professionals\\nin clean environments, ideally in a single anechoic studio.\\nThese recordings are of high studio quality and carefully ar-\\nticulated but are not scalable. For instance, the well-known\\nCMU Arctic database includes recordings from fewer than\\n10 voice professionals, each reading approximately 1,000\\nspeech prompts [32]. Modern TTS systems, however, often\\nrequire signiﬁcantly more training data. Instead of relying\\non these small-scale, TTS-speciﬁc databases, contemporary\\nmodels frequently use audiobook datasets (e.g., MLS[33]),\\nwhich, while not studio-grade, consist of relatively clean au-\\ndiobook recordings made by numerous readers in their homes\\nor ofﬁces.\\nCurrent SDD datasets tend to lean towards the generation\\nside of the generation-recognition trade-off. They use source\\ndatasets that consist of either studio-quality or high-quality\\nspeech, facilitating the training of TTS and VC systems and\\nthe successful generation of spoofed speech samples. How-\\never, both the bona ﬁde and spoofed speech in these datasets\\nare exceedingly clean, making them far from real-world, noisy\\nspeech data.\\nSpoofCeleb is the ﬁrst dataset to use real-world, noisy,\\nand reverberant data originating from TITW, which originates\\nfrom V oxCeleb1, as the source for training and synthesiz-\\ning spoofed speech. We tackle the generation-recognition\\ntrade-off by using our carefully curated, fully automated pre-\\nprocessing pipeline that enables TTS models to be trained on\\ndata that more closely mirrors real-world conditions.\\nDatasets for SASV:As SASV is an emerging task extending\\nthe scope of ASV systems with spooﬁng robustness, there is a\\nlack of dedicated datasets for SASV . Earlier studies on SASV\\nhave relied on SDD datasets[34], [35]. However, current SDD\\ndatasets do not prioritize speaker diversity and balance, both\\nof which are critical for SASV . Most datasets also lack a\\nsufﬁcient number of speakers.\\nTo the best of our knowledge, VSASV [30], a parallel\\ndata collection effort to SpoofCeleb, is the only attempt at\\naddressing these limitations by creating a dataset speciﬁ-\\ncally for SASV . SpoofCeleb complements VSASV while also\\nhaving several distinctions. While VSASV includes three\\nspooﬁng attacks, SpoofCeleb contains 23. Although VSASV\\nuses in-the-wild bona ﬁde data, its spoofed data are derived\\nfrom high-quality sources due to the challenges in developing\\nTTS systems with in-the-wild data. In contrast, SpoofCeleb\\nadopts TITW which originates from V oxCeleb1, a widely-\\nused ASV dataset recorded in the wild, as its bona ﬁde\\nsource. Additionally, VSASV includes approximately 300 k\\nsamples, whereas SpoofCeleb offers over 2.5 M samples.\\nVOLUME 6, 2025 69'), Document(metadata={'producer': 'Acrobat Distiller 11.0 (Windows); modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref package', 'creationdate': '2025-02-08T10:21:28+05:30', 'moddate': '2025-02-12T12:23:54-05:00', 'ieee article id': '10839331', 'ieee issue id': '10834412', 'subject': 'IEEE Open Journal of Signal Processing;2025;6; ;10.1109/OJSP.2025.3529377', 'ieee publication id': '8782710', 'title': 'SpoofCeleb: Speech Deepfake Detection and SASV in the Wild', 'source': '/content/Documents/SpoofCeleb_Speech_Deepfake_Detection_and_SASV_in_the_Wild (1).pdf', 'total_pages': 10, 'page': 2, 'page_label': '3'}, page_content='JUNG ET AL.: SPOOFCELEB: SPEECH DEEPFAKE DETECTION AND SASV IN THE WILD\\nTABLE 1. List of Datasets in Speech Deepfake Detection (SDD) and\\nSpooﬁng-Robust Automatic Speaker Veriﬁcation (SASV)\\nTable 1 compares SpoofCeleb with other SDD and SASV\\ndatasets.\\nIII. SOURCE DATASET: TITW\\nOur goal is to create a dataset for SDD and SASV using\\nV oxCeleb1 as the source so that both bona ﬁde and spoofed\\nsamples would reﬂect real-world scenarios. However, V ox-\\nCeleb1 is not suitable for direct use in TTS training.2 The\\nchallenges with V oxCeleb1 are multifaceted. For example,\\nthe speech samples often (i) contain overly emotional ex-\\npressions, (ii) include extended non-speech segments, or (iii)\\nhave excessively long durations. To address these issues, our\\ndeveloped fully automated pipeline processes V oxCeleb1 into\\nthe TITW dataset, which can be used for TTS training.\\nFig. 1(a) illustrates the automated processing pipeline that\\nwas used to generate the TITW dataset. The pipeline begins\\nby transcribing and obtaining word-level alignment using the\\nWhisperX toolkit [36]. This toolkit transcribes the speech\\nusing the pre-trained Whisper Large v2 Automatic Speech\\nRecognition (ASR) model [37], while word-level segmen-\\ntation is derived from another phoneme-based ASR model.\\nFor a small subset of randomly selected samples, we also\\ntranscribe the text using the OWSMv3.1 model[38] and cross-\\ncheck the accuracy of the transcriptions. We then segment the\\nutterances from V oxCeleb1 whenever a silence longer than\\n500 ms is detected, resulting in multiple segments from a\\nsingle utterance. Next, we apply a series of heuristic-driven\\nrules – developed through several iterations of TTS training –\\n2Our preliminary attempts to train TTS systems using the raw V oxCeleb1\\ndata without further processing were unsuccessful.\\nFIGURE 1. Overall process pipeline of SpoofCeleb dataset collection. (a):\\nour proposed fully automated pipeline transcribes, segments, ﬁlters,\\nenhances, and again ﬁlters with DNSMOS to derive TITW-Easy[15] from\\nVoxCeleb1 [14], which is adequate for TTS training. (b): 23 different TTS\\nsystems are trained using TITW-Easy and spoof speech samples are\\ngenerated. All generated spooﬁng samples are combined with TITW-Easy\\nto constitute SpoofCeleb.\\nto ﬁlter the data. We discarded any samples that (i) were non-\\nEnglish, (ii) were shorter than 1 s or longer than 8 seconds,\\n(iii) contained one or more words with a duration exceeding\\n500 ms, or (iv) had empty transcriptions.\\nAfter completing the initial processing steps (referred to\\nas TITW-Hard in[15]), we conducted multiple iterations of\\nTTS training trials. Despite these efforts, training remained\\nextremely challenging for most TTS systems, with only a\\nfew recent models showing success. The generated speech\\nwas still insufﬁcient to deceive pre-trained ASV systems, as\\nmeasured using the SPooF Equal Error Rate (SPF-EER) met-\\nric [13].3 To address this, we applied speech enhancement\\nusing a pre-trained model named DEMUCS and excluded\\nsamples with DNSMOS “BAK” (background noisy qual-\\nity) scores below 3.0. The ﬁnal number of speech segments\\n(TITW-Easy in [15]) is approximately 248 k, which serves\\nas the bona ﬁde portion of the SpoofCeleb dataset. For full\\ndetails on the preparation of TITW from V oxCeleb1, refer\\nto [15]. Nonetheless, we note that this choice of enhancing the\\nbona ﬁde speech may confuse the training of detection models\\nbecause inevitable artifacts can be added with the enhance-\\nment process. Yet, we employ TITW-Easy as the bona ﬁde,\\nnot TITW-Hard, because of the aforementioned practicality.\\nIV. SPOOFCELEB\\nFig. 1(b) illustrates the composition of SpoofCeleb. The\\nTITW dataset serves as the foundation for training multi-\\nple TTS systems. These systems are then used to synthe-\\nsize spoofed speech samples, which are combined with the\\n3The SPF-EER is calculated by assessing an ASV system’s ability to\\ncorrectly accept target trials while rejecting spoofed non-target trials. Bona\\nﬁde non-target trials are excluded from this protocol, as the focus is solely on\\nevaluating the ASV system’s spooﬁng robustness.\\n70 VOLUME 6, 2025'), Document(metadata={'producer': 'Acrobat Distiller 11.0 (Windows); modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref package', 'creationdate': '2025-02-08T10:21:28+05:30', 'moddate': '2025-02-12T12:23:54-05:00', 'ieee article id': '10839331', 'ieee issue id': '10834412', 'subject': 'IEEE Open Journal of Signal Processing;2025;6; ;10.1109/OJSP.2025.3529377', 'ieee publication id': '8782710', 'title': 'SpoofCeleb: Speech Deepfake Detection and SASV in the Wild', 'source': '/content/Documents/SpoofCeleb_Speech_Deepfake_Detection_and_SASV_in_the_Wild (1).pdf', 'total_pages': 10, 'page': 3, 'page_label': '4'}, page_content='bona ﬁde speech samples from TITW to form the complete\\nSpoofCeleb. To achieve this, we use 4 acoustic models, 6\\nwaveform models (i.e., vocoders), and 5 End-to-End (E2E)\\nmodels. Unless mentioned otherwise, all models were trained\\nfrom scratch using the TITW-Easy data. SpoofCeleb does\\nnot include voice conversion systems, as TTS systems pose\\nmore immediate and prevalent security threats with publicly\\navailable APIs. Incorporating voice conversion systems would\\nalso require more complex conﬁgurations, such as deﬁning\\nsource and target speaker pairs. Hence we leave this part for\\nfuture work.\\nA. ACOUSTIC MODELS\\nTraining acoustic models using in-the-wild data was one of\\nthe most challenging aspects of SpoofCeleb creation. We ap-\\nplied several criteria to evaluate the success of the training,\\nincluding (but not limited to) speech intelligibility, measured\\nby the Word Error Rate (WER), noisiness, assessed using\\nDNSMOS, and speaker identity, evaluated using SPF-EER.\\nAmong these metrics, SPF-EER was prioritized as the primary\\nmeasure, since the most critical factor in a spooﬁng attack is\\nwhether it can deceive an ASV system. The ﬁnal models that\\nwere successfully trained include TransformerTTS, GradTTS,\\nMatcha-TTS, and BV AE-TTS.\\nTransformerTTS [39] is an autoregressive TTS model\\nthat generates mel-spectrograms from textual input using a\\ntransformer-based architecture. The model uses a sequence\\nof transformer encoder and decoder blocks with multi-head\\nself-attention. We trained TransformerTTS using the ESPnet\\ntoolkit [40].4\\nGradTTS [41] is a TTS model with a score-based decoder\\nthat generates mel-spectrograms by gradually transforming\\nnoise predicted by the text encoder. During inference, we set\\nthe denoise step to 50 to ensure high-quality speech gener-\\nation. We used the ofﬁcial implementation and followed the\\ndefault settings.5\\nMatcha-TTS [42] is an efﬁcient non-autoregressive TTS\\nmodel based on an optimal-transport conditional ﬂow match-\\ning decoder [1]. Unlike score-based models, it constructs a\\nmore direct sampling trajectory, enabling high-quality gen-\\neration with fewer sampling steps. We used the ofﬁcial\\nimplementation.6\\nBVAE-TTS [43] uses a Bidirectional-inference Variational\\nAutoEncoder (BV AE) to model the hierarchical relation-\\nships between text and speech. By leveraging the atten-\\ntion maps generated using BV AE-TTS, the model jointly\\ntrains a duration predictor, enabling robust and efﬁcient\\nnon-autoregressive speech generation. We used the ofﬁcial\\nimplementation.7\\n4[Online]. Available: https://github.com/ESPnet/ESPnet.\\n5[Online]. Available: https://github.com/huawei-noah/Speech-Backbones.\\n6[Online]. Available: https://github.com/shivammehta25/Matcha-TTS.\\n7[Online]. Available: https://github.com/LEEYOONHYUNG/BV AE-TTS.\\nB. WAVEFORM MODELS\\nThe training of waveform models was comparatively straight-\\nforward. We employed a mix of both classic and recent\\nwaveform models, including DiffWave, HiFiGAN, Parallel\\nWaveGAN, Neural source-ﬁlter model with HiFi-GAN dis-\\ncriminators (NSF-HiFiGAN), BigVGAN, and WaveGlows.\\nDiffWave [44] is a diffusion probabilistic model designed\\nfor both conditional and unconditional waveform generation.\\nWe used the ofﬁcial implementation.8\\nHiFiGAN [45] is a widely known GAN-based wave-\\nform model that uses multiple transposed convolution\\nblocks to progressively upsample and transform input mel-\\nspectrograms into speech waveforms. The generator is opti-\\nmized using multiple discriminator losses, a feature matching\\nloss, and L1 loss between the generated and ground truth mel-\\nspectrograms. We used the HiFiGAN V1 architecture from the\\nofﬁcial implementation.9\\nParallel WaveGAN [46] is a lightweight vocoder model.\\nIt uses a non-autoregressive WaveNet[47] architecture com-\\nbined with multi-resolution Short-Time Fourier Transform\\n(STFT) loss and waveform adversarial loss. We used the ofﬁ-\\ncial implementation.10\\nNSF HiFiGAN [48] is similar to Parallel WaveGAN but\\nexplicitly incorporates a sine-based source signal as input to\\nthe generator. It also includes a noise branch that transforms\\nrandom noise into an aperiodic signal. This aperiodic signal is\\ncombined with the generator’s periodic output for harmonic-\\nplus-noise speech waveform generation. We used the ofﬁcial\\nimplementation.11\\nBigVGAN [49] is a universal GAN-based vocoder that gen-\\neralizes effectively across diverse scenarios, including unseen\\nspeakers, languages, and recording environments. By using\\nperiodic activation functions and anti-aliased representations,\\nBigVGAN introduces a beneﬁcial inductive bias for speech\\nsynthesis. We used the ofﬁcial implementation12\\nWaveGlow [50] generates waveforms through a series of\\nneural network-based invertible afﬁne transformations condi-\\ntioned on input mel-spectrograms. During training, the model\\nparameters are optimized to whiten the ground-truth wave-\\nform as much as possible. We used the same toolkit as with\\nNSF HiFiGAN.\\nC. E2E AND SPEECH-LANGUAGE MODELS WITH NEURAL\\nCODECS\\nWhile two-stage TTS pipelines have proven effective for mod-\\neling speech from text, they often suffer from poor quality\\ndue to the mismatch between acoustic and waveform models.\\nWaveform models are trained on predeﬁned features but must\\n8[Online]. Available: https://github.com/lmnt-com/diffwave.\\n9[Online]. Available: https://github.com/jik876/hiﬁ-gan.\\n10[Online]. Available: https://github.com/kan-bayashi/ParallelWaveGAN.\\n11[Online]. Available: https://github.com/nii-yamagishilab/project-NN-\\nPytorch-scripts.\\n12[Online]. Available: https://github.com/NVIDIA/BigVGAN.\\nVOLUME 6, 2025 71'), Document(metadata={'producer': 'Acrobat Distiller 11.0 (Windows); modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref package', 'creationdate': '2025-02-08T10:21:28+05:30', 'moddate': '2025-02-12T12:23:54-05:00', 'ieee article id': '10839331', 'ieee issue id': '10834412', 'subject': 'IEEE Open Journal of Signal Processing;2025;6; ;10.1109/OJSP.2025.3529377', 'ieee publication id': '8782710', 'title': 'SpoofCeleb: Speech Deepfake Detection and SASV in the Wild', 'source': '/content/Documents/SpoofCeleb_Speech_Deepfake_Detection_and_SASV_in_the_Wild (1).pdf', 'total_pages': 10, 'page': 4, 'page_label': '5'}, page_content='JUNG ET AL.: SPOOFCELEB: SPEECH DEEPFAKE DETECTION AND SASV IN THE WILD\\nprocess the outputs generated by acoustic models during in-\\nference, leading to potential inconsistencies. To address this\\nissue, several E2E models have been proposed, and we have\\nsuccessfully trained multiple E2E models using the TITW\\ndataset.\\nSpeech-Language Models (SpeechLMs) represent an\\nemerging category of TTS models. Similar to language mod-\\nels in natural language processing, they are trained to predict\\ntokens, in this case, tokens of neural codecs, which are then\\ndecoded via a neural codec system’s decoder. Unlike acoustic\\nmodels, which can be paired with any compatible waveform\\nmodel, SpeechLMs rely on a predetermined decoder based on\\nthe neural codec used during training, limiting their ability to\\nfunction with multiple decoders.\\nVALL-E, Multi-Scale Transformer , and Delay:V ALL-E[2]\\npredicts the ﬁrst token of each frame using an autoregressive\\nmodule, followed by a non-autoregressive prediction for the\\nremaining tokens. Multi-Scale Transformer[51] uses a global\\nTransformer for inter-frame modeling and a local Transformer\\nfor intra-frame modeling, maintaining full autoregression\\nwithout approximation. In Delay[52], the multi-stream token\\nsequences are processed using a “delay” interleave pattern,\\nwhich enables approximate autoregressive prediction for both\\ninter- and intra-frame modeling, achieving high efﬁciency.\\nWe used implementations of the three models in the ESPnet\\ntoolkit.4\\nMQTTS [3] is designed to synthesize speech using real-\\nworld data from YouTube and podcasts. To address mis-\\nalignments common in mel-spectrogram-based autoregressive\\nmodels, it uses a multi-codebook vector quantization ap-\\nproach to improve both speech intelligibility and diversity.\\nMQTTS aligns closely with the goals of this work, as we\\naim to develop a dataset that spans real-world data for both\\nbona ﬁde and spoofed speech. We used the ofﬁcial implemen-\\ntation.13\\nVITS [53] is an E2E TTS model that combines a conditional\\nV AE with stochastic duration prediction to generate wave-\\nforms from textual input. The model uses normalizing ﬂow to\\nlearn latent representations from speech, while the stochastic\\nduration predictor captures diverse speech prosody from text.\\nFor waveform generation, adversarial loss is used to produce\\nhigh-quality waveforms from the latent representations. We\\ntrained VITS using the ESPnet toolkit.4\\nD. ATTACK GENERATION, PARTITIONING, AND PROTOCOLS\\nDiverse combinations of acoustic and waveform models,\\nalongside E2E and SpeechLM models, result in a total of\\n23 spooﬁng attacks. This approach is inspired by previous\\nresearch, which demonstrated that both acoustic and wave-\\nform models impact the perceptual quality of synthesized\\nspeech [54]. Table 3 provides a detailed overview of the 23\\nspooﬁng attacks included in SpoofCeleb.\\nData partitioning for SpoofCeleb requires a more sophisti-\\ncated approach compared to existing ASV or SDD datasets.\\n13[Online]. Available: https://github.com/b04901014/MQTTS.\\nTABLE 2. Number of Speech Files and Protocols\\nTABLE 3. Spooﬁng Attacks of SpoofCeleb\\nAn SDD dataset only requires the binary bona ﬁde or spoof\\nlabel, while an ASV dataset focuses on speaker identities.\\nSpoofCeleb, as a dataset for both SDD and SASV , must ac-\\ncount for both bona ﬁde/spoof labels and speaker identities\\nsimultaneously.\\nSpeakers: For the speaker partitioning, we divide the 1,251\\nspeakers in the bona ﬁde data into three sets: 1,171 for train-\\ning, 40 for validation, and 40 for evaluation. This ensures that\\nthere are no overlapping speakers between any of the sets.\\nSpooﬁng attacks: For spooﬁng attacks, we divide the bona\\nﬁde data (A00) and the 23 spooﬁng attacks (A01–A23) as fol-\\nlows. In the training set, 10 attacks (A01 to A10) are combined\\nwith the bona ﬁde data. Among these attacks, six are derived\\nfrom a combination of acoustic and waveform models, while\\nthe remaining four originate from E2E and SpeechLM TTS\\nsystems.\\nIn the validation set, there are 6 attacks: A06, A07, and A11\\nto A14, combined with the bona ﬁde data (A00). Attacks A06\\nand A07 represent known attacks from unknown speakers.\\nAttacks A11 and A12 involve the same architecture as other\\n72 VOLUME 6, 2025'), Document(metadata={'producer': 'Acrobat Distiller 11.0 (Windows); modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref package', 'creationdate': '2025-02-08T10:21:28+05:30', 'moddate': '2025-02-12T12:23:54-05:00', 'ieee article id': '10839331', 'ieee issue id': '10834412', 'subject': 'IEEE Open Journal of Signal Processing;2025;6; ;10.1109/OJSP.2025.3529377', 'ieee publication id': '8782710', 'title': 'SpoofCeleb: Speech Deepfake Detection and SASV in the Wild', 'source': '/content/Documents/SpoofCeleb_Speech_Deepfake_Detection_and_SASV_in_the_Wild (1).pdf', 'total_pages': 10, 'page': 5, 'page_label': '6'}, page_content='FIGURE 2. Illustration of how SpoofCeleb is partitioned.\\nattacks but differ in model training details. Speciﬁcally, A11\\nis fully trained from scratch using the TITW dataset, while\\nin A02, the decoder was pre-trained. Similarly, A12 is fully\\ntrained from scratch on TITW, whereas A04 was pre-trained\\non LibriSpeechGigaSpeech [55] and the English subset of\\nMultilingual LibriSpeech[33], then ﬁne-tuned on TITW. At-\\ntacks A13 and A14 serve as partially known attacks. In A13,\\nthe acoustic model (GradTTS) is known, but the waveform\\nmodel (NSF HiFiGAN) is unknown. Similarly, in A14, the\\nacoustic model (Matcha-TTS) is known, but the waveform\\nmodel (HiFiGAN) is unknown.\\nIn the evaluation set, there are 9 attacks, A15 to A23.\\nAttacks A15 and A16 involve known architectures but differ\\nin conﬁgurations. For A15, the decoder is initialized with a\\npre-trained model, and the speaker embeddings are taken from\\ntarget utterances, simulating a scenario in which an attacker\\nhas access to the target speaker’s utterance pool. A16 was\\npre-trained using the same data composition as A04. Attacks\\nA17 and A18 represent partially known attacks where the\\nacoustic models are known, but the waveform models are not.\\nFinally, A19 to A23 are fully unknown attacks, meaning no\\npart of their models was encountered during training.\\nFig. 2 illustrates the three partitions of SpoofCeleb and\\nTable 2 provides the statistics of each partition. In total,\\nSpoofCeleb contains over 2.5 M speech samples.\\nProtocols: SpoofCeleb includes protocols for validating\\nand evaluating developed SDD and SASV models. The SDD\\nprotocols for validation and evaluation specify the speech\\nsamples to be assessed, while the SASV protocols list pairs of\\ntrials with an enrollment utterance and a test utterance. Table2\\nprovides details on the number of utterances for the SDD\\nprotocols and the number of trials for the SASV protocols.\\nV. BASELINES\\nA. SDD\\nTwo E2E SDD models, RawNet2[56] and AASIST[57],a r e\\nused as the baselines. The RawNet2 model for SDD is an\\nadapted version of RawNet2 originally designed for ASV . It\\nfeatures an input layer that processes raw waveforms directly\\nand uses convolution-based residual blocks. Frame-level rep-\\nresentations are aggregated, projected, then passed through a\\nbinary classiﬁcation head.\\nAASIST is one of the most widely used SDD models in\\nrecent literature. Like RawNet2, it includes an input layer that\\nprocesses raw waveforms and uses convolution-based resid-\\nual blocks. However, unlike RawNet2, AASIST incorporates\\ngraph attention network-based modules designed to capture\\nspectral and temporal spooﬁng artifacts separately. It then uses\\nheterogeneous stacking of graph attention layers to jointly\\nmodel spectral and temporal information concurrently.\\nB. SASV\\nWe employ three models as SASV baselines, all of which\\nuse the SKA-TDNN architecture[58]. These models are used\\nto assess the impact of different training data and scenarios.\\nSKA-TDNN is a convolution-based model with residual con-\\nnections, incorporating dedicated modules and architectural\\ndesign choices for multi-scale processing. It is an advanced\\nversion of the ECAPA-TDNN architecture[59].\\nAmong the three SASV baselines, the ﬁrst model (“Con-\\nventional ASV”) is trained as a conventional ASV system us-\\ning the V oxCeleb1&2 datasets, without considering spoof ro-\\nbustness. We use a pre-trained model from ESPnet-SPK[60].\\nThe second model (“SASV trained on out-of-domain data”) is\\ntrained as an SASV model but uses out-of-domain data from\\nthe ASVspoof2019 logical access dataset[9].W eu s eap r e -\\ntrained model from[61]. The third model (“SASV trained on\\nSpoofCeleb”) is trained as an SASV model using the training\\nset from SpoofCeleb.\\nVI. METRICS\\nA diverse set of metrics is employed to evaluate the\\nSpoofCeleb dataset, as well as the SDD and SASV models.\\nTo assess the quality of the speech samples and the strength\\nof the attacks, we use SPF-EER, Mean Cepstral Distortion\\n(MCD), UTMOS[62], DNSMOS[63], and Word Error Rate\\n(WER), with the WER evaluated using the OpenAI Whisper-\\nLarge model[64]. SPF-EER measures speaker characteristics,\\nUTMOS and DNSMOS are objective approximations of per-\\nceived quality and noisiness of synthesized speech, and WER\\nmeasures intelligibility. For evaluating the performances of\\nthe SDD baselines, we use Equal Error Rate (EER) and the\\nmin Detection Cost Function (minDCF)[65]. To assess the\\nSASV baselines, we adopt the recently proposed architecture-\\nagnostic Detection Cost Function (min a-DCF)[66], along\\nwith Speaker Veriﬁcation EER (SV-EER) and SPooF EER\\n(SPF-EER). Table 5 outlines the trial types involved in the\\nSASV metrics; a-DCF includes all three trial types, while\\nSV-EER and SPF-EER cover only a subset.\\nVII. RESULTS\\nA. SPOOFING ATTACKS\\nTable4 presents various metrics to assess the speech quality of\\nthe 23 synthesized spooﬁng attacks and how effectively they\\nthreaten ASV systems. SPF-EER is the most critical metric,\\nas it measures the extent to which the generated attacks can\\ndeceive existing ASV systems. We evaluated SPF-EER using\\nVOLUME 6, 2025 73'), Document(metadata={'producer': 'Acrobat Distiller 11.0 (Windows); modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref package', 'creationdate': '2025-02-08T10:21:28+05:30', 'moddate': '2025-02-12T12:23:54-05:00', 'ieee article id': '10839331', 'ieee issue id': '10834412', 'subject': 'IEEE Open Journal of Signal Processing;2025;6; ;10.1109/OJSP.2025.3529377', 'ieee publication id': '8782710', 'title': 'SpoofCeleb: Speech Deepfake Detection and SASV in the Wild', 'source': '/content/Documents/SpoofCeleb_Speech_Deepfake_Detection_and_SASV_in_the_Wild (1).pdf', 'total_pages': 10, 'page': 6, 'page_label': '7'}, page_content='JUNG ET AL.: SPOOFCELEB: SPEECH DEEPFAKE DETECTION AND SASV IN THE WILD\\nTABLE 4. Quality and Strength of 23 Spooﬁng Attacks Included in SpoofCeleb\\nTABLE 5. Three Metrics Used for Gauging Performances of SASV Baselines\\na pre-trained RawNet3 model[68], which is publicly available\\nthrough ESPnet-SPK[60].\\nIn the top row, the speech quality evaluations for A00 (bona\\nﬁde speech) are provided as reference values. The results\\nconﬁrm that the spooﬁng attacks in SpoofCeleb are highly\\nthreatening, with most attacks achieving an SPF-EER over\\n20%. The majority of attacks exhibit relatively minor degra-\\ndation in UTMOS and DNSMOS, indicating the high quality\\nof the synthesized speech samples. Intelligibility, measured\\nusing the WER, shows that for most attacks, there is no more\\nthan a 10% deterioration in performance.\\nB. SDD\\nTable 6 presents the results of four baseline SDD sys-\\ntems. We evaluate two SDD models, RawNet2 and AASIST,\\nTABLE 6. SDD Baseline Performances\\ntrained on two different datasets. The models trained on the\\nASVspoof2019 logical access dataset are used to assess the\\nzero-shot performance on validation and evaluation SDD pro-\\ntocols of SpoofCeleb. The other two models demonstrate the\\nperformance of systems trained on in-domain SpoofCeleb\\ntraining data.\\nThe zero-shot results in the top two rows indicate that ex-\\nisting SDD models not trained on in-the-wild data struggle to\\ndistinguish between spoofed samples and bona ﬁde speech.\\nAs shown in rows 3 and 4, there is a signiﬁcant perfor-\\nmance improvement when these models are trained using the\\nSpoofCeleb training set, highlighting the importance of train-\\ning SDD models on in-the-wild data. However, the RawNet2’s\\nresult in row 3 is unexpected, as it shows better performance\\non the evaluation set than on the validation set, while the\\nevaluation set includes totally unknown attacks. To further\\n74 VOLUME 6, 2025'), Document(metadata={'producer': 'Acrobat Distiller 11.0 (Windows); modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref package', 'creationdate': '2025-02-08T10:21:28+05:30', 'moddate': '2025-02-12T12:23:54-05:00', 'ieee article id': '10839331', 'ieee issue id': '10834412', 'subject': 'IEEE Open Journal of Signal Processing;2025;6; ;10.1109/OJSP.2025.3529377', 'ieee publication id': '8782710', 'title': 'SpoofCeleb: Speech Deepfake Detection and SASV in the Wild', 'source': '/content/Documents/SpoofCeleb_Speech_Deepfake_Detection_and_SASV_in_the_Wild (1).pdf', 'total_pages': 10, 'page': 7, 'page_label': '8'}, page_content='TABLE 7. Attack-Wise Performance of RawNet2 SDD Baseline on Validation and Evaluation Sets\\nTABLE 8. SASV Baseline Performances. SKA-TDNN[58] Model Architecture is Employed\\ninvestigate this, we conduct an analysis of the attack-wise\\nresults.\\nTable 7 presents the attack-wise performance of the\\nRawNet2 baseline SDD model trained on the SpoofCeleb\\ntraining set. Attacks A06 and A07 are classiﬁed as known\\nattacks. Attacks A11 to A18 are partially unknown; in these\\ncases, either the acoustic or waveform model is known, or the\\narchitecture is familiar but trained with a different conﬁgura-\\ntion. Attacks A19 to A23 represent entirely unknown attacks.\\nWe found that the inferior performance on attack A11 con-\\ntributed to the validation set results being worse than those\\non the evaluation set. Interestingly, when comparing A11 and\\nA15, attack A15 is more difﬁcult to distinguish for a con-\\nventional ASV system that does not account for spooﬁng,\\nwith SPF-EER values of 47.78% for A11 and 65.21% for\\nA15. Both attacks originate from MQTTS; however, A11 was\\ntrained entirely from scratch, while A15 utilized a pre-trained\\ndecoder. Once an SASV system is trained on the SpoofCeleb\\ntraining data, A11 becomes more challenging to detect. A\\ndeeper investigation into the reasons behind this phenomenon\\nis left for future work.\\nThe comparative analysis in Tables 4 and 7 reveals a\\ndiscrepancy between the rankings of attacks’ SPF-EER on\\nthe pre-trained ASV system trained with V oxCeleb and the\\nrankings of attacks’ EER on the SDD system trained with\\nSpoofCeleb. This divergence may be attributed to the differ-\\nences in training data, whether the models were trained on\\nSpoofCeleb. The discrepancy could be a result of the funda-\\nmental differences in the tasks themselves, as SDD and SASV\\nsystems are optimized for distinct objectives.\\nC. SASV\\nTable 8 presents the performances of three SASV baselines\\non the SpoofCeleb validation and evaluation protocols. Min\\na-DCF assesses the overall performance, while SV-EER and\\nSPF-EER evaluate the systems’ ability to reject bona ﬁde and\\nspoof non-target trials, respectively.\\nAs expected, a conventional ASV system that does not\\naccount for spoof attacks, shown in the ﬁrst row, fails to reject\\nsynthesized speech samples, with an a-DCF exceeding 0.49\\non both the validation and evaluation sets. However, it per-\\nforms well at rejecting bona ﬁde non-target trials. The results\\nin the second row indicate an improvement in a-DCF for the\\nvalidation set, but even worse performance on the evaluation\\nset. Both SV-EER and SPF-EER remain very high, indicating\\nthat the system trained for SASV with out-of-domain data\\nstruggles to reject both types of non-target trials. The a-DCF\\nof 0.9998 also signiﬁes that the model fails to ﬁnd an operat-\\ning point where it can reject both types of non-target trials.\\nFinally, when trained on the SpoofCeleb training data, the\\na-DCF on the evaluation set drops to its lowest value (0.2902),\\nand both SV-EER and SPF-EER are more balanced compared\\nwith row 1, where the system was only capable of rejecting\\nbona ﬁde non-target trials.\\nVIII. CONCLUSION AND REMARKS\\nThis paper introduces SpoofCeleb, a dataset for SDD and\\nSASV based on in-the-wild data. To create a dataset that\\nincorporates real-world conditions, we used a fully automated\\npipeline to process the V oxCeleb1 dataset, making it possible\\nto use it for training TTS systems. We further trained 23\\nTTS systems, partitioning TITW and the TTS systems into\\nSpoofCeleb, which includes training, validation, and evalua-\\ntion sets. Protocols were deﬁned to train and test both SDD\\nand SASV models, and baseline systems for SDD and SASV\\nwere established, trained, and evaluated.\\nWhile there are numerous SDD datasets, many are limited\\nin scale or speaker diversity, which has hindered research\\non single SASV models. We hope SpoofCeleb will serve as\\nthe ﬁrst dataset with enough data to effectively train single\\nSASV systems. Yet, SpoofCeleb has its limitations. In the\\nexperiments, some spooﬁng attacks are shown to be less chal-\\nlenging, as the wild nature of the TITW data complicates the\\ntraining of robust TTS systems. Future work will focus on\\nadvancing TTS training techniques that can better leverage\\nthis challenging in-the-wild data.\\nREFERENCES\\n[1] Y . Lipman et al., “Flow matching for generative modeling,” inProc. Int.\\nConf. Learn. Representations, 2023.\\nVOLUME 6, 2025 75'), Document(metadata={'producer': 'Acrobat Distiller 11.0 (Windows); modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref package', 'creationdate': '2025-02-08T10:21:28+05:30', 'moddate': '2025-02-12T12:23:54-05:00', 'ieee article id': '10839331', 'ieee issue id': '10834412', 'subject': 'IEEE Open Journal of Signal Processing;2025;6; ;10.1109/OJSP.2025.3529377', 'ieee publication id': '8782710', 'title': 'SpoofCeleb: Speech Deepfake Detection and SASV in the Wild', 'source': '/content/Documents/SpoofCeleb_Speech_Deepfake_Detection_and_SASV_in_the_Wild (1).pdf', 'total_pages': 10, 'page': 8, 'page_label': '9'}, page_content='JUNG ET AL.: SPOOFCELEB: SPEECH DEEPFAKE DETECTION AND SASV IN THE WILD\\n[2] C. Wang et al., “Neural codec language models are zero-shot text to\\nspeech synthesizers,” 2023,arXiv:2301.02111.\\n[3] L.-W. Chen, S. Watanabe, and A. Rudnicky, “A vector quantized\\napproach for text to speech synthesis on real-world spontaneous\\nspeech,” in Proc. AAAI Conf. Artif. Intell. , 2023, vol. 37, no. 11,\\npp. 12644–12652.\\n[4] 2016. Accessed: Sep. 16, 2024. [Online]. Available: https://www.\\nresemble.ai/api/\\n[5] 2024.Accessed: Sep. 16, 2024. [Online]. Available: https://voice.ai/\\nvoice-cloning\\n[6] “A voice deepfake was used to scam a CEO out of 243,000,” Accessed:\\nSep. 16, 2024. [Online]. Available: https://www.forbes.com/sites/\\njessedamiani/2019/09/03/a-voice-deepfake-was-used-to-scam-a-ceo-\\nout-of-243000/\\n[7] “Fake Joe Biden robocall tells New Hampshire democrats not\\nto vote tuesday,” Accessed: Sep. 16, 2024. [Online]. Available:\\nhttps://nbcnews.com/politics/2024-election/fake-joe-biden-robocall-\\ntells-new-hampshire-democrats-not-vote-tuesday-rcna134984\\n[8] J.-w. Jung et al., “To what extent can ASV systems naturally defend\\nagainst spooﬁng attacks?,” inProc. Annu. Conf. Int. Speech Commun.\\nAssoc., 2024, pp. 3240–3244.\\n[9] X. Wang et al., “ASVspoof 2019: A large-scale public database of\\nsynthesized, converted and replayed speech,”Comput. Speech Lang.,\\nvol. 64, 2020, Art. no. 101114.\\n[10] Z. Wu et al., “SAS: A speaker veriﬁcation spooﬁng database contain-\\ning diverse attacks,” inProc. IEEE Int. Conf. Acoust., Speech, Signal\\nProcess., 2015, pp. 4440–4444.\\n[11] Z. Wu et al., “ASVspoof 2015: The ﬁrst automatic speaker veriﬁcation\\nspooﬁng and countermeasures challenge,” in Proc. Annu. Conf. Int.\\nSpeech Commun. Assoc., 2015, pp. 2037–2041.\\n[12] J. Yi et al., “ADD 2022: The ﬁrst audio deep synthesis detection chal-\\nlenge,” inProc. IEEE Int. Conf. Acoust., Speech, Signal Process., 2022,\\npp. 9216–9220.\\n[13] J. W. Jung et al., “SASV 2022: The ﬁrst spooﬁng-aware speaker veri-\\nﬁcation challenge,” inProc. Annu. Conf. Int. Speech Commun. Assoc.,\\n2022, pp. 2893–2897.\\n[14] A. Nagrani, J. S. Chung, and A. Zisserman, “V oxCeleb: A large-scale\\nspeaker identiﬁcation dataset,” inProc. Annu. Conf. Int. Speech Com-\\nmun. Assoc., 2017, pp. 2616–2620.\\n[15] J.-W. Jung et al., “Text-to-speech synthesis in the wild,” 2024,\\narXiv:2409.08711.\\n[16] H. Khalid et al., “FakeA VCeleb: A novel audio-video multimodal deep-\\nfake dataset,” inProc. NeurIPS Syst. Datasets Benchmarks Track, 2021.\\n[17] N. Müller et al., “Does audio deepfake detection generalize?” inProc.\\nAnnu. Conf. Int. Speech Commun. Assoc., 2022, pp. 2783–2787.\\n[18] X. Tian et al., “Spooﬁng detection under noisy conditions: A prelimi-\\nnary investigation and an initial database,” 2016,arXiv:1602.02950.\\n[19] J. Yamagishi et al., “ASVspoof 2021: Accelerating progress in spoofed\\nand deepfake speech detection,” in Proc. ASVspoof 2021 Workshop\\n(Interspeech 2021 Satell.), 2021, pp. 47–54.\\n[20] X. Wang and J. Yamagishi, “Spoofed training data for speech spooﬁng\\ncountermeasure can be efﬁciently created using neural vocoders,” in\\nProc. IEEE Int. Conf. Acoust., Speech, Signal Process., 2023, pp. 1–5.\\n[21] L. Zhang, X. Wang, E. Cooper, N. Evans, and J. Yamagishi, “The\\npartialspoof database and countermeasures for the detection of short\\nfake speech segments embedded in an utterance,”IEEE/ACM Trans.\\nAudio, Speech, Lang. Process., vol. 31, pp. 813–825, 2023.\\n[22] J. Frank and L. Schönherr, “WaveFake: A data set to facilitate audio\\ndeepfake detection,” inProc. Int. Conf. Adv. Neural Inf. Process. Syst.,\\n2021.\\n[23] J. Yi et al., “ADD 2023: The second audio deepfake detection chal-\\nlenge,” inProc. IJCAI Workshop Deepfake Audio Detection Anal., 2023.\\n[24] J. Yi et al., “Half-truth: A partially fake audio detection dataset,” in\\nProc. Annu. Conf. Int. Speech Commun. Assoc., 2021, pp. 1654–1658.\\n[25] H. Ma et al., “CFAD: A. Chinese dataset for fake audio detection,”\\nSpeech Commun., vol. 164, 2024, Art. no. 103122.\\n[26] X. Wang et al., “ASVspoof 5: Crowdsourced speech data, deepfakes,\\nand adversarial attacks at scale,” inProc. ASVspoof 5 Workshop (Inter-\\nspeech 2024 Satell.), 2024.\\n[27] N. M. Müller et al., “MLAAD: The multi-language audio anti-spooﬁng\\ndataset,” inProc. Int. Joint Conf. Neural Netw., 2024.\\n[28] Z. Zhang et al., “FMFCC-A: A challenging Mandarin dataset for syn-\\nthetic speech detection,” inProc. Int. Workshop Digit. Watermarking,\\n2021, pp. 117–131.\\n[29] R. Reimao and V . Tzerpos, “FoR: A dataset for synthetic speech de-\\ntection,” inProc. Int. Conf. Speech Technol. Hum.-Comput. Dialogue,\\n2019, pp. 1–10.\\n[ 3 0 ] V .H o a n g ,V .T .P h a m ,H .N .X u a n ,N .P h a m ,P .D a t ,a n dT .T .T .\\nNguyen, “VSASV: A Vietnamese dataset for spooﬁng-aware speaker\\nveriﬁcation,” inProc. Annu. Conf. Int. Speech Commun. Assoc., 2024,\\npp. 4288–4292.\\n[31] H. Tak, M. Kamble, J. Patino, M. Todisco, and N. Evans, “RawBoost:\\nA raw data boosting and augmentation method applied to automatic\\nspeaker veriﬁcation anti-spooﬁng,” in Proc. IEEE Int. Conf. Acoust.,\\nSpeech, Signal Process., 2022, pp. 6382–6386.\\n[32] J. Kominek and A. W. Black, “The CMU arctic speech databases,” in\\nProc. 5th ISCA Workshop Speech Synth., 2004, pp. 223–224.\\n[33] V . Pratap et al., “MLS: A large-scale multilingual dataset for speech\\nresearch,” in Proc. Annu. Conf. Int. Speech Commun. Assoc., 2020,\\npp. 2757–2761.\\n[34] M. Todisco et al., “Integrated presentation attack detection and au-\\ntomatic speaker veriﬁcation: Common features and Gaussian back-\\nend fusion,” in Proc. Annu. Conf. Int. Speech Commun. Assoc. ,\\n2018.\\n[35] H.-j. Shim et al., “Integrated replay spooﬁng-aware text-independent\\nspeaker veriﬁcation,”Appl. Sci., vol. 10, no. 18, 2020, Art. no. 6292.\\n[36] M. Bain et al., “WhisperX: Time-accurate speech transcription of long-\\nform audio,” inProc. Annu. Conf. Int. Speech Commun. Assoc., 2023,\\npp. 4489–4493.\\n[37] A. Radford et al., “Robust speech recognition via large-scale weak\\nsupervision,” inProc. Int. Conf. Mach. Learn., 2023, pp. 28492–28518.\\n[38] Y . Peng et al., “OWSM v3.1: Better and faster open whisper-style\\nspeech models based on E-branchformer,” inProc. Annu. Conf. Int.\\nSpeech Commun. Assoc., 2024, pp. 352–356.\\n[39] N. Li et al., “Neural speech synthesis with transformer network,” in\\nProc. AAAI Conf. Artif. Intell., 2019, vol. 33, no. 01, pp. 6706–6713.\\n[40] S. Watanabe et al., “ESPNet: End-to-end speech processing toolkit,” in\\nProc. Annu. Conf. Int. Speech Commun. Assoc., 2018, pp. 2207–2211.\\n[41] V . Popov, I. V ovk, V . Gogoryan, T. Sadekova, and M. A. Kudinov,\\n“Grad-TTS: A diffusion probabilistic model for text-to-speech,” in\\nProc. Int. Conf. Mach. Learn., 2021, pp. 8599–8608.\\n[42] S. Mehta, R. Tu, J. Beskow, É. Székely, and G. E. Henter, “Matcha-\\nTTS: A fast TTS architecture with conditional ﬂow matching,”\\nin Proc. IEEE Int. Conf. Acoust., Speech, Signal Process. , 2024,\\npp. 11341–11345.\\n[43] Y . Lee, J. Shin, and K. Jung, “Bidirectional variational inference for\\nnon-autoregressive text-to-speech,” inProc. Int. Conf. Learn. Represen-\\ntations, 2021.\\n[44] Z. Kong et al., “Diffwave: A versatile diffusion model for audio synthe-\\nsis,” inProc. Int. Conf. Learn. Representations, 2020.\\n[45] J. Kong, J. Kim, and J. Bae, “HiFi-GAN: Generative adversarial net-\\nworks for efﬁcient and high ﬁdelity speech synthesis,” inProc. Int.\\nConf. Adv. Neural Inf. Process. Syst., 2020, pp. 17022–17033.\\n[46] R. Yamamoto, E. Song, and J.-M. Kim, “Parallel WaveGAN: A fast\\nwaveform generation model based on generative adversarial networks\\nwith multi-resolution spectrogram,” inProc. IEEE Int. Conf. Acoust.,\\nSpeech, Signal Process., 2020, pp. 6199–6203.\\n[47] A. Van Den Oord et al., “WaveNet: A generative model for raw audio,”\\n2016, arXiv:1609.03499.\\n[48] X. Wang, S. Takaki, and J. Yamagishi, “Neural source-ﬁlter waveform\\nmodels for statistical parametric speech synthesis,”IEEE/ACM Trans.\\nAudio, Speech, Lang. Process., vol. 28, pp. 402–415, 2020.\\n[49] S.-G. Lee et al., “BigVGAN: A universal neural vocoder with large-\\nscale training,” inProc. Int. Conf. Learn. Representations, 2023.\\n[50] R. Prenger, R. Valle, and B. Catanzaro, “WaveGlow: A ﬂow-based gen-\\nerative network for speech synthesis,” inProc. IEEE Int. Conf. Acoust.,\\nSpeech, Signal Process., 2019, pp. 3617–3621.\\n[51] D. Yang et al., “UniAudio: Towards universal audio generation with\\nlarge language models,” in Proc. Int. Conf. Mach. Learn. , 2024,\\npp. 56422–56447.\\n[52] J. Copet et al., “Simple and controllable music generation,” inProc. Int.\\nConf. Adv. Neural Inf. Process. Syst., 2023, vol. 36, pp. 47704–47720.\\n[53] J. Kim, J. Kong, and J. Son, “Conditional variational autoencoder with\\nadversarial learning for end-to-end text-to-speech,” inProc. Int. Conf.\\nMach. Learn., 2021, pp. 5530–5540.\\n[54] O. Watts, G. Eje Henter, J. Fong, and C. Valentini-Botinhao, “Where do\\nthe improvements come from in sequence-to-sequence neural TTS?,” in\\nProc. Speech Synth. Workshop, 2019, pp. 217–222.\\n76 VOLUME 6, 2025'), Document(metadata={'producer': 'Acrobat Distiller 11.0 (Windows); modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref package', 'creationdate': '2025-02-08T10:21:28+05:30', 'moddate': '2025-02-12T12:23:54-05:00', 'ieee article id': '10839331', 'ieee issue id': '10834412', 'subject': 'IEEE Open Journal of Signal Processing;2025;6; ;10.1109/OJSP.2025.3529377', 'ieee publication id': '8782710', 'title': 'SpoofCeleb: Speech Deepfake Detection and SASV in the Wild', 'source': '/content/Documents/SpoofCeleb_Speech_Deepfake_Detection_and_SASV_in_the_Wild (1).pdf', 'total_pages': 10, 'page': 9, 'page_label': '10'}, page_content='[55] G. Chen et al., “GigaSpeech: An evolving, multi-domain ASR corpus\\nwith 10,000 hours of transcribed audio,” in Proc. Annu. Conf. Int.\\nSpeech Commun. Assoc., 2021, pp. 3670–3674.\\n[56] H. Tak, J. Patino, M. Todisco, A. Nautsch, N. Evans, and A. Larcher,\\n“End-to-end anti-spooﬁng with RawNet2,” in Proc. IEEE Int. Conf.\\nAcoust., Speech, Signal Process., 2021, pp. 6369–6373.\\n[57] J.-W. Jung et al., “AASIST: Audio anti-spooﬁng using integrated\\nspectro-temporal graph attention networks,” inProc. IEEE Int. Conf.\\nAcoust., Speech, Signal Process., 2022, pp. 6367–6371.\\n[58] S. H. Mun, J.-W. Jung, M. H. Han, and N. S. Kim, “Frequency and\\nmulti-scale selective kernel attention for speaker veriﬁcation,” inProc.\\nIEEE Spoken Lang. Technol. Workshop, 2023, pp. 548–554.\\n[59] B. Desplanques, J. Thienpondt, and K. Demuynck, “ECAPA-TDNN:\\nEmphasized channel attention, propagation and aggregation in TDNN\\nbased speaker veriﬁcation,” inProc. Annu. Conf. Int. Speech Commun.\\nAssoc., 2020, pp. 3830–3834.\\n[60] J.-w. Jung et al., “ESPNet-SPK: Full pipeline speaker embedding\\ntoolkit with reproducible recipes, self-supervised front-ends, and off-\\nthe-shelf models,” inProc. Annu. Conf. Int. Speech Commun. Assoc.,\\n2024, pp. 4278–4282.\\n[61] S. H. Mun et al., “Towards single integrated spooﬁng-aware speaker\\nveriﬁcation embeddings,” inProc. Annu. Conf. Int. Speech Commun.\\nAssoc., 2023, pp. 3989–3993.\\n[62] T. Saeki et al., “UTMOS: UTokyo-SaruLab system for V oiceMOS chal-\\nlenge 2022,” inProc. Annu. Conf. Int. Speech Commun. Assoc., 2022,\\npp. 4521–4525.\\n[63] C. K. A. Reddy, V . Gopal, and R. Cutler, “DNSMOS: A non-intrusive\\nperceptual objective speech quality metric to evaluate noise suppres-\\nsors,” inProc. IEEE Int. Conf. Acoust., Speech, Signal Process., 2021,\\npp. 6493–6497.\\n[64] A. Radford et al., “Robust speech recognition via large-scale weak\\nsupervision,” inProc. Int. Conf. Mach. Learn., 2023, pp. 28492–28518.\\n[65] “NIST 2016 speaker recognition evaluation plan.” Accessed:\\nSep. 16, 2024. [Online]. Available: https://www.nist.gov/system/\\nﬁles/documents/2016/10/07/sre16_eval_plan_v1.3.pdf\\n[66] H.-J. Shim et al., “A-DCF: An architecture agnostic metric with applica-\\ntion to spooﬁng-robust speaker veriﬁcation,” inProc. Speaker Odyssey,\\n2024, pp. 158–164.\\n[67] J. S. Chung, A. Nagrani, and A. Zisserman, “V oxceleb2: Deep speaker\\nrecognition,” inProc. Annu. Conf. Int. Speech Commun. Assoc., 2018,\\npp. 1086–1090.\\n[68] J.-W. Jung et al., “Pushing the limits of raw waveform speaker recog-\\nnition,” in Proc. Annu. Conf. Int. Speech Commun. Assoc. , 2022,\\npp. 2228–2232.\\nVOLUME 6, 2025 77'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-02T00:32:57+00:00', 'author': 'Author name(s) withheld', 'keywords': '', 'moddate': '2023-06-02T00:32:57+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Submitted to INTERSPEECH', 'trapped': '/False', 'source': '/content/Documents/Towards Single Integrated Spoofing-Aware Speaker Verification Embeddings.pdf', 'total_pages': 5, 'page': 0, 'page_label': '1'}, page_content='Towards single integrated spoofing-aware speaker verification embeddings\\nSung Hwan Mun1,∗, Hye-jin Shim2,∗, Hemlata Tak3,∗, Xin Wang4, Xuechen Liu2,5, Md Sahidullah6,\\nMyeonghun Jeong1, Min Hyun Han1, Massimiliano Todisco3, Kong Aik Lee7, Junichi Yamagishi4,\\nNicholas Evans3, Tomi Kinnunen2, Nam Soo Kim1, and Jee-weon Jung8,†\\n1Seoul National University, South Korea,2University of Eastern Finland, Finland,\\n3EURECOM, France, 4National Institute of Informatics, Japan, 5Inria, France, 6TCG CREST, India,\\n7Institute for Infocomm Research, A*STAR, Singapore, 8Carnegie Mellon University, USA\\nshmun@hi.snu.ac.kr, hyejin.shim@uef.fi, tak@eurecom.fr, jeeweonj@andrew.cmu.edu\\nAbstract\\nThis study aims to develop a single integrated spoofing-aware\\nspeaker verification (SASV) embeddings that satisfy two as-\\npects. First, rejecting non-target speakers’ input as well as\\ntarget speakers’ spoofed inputs should be addressed. Second,\\ncompetitive performance should be demonstrated compared to\\nthe fusion of automatic speaker verification (ASV) and counter-\\nmeasure (CM) embeddings, which outperformed single embed-\\nding solutions by a large margin in the SASV2022 challenge.\\nWe analyze that the inferior performance of single SASV em-\\nbeddings comes from insufficient amount of training data and\\ndistinct nature of ASV and CM tasks . To this end, we pro-\\npose a novel framework that includes multi-stage training and\\na combination of loss functions. Copy synthesis, combined\\nwith several vocoders, is also exploited to address the lack of\\nspoofed data. Experimental results show dramatic improve-\\nments, achieving an SASV-EER of 1.06% on the evaluation\\nprotocol of the SASV2022 challenge.\\nIndex Terms: spoofing-aware speaker verification, speaker ver-\\nification, anti-spoofing\\n1. Introduction\\nWhile today’s state-of-the-art automatic speaker verification\\n(ASV) systems [1,2] offer exceptional reliability across diverse\\nscenarios and domains [3, 4], their vulnerabilities to spoofing\\nand deepfake attacks are well acknowledged. Such malicious\\nattacks can be generated using text-to-speech (TTS) and voice\\nconversion (VC) algorithms, for instance, in order to deceive an\\nASV system and provoke an increased rate of false positives.\\nAs a result, the development of spoofing and deepfake detec-\\ntion systems, known as countermeasures (CMs), has attracted\\nconsiderable research interest in recent years [5–7].\\nThe majority of CM solutions take the form of binary clas-\\nsifiers. Their role is to protect the ASV system by determining\\nwhether a given utterance is bona fide (i.e., genuine) or spoofed.\\nWhile the development of such separate CM and ASV sub-\\nsystems is common to the majority of related research, the re-\\ncent Spoofing-Aware Speaker Verification (SASV) initiative [8]\\nwas formed in 2022 to promote the exploration of jointly opti-\\nmized CM and ASV solutions as well as integrated approaches\\nwhereby the same tasks are performed by a single classifier.\\nThe initiative was well supported and attracted the registra-\\ntion of 53 participants. While a majority of participants utilized\\nfusion-based approaches, encouragingly, some participants ex-\\nplored end-to-end (E2E) approaches and the training of single,\\nintegrated systems [9–11]. E2E approaches involve the training\\n∗Equal contribution.\\n† Work done while author was in Naver Corporation, South Korea.\\nof an SASV model to extractSASV embeddings directly (similar\\nto speaker embeddings for an ASV task) without optimization\\nof separate ASV and CM models. These, however, significantly\\nunder-perform solutions based upon the fusion of independent,\\npre-trained CM and ASV sub-systems at embedding, score, or\\ndecision levels [12–17].\\nEven if their performance is currently behind that of their\\nmore traditional counterparts, single, integrated systems are not\\nwithout appeal. A single classifier might be more efficient than\\ntwo, depending on their relative complexity. Both CMs and\\nASV subsystems have potential to deflect spoofing attacks [18],\\nand jointly-optimized solutions are gaining ground on separate,\\nindependently optimized alternatives [14,19–21]. Single classi-\\nfier solutions might also help to avoid the more costly joint op-\\ntimization or adaptation of dual classifier solutions which might\\nbe applied periodically to protect reliability in the face of newly\\nemerging threats. Finally, given that they have received far less\\nresearch attention than their dual classifier counterparts, it is\\nsensible to assume that there might be as-yet untapped poten-\\ntial to improve the competitiveness of single classifier solutions\\nwith further research investment. We believe that single, inte-\\ngrated solutions still merit attention.\\nTo this end, we propose a novel multi-stage framework\\nfor training single SASV embedding extractors. The frame-\\nwork employs different stages to leverage vast amounts of\\ndata, including V oxCeleb2 [22] as well as copy synthesis (CS)\\ndata [23]. Furthermore, considering the characteristics of SASV\\ntask, involving two different tasks in nature, delicate optimiza-\\ntion using diverse combinations of loss functions is explored\\nto go beyond the simple binary classification. We conducted\\nextensive experiments using two recent speaker embedding ex-\\ntractors, namely a selective kernel attention-based time delay\\nneural network (SKA-TDNN) [24] and a multi-scale feature\\naggregation conformer (MFA-Conformer) [3]. Together, these\\ncontributions narrow significantly the performance gap between\\nembedding-level, fusion-based and single, integrated SASV so-\\nlutions. 1.\\n2. Proposed SASV framework\\nPrevious studies in SASV have shown the improvements to per-\\nformance that can be found from using large-scale datasets [8,\\n12, 16, 17, 20]. However, while bona fide data is available in\\nabundance, spoofed data is comparatively scarce. New tech-\\nniques are needed to compensate for this data imbalance while\\nstill exploiting the volume of bona fide data needed to support\\nreliable ASV . We propose a multi-stage training scheme for the\\nextraction of SASV embeddings and the use of appropriate di-\\nverse loss functions.\\n1Code and models are available in https://github.com/\\nsasv-challenge/ASVSpoof5-SASVBaseline.\\narXiv:2305.19051v2  [eess.AS]  1 Jun 2023'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-02T00:32:57+00:00', 'author': 'Author name(s) withheld', 'keywords': '', 'moddate': '2023-06-02T00:32:57+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Submitted to INTERSPEECH', 'trapped': '/False', 'source': '/content/Documents/Towards Single Integrated Spoofing-Aware Speaker Verification Embeddings.pdf', 'total_pages': 5, 'page': 1, 'page_label': '2'}, page_content='First, the ability to discriminate between target and bona\\nfide non-target speakers can be learned using the V oxCeleb2\\ndatabase which contains data collected from thousands of bona\\nfide speakers. Second, we augment the model with the ability\\nto discriminate between bona fide and spoofed inputs by us-\\ning large-scale data generated through an oracle speech synthe-\\nsis system, referred to as copy synthesis [25]. Third, we fine-\\ntune the model using the ASVspoof 2019 Logical Access (LA)\\ndatabase which contains a mix of bona fide and spoofed utter-\\nances. Figure 1 illustrates the multi-training scheme.\\n2.1. Stage 1: Speaker classification-based pre-training\\nIn Stage 1, pre-training is performed using V oxCeleb2 so that\\nthe model learns to distinguish between target and non-target\\ntrials, both of which are bona fide (i.e., conventional ASV). We\\noptimize the model using the equal-weighted summation of ad-\\nditive angular margin (AAM) softmax loss [26] and contrastive\\nloss functions. The combination of two functions has syner-\\ngistic effects, resulting in better performance than when using\\neither alone [24, 27, 28].\\nGiven a mini-batch of SASV embeddings, xi,1 and xi,2,\\nwhere xi,k denotes a SASV embedding extracted from the\\nkth ∈ [1, 2] utterance of the ith ∈ [1, . . . , Cspk] speaker, the\\ncontrastive loss function is defined as follows:\\nLasv\\ncont = − 1\\nN\\nNX\\ni\\nlog exp(f(xi,1, xi,2))PN\\nj exp(f(xi,1, xj,2))\\n, (1)\\nwhere N is the number of speakers within a mini-batch and\\nf(·) =α cos(·) +β is a cosine similarity function between two\\nvectors with trainable scale α and bias β parameters. Given a\\ntarget speaker label yi,k = i ∈ [1, . . . , Cspk] corresponding to\\nxi,k, the AAM softmax loss is defined as:\\nLasv\\naam = AAMsoftmax([xi,k, yi,k]∀(i,k); Cspk) (2)\\n= − 1\\n2N\\n2NX\\ni,k\\nlog e\\ns cos(θyi,k,i+m)\\ne\\ns cos(θyi,k,i+m)\\n+ P\\nj̸=yc\\ni,k\\nes cos(θj,i)\\n, (3)\\nwhere s is a scale factor, m is a margin, cos(θj,i) is the normal-\\nized dot product between the weight for the jth ∈ [1, .., Cspk]\\nclass and the input SASV embedding xi,k. Finally, the ASV\\nloss function for Stage 1 is defined as Lasv = Lasv\\ncont + Lasv\\naam.\\n2.2. Stage 2: copy synthesis training with adapted SASV\\nloss functions\\nThe training of CMs also requires large databases of both\\nspoofed and bona fide data. Compared to abundant bona fide\\ndata, generating spoofed data remains technically demanding\\nand time-consuming. 2 Nevertheless, generating spoofed data\\nwith a wide variety of different algorithms is essential for gener-\\nalization, namely the reliable detection performance in the face\\nof unseen spoofing attacks. Greater quantities, more on par with\\nthat of bona fide data, may also help to avoid the impact of data\\nimbalance. Moreover, no systematic study has investigated the\\nloss function which is most appropriate for SASV , even though\\nthe training objective is critical. To this end, we use copy syn-\\nthesis (CS) as a data augmentation to enlarge the volume of\\nspoofed data and compare different loss functions.\\n2To give an idea of the scale, the creation of the ASVspoof 2019\\ndataset [6] required more than 6 months of intensive effort from several\\ncontributors.\\n𝐱𝐱1,1\\nbna\\n 𝐱𝐱1,2\\nbna\\n𝐱𝐱2,1\\nbna\\n 𝐱𝐱2,2\\nbna\\n𝐱𝐱3,1\\ncs3\\n 𝐱𝐱3,2\\ncs3\\n𝐱𝐱2,1\\ncs2\\n 𝐱𝐱2,2\\ncs2\\n𝐱𝐱3,1\\nbna\\n 𝐱𝐱3,2\\nbna\\n𝐱𝐱1,1\\ncs1\\n 𝐱𝐱1,2\\ncs1\\ncs3\\ncs2\\ncs1\\nCopy-synthesis\\nSASV\\nencoder\\nVocoder\\nbna spk1\\nℒcont\\nsasv+ℒid\\nsasv\\n𝐱𝐱1,1\\nbna\\n 𝐱𝐱1,2\\nbna\\n𝐱𝐱2,1\\nbna\\n 𝐱𝐱2,2\\nbna\\n𝐱𝐱1,1\\nspf\\n 𝐱𝐱1,2\\nspf\\n𝐱𝐱2,1\\nspf\\n 𝐱𝐱2,2\\nspf\\n𝐱𝐱3,1\\nbna\\n 𝐱𝐱3,2\\nbna\\n𝐱𝐱3,1\\nspf\\n 𝐱𝐱3,2\\nspf\\nSASV\\nencoder\\nspoof1\\nspoof2\\nspoof3\\nASVspoof 2019 LA\\nbna spk2\\nbna spk3\\nStage 3 \\nStage 2 \\nInitialized w/ pre-trained weights from Stage 2\\nStage 1 \\nwav1,1\\nbna\\n wav1,2\\nbna\\nwav2,1\\nbna\\n wav2,2\\nbna\\nwav3,1\\nbna\\n wav3,2\\nbna\\n𝐱𝐱1,1\\nbna\\n 𝐱𝐱1,2\\nbna\\n𝐱𝐱2,1\\nbna\\n 𝐱𝐱2,2\\nbna\\n𝐱𝐱3,1\\nbna\\n 𝐱𝐱3,2\\nbna\\nSASV\\nencoderbna spk1\\nbna spk2\\nbna spk3\\nVoxCeleb2\\nInitialized w/ pre-trained weights from Stage 1\\nwav1,1\\nbna\\n wav1,2\\nbna\\nwav2,1\\nbna\\n wav2,2\\nbna\\nwav3,1\\nbna\\n wav3,2\\nbna\\nwav1,1\\nspf\\n wav1,2\\nspf\\nwav2,1\\nspf\\n wav2,2\\nspf\\nwav3,1\\nspf\\n wav3,2\\nspf\\nwav1,1\\nbna\\n wav1,2\\nbna\\nwav2,1\\nbna\\n wav2,2\\nbna\\nwav3,1\\nbna\\n wav3,2\\nbna\\nwav3,1\\ncs3\\n wav3,2\\ncs3\\nwav2,1\\ncs2\\n wav2,2\\ncs2\\nwav1,1\\ncs1\\n wav1,2\\ncs1\\nℒcont\\nsasv+ℒid\\nsasv ℒcont\\nasv+ℒaamasv\\nTraining from scratch\\nFigure 1: Proposed SASV framework with three training Stages\\nand diverse loss functions. wav bna\\ni,j and wavspf\\ni,j denote bona fide\\nand spoof waveform of i-th speaker’s j-th utterance. CS k de-\\nnotes the copy synthesis spoofed data from k-th speaker.\\nUtilization of copy synthesis data. Recently, an efficient\\nmeans to generate massive databases for spoofing CM training\\nwas proposed in [23]. It uses neural network-based vocoders\\nand a CS technique to generate spoofs from bona fide utterances\\nusing oracle acoustic features (e.g. F0 and mel spectrogram rep-\\nresentations), and speaker embeddings (e.g. x-vectors) [29, 30].\\nSimilar ideas have been reported previously [29–34], even\\nthough most of the previous studies use traditional non-neural\\nvocoders. Adopting the same idea, we use the data gener-\\nated via CS from V oxCeleb2 and ASVspoof 2019 LA training\\ndatasets. We employ the same four neural vocoders as described\\nin [23]: HiFiGAN [35]; WaveGlow [36]; a neural source-filter\\nmodel (NSF) [37]; a NSF model with a HiFiGAN discrimina-\\ntor. This set of vocoders includes flow, generative adversar-\\nial network (GAN), and differentiable digital signal processing\\ntechniques [38], and can generate waveforms at real-time speed,\\nmaking them ideally suited to CS using large datasets.\\nAdaptive contrastive loss for SASV .The contrastive loss func-\\ntion, which is widely used in speaker verification [39] and also\\nin Stage 1, maximizes speaker discrimination in the embeddings\\ncorresponding to different speakers. However, in the case of\\na spoof-aware scenario, this may affect (degrade) performance\\nsince CM labels (i.e., spoof, target bona fide) are not taken into\\naccount. Therefore we explore a contrastive loss function for\\nthe training of a single, integrated SASV model. Given a mini-\\nbatch of SASV embeddings, xc\\ni,1 and xc\\ni,2, where xc\\ni,k denotes\\na SASV embedding extracted from the kth ∈ [1, 2] utterance of\\nthe ith ∈ [1, .., Cspk] speaker with CM label c ∈ [bna, spf]3, the\\nSASV contrastive loss is defined as follows:\\nLsasv\\ncont = − 1\\nNbna\\nNbnaX\\ni\\nlog exp(f(xbna\\ni,1, xbna\\ni,2))PN\\nj,c exp(f(xbna\\ni,1, xc\\nj,2))\\n, (4)\\nwhere Nbna is the number of bona fide speakers within a mini-\\n3The superscript or subscript ‘bna’ and ‘spf’ denote bona fide and\\nspoof data, respectively.'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-02T00:32:57+00:00', 'author': 'Author name(s) withheld', 'keywords': '', 'moddate': '2023-06-02T00:32:57+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Submitted to INTERSPEECH', 'trapped': '/False', 'source': '/content/Documents/Towards Single Integrated Spoofing-Aware Speaker Verification Embeddings.pdf', 'total_pages': 5, 'page': 2, 'page_label': '3'}, page_content='batch and where N = Nbna + Nspf indicates the total num-\\nber of bona fide and spoof speakers within a mini-batch. To\\nfocus on the differences in artefacts stemming from vocoding,\\nas opposed to other sources of variation (e.g., speakers, pho-\\nnetic characteristics, etc.), we include both bona fide utterances\\nand their copy synthesized counterparts when creating mini-\\nbatches. Given a bona fide utterance, xbna\\ni,k, the correspond-\\ning CS utterance is denoted by xcs\\ni,k. Mini-batches are config-\\nured with a size of 4Nspk as follows: {[(xbna\\ni,k, xcs\\ni,k)]i,k|∀i ∈\\n[1, .., Nspk], ∀k ∈ [1, 2]}, where the vocoder type used for copy\\nsynthesis is randomly selected from the four.\\nIntegrated & Multi-task SASV identification loss. The\\nSASV task can be considered as a multi-task problem in which\\nthere is a need to distinguish between target and non-target ut-\\nterances (the traditional task of ASV), as well as bona fide and\\nspoofed utterances (the task of CMs). We investigate two al-\\nternative approaches: (1) integrated SASV identification (i.e.,\\nCspk + 1 class classification) with an additional spoof class;\\n(2) multi-task SASV identification (i.e., Cspk speaker classifi-\\ncation and binary spoofing detection tasks).\\nFirst, bona fide and spoof SASV embeddings are defined by\\nxbna\\ni,k and xspf\\ni,k. We set their corresponding SASV identification\\ntargets to ˆybna\\ni,k = i ∈ [1, .., Cspk] and ˆyspf\\ni,k = Cspk + 1, respec-\\ntively. We use the AAM softmax [26] loss to perform Cspk + 1\\nclass identification. For given the input SASV embeddings xc\\ni,k\\nand their corresponding SASV identification targets ˆyc\\ni,k, the\\nAAM softmax is defined as:\\nLsasv\\nid1 = AAMsoftmax([xc\\ni,k, ˆyc\\ni,k]∀(i,k,c); Cspk + 1). (5)\\nSecond, instead of learning the SASV embeddings from an\\nintegrated single output logit, a multi-task learning [40] frame-\\nwork can be utilized. Speaker and CM labels are denoted by\\nyc\\ni,k = i ∈ [1, .., Cspk] and ˜yc\\ni,k = c ∈ [bna, spf] for SASV em-\\nbedding xc\\ni,k, respectively. The multi-task SASV identification\\nloss is then given by:\\nLsv\\nid2 = AAMsoftmax([xc\\ni,k, yc\\ni,k]∀(i,k,c); Cspk), (6)\\nLspf\\nid2 = AAMsoftmax([xc\\ni,k, ˜yc\\ni,k]∀(i,k,c); 2), (7)\\nLsasv\\nid2 = Lsv\\nid2 + Lspf\\nid2 , (8)\\nTherefore, the final loss functions in Stage 2 can be either of\\nL = Lsasv\\ncont + Lsasv\\nid1 or L = Lsasv\\ncont + Lsasv\\nid2 . We compare the\\ntwo empirically and also explore model training with onlyLsasv\\ncont,\\nLsasv\\nid1 , or Lsasv\\nid2 .\\n2.3. Stage 3: in-domain fine-tuning\\nEven though training in Stages 1 and 2 learn to discriminate\\nbona fide non-target and spoof non-target inputs, there is a re-\\nmaining domain mismatch with the evaluation protocol. Fur-\\nthermore, artefacts from the acoustic model have yet to be\\nlearned. Hence, as a final step, we fine-tune the model us-\\ning in-domain bona fide and spoofed data contained within the\\nASVspoof 2019 LA training partition. While training data cor-\\nresponds to the same domain as that used for evaluation, there\\nare differences in the spoofing algorithms used in their gener-\\nation. In Stage 3, the same loss functions of Stage 2 are used,\\nbut with different mini-batch formulation. Specifically, and dif-\\nferent to Stage 2, in Stage 3 we randomly sample 2Nspk bona\\nfide (2 utterances each from Nspk speakers) and 2Nspf spoofed\\nutterances from the ASVspoof 2019 LA [6] training partition.\\nTable 1: Proposed multi-Stage training. Results are reported\\nin terms of SASV-EER (%). trn and trn&dev refer to using\\ntrain only and both train and dev set of the ASVspoof 2019\\nLA in Stage 3. Vox bna, Voxbna+cs, ASVo, ASVbna+cs indicate Vox-\\nCeleb2 (bona fide only), VoxCeleb2 and its CS version, original\\nASVspoof 2019 LA, and ASVspoof 2019 bona fide and its CS\\nversion, respectively. Rows 2, 4, and 6 do not apply Stage 3, and\\nhence only the result after the last applied Stage is reported.\\nrows\\nStage 1 Stage 2 Stage 3 SKA-TDNN MFA-Conformer\\npre-train CS fine-tune trn trn&dev trn trn&dev\\n1 ASVo 10.04 5.94 11.47 7.67\\n2 V oxbna 16.74 20.22\\n3 V oxbna ASVo 2.67 1.25 2.13 1.49\\n4 V oxbna+cs 13.11 14.27\\n5 V oxbna+cs ASVo 2.49 1.93 1.91 1.35\\n6 V oxbna V oxbna+cs 10.24 12.33\\n7 V oxbna V oxbna+cs ASVo 1.83 1.56 1.19 1.06\\n8 ASVbna+cs 13.10 10.49 13.68 12.48\\n9 ASVbna+cs ASVo 9.57 6.17 13.46 10.11\\n10 V oxbna ASVbna+cs 5.62 4.93 9.31 8.32\\n11 V oxbna ASVbna+cs ASVo 2.48 1.44 2.72 1.76\\n3. Experimental setup\\n3.1. Datasets and metrics\\nDuring the training Stages 1 and 2, we apply CS-based data aug-\\nmentations using the V oxCeleb2 [22] or/and ASVspoof 2019\\nLA datasets [6]. During the training Stage 3 and evaluation\\nphases, we only use SASV challenge data to follow the stan-\\ndard protocol [8]. Specifically, in Stage 1 training, we employ\\nthe original V oxCeleb2 dataset.In Stage 2, we utilize either the\\nV oxCeleb2 dataset along with the corresponding CS data, or the\\nASVspoof 2019 LA train and development portions, also paired\\nwith the corresponding CS data. Finally, in Stage 3, we solely\\nrely on the ASVspoof 2019 LA train and development parti-\\ntions, comprising 50,224 utterances (5,128 genuine and 45,096\\nspoofed) collected from 40 speakers.\\nResults are reported in terms of three metrics, namely the\\nSASV-EER, the SV-EER, and the SPF-EER [8]. The SASV-\\nEER is computed using target, non-target and spoofed trials,\\nwhere only target trials should be accepted. The SV-EER is\\nestimated using target and non-target trials. The SPF-EER is\\ncomputed using target and spoofed trials; it reflects the vulner-\\nability of the ASV system when non-target trials are replaced\\nwith spoofed trials.\\n3.2. Implementation details\\nV oxCeleb2 and ASVspoof 2019 LA utterances are cropped or\\npadded to durations of 2 and 5 seconds respectively. Features\\nare 80-dimensional log mel-filterbank outputs computed with\\na window size of 25 ms and a frame-shift of 10 ms with mean\\nand variance normalization. All four vocoders described in Sec-\\ntion 2.2 were trained from scratch using the V oxCeleb2 devel-\\nopment partition and were used to generate copy synthesis data\\nfor V oxCeleb2 and the ASVspoof 2019 LA datasets.\\nFor all Stages, we used contrastive and AAM-softmax\\nlosses concurrently with AdamW optimizer [41], except for ab-\\nlation experiments. In Stage 1, we used a batch size of 200 (2 ut-\\nterances per 100 speakers), a scheduler cycle size of 25 epochs\\nand a maximum learning rate of 0.001 which was decreased by\\n0.5 between each cycle. For Stage 2, we used a batch size of 200\\n(2 utterances per 50 speakers for bona fide and corresponding'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-02T00:32:57+00:00', 'author': 'Author name(s) withheld', 'keywords': '', 'moddate': '2023-06-02T00:32:57+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Submitted to INTERSPEECH', 'trapped': '/False', 'source': '/content/Documents/Towards Single Integrated Spoofing-Aware Speaker Verification Embeddings.pdf', 'total_pages': 5, 'page': 3, 'page_label': '4'}, page_content='Table 2: Ablation on the composition of loss functions in\\nStage 3, after Stage 1 (i.e., row 3 of Table 1). trn&dev parti-\\ntion of ASVspoof2019 LA is used for ablation study.\\nLoss functions SKA-TDNN MFA-Conformer\\nSASV SV SPF SASV SV SPF\\nLsasv\\ncont 3.62 1.64 4.63 1.89 1.52 2.11\\nLsasv\\nid1 1.75 2.35 1.06 1.96 2.70 1.01\\nLsasv\\nid2 3.57 5.23 2.03 1.88 2.85 0.82\\nLsasv\\ncont +Lsasv\\nid1 1.25 1.27 1.23 1.49 1.79 1.28\\nLsasv\\ncont+Lsasv\\nid2 3.00 3.25 2.81 2.39 3.00 1.81\\nCS data), a scheduler cycle size of 20 epochs and a maximum\\nlearning rate of 1e-5 which was decreased by 0.5 every three cy-\\ncles. In the case of ASVspoof 2019 LA bona fide and CS data,\\nwe used a batch size of 32 (2 utterances per 8 speakers for bona\\nfide and CS data). For Stage 3, we used the same settings as for\\nASVspoof 2019 LA data in Stage 2, and data for 16 randomly\\nchosen spoofing attacks instead of 16 CS utterances.\\n4. Results\\n4.1. Multi-stage training and CS results\\nResults are shown in Table 1. The comparison of results in\\nrows 1 and 3 shows the benefit of Stage 1 ASV pre-training\\nwhen used with Stage 3 fine-tuning. We report four types of\\nresults, using two model architectures and two Stage 3 train-\\ning data configurations. The objective is to verify whether the\\nadditional use of development partition of ASVspoof2019LA\\nin Stage 3 can be advantageous. SASV-EERs improved in all\\nfour columns, where it was more advantageous when stage 3\\nonly included the train partition ( trn). Rows 4-7 show results\\nfor Stage 2 CS-training, with or without Stage 1 and Stage 3\\ntraining. Results in row 4 show that Stage 2 is more benefi-\\ncial than Stage 1; performance improvements are observed for\\nboth models. This makes sense since data used for Stage 1 can\\nbe viewed as a subset of the data used for Stage 2. The com-\\nparison of results in rows 5 and 7 show that Stage 1 training is\\ncomplementary to Stage 2.\\nRows 8 to 11 show results using Stage 2 training with\\nASVspoof 2019 LA CS instead of V oxCeleb2 CS. The compar-\\nison of results in rows 8 and 9 with those in rows 4 and 5 shows\\nsimilar results with no fine-tuning, but worse results with; fine\\ntuning using V oxCeleb, which contains data collected from a far\\ngreater number of speakers, is the most beneficial. The compar-\\nison of results in rows 10 and 11 to rows 8 and 9 show slightly\\nimproved performance when ASVspoof 2019 LA data is used in\\nStage 2 and combined with V oxCeleb data in Stage 1. Even so,\\nresults are slightly worse than corresponding results in rows 4-7\\nfor which V oxCeleb data is used for Stage 2. These results show\\nthat the use of data collected from a larger number of speakers\\nleads to better speaker discrimination and SASV performance.\\nResults for the two models show similar trends. The use of\\nall three stages together with V oxCeleb data for Stage 2 (row\\n7), results in the lowest SASV-EER for both models; the only\\nexception is for the SKA-TDNN model fine-tuned in Stage 3\\nwith both train and development sets. The best result of 1.06%\\nis produced with the MFA-Conformer model when trained us-\\ning all three stages and when fine-tuned using both train and\\ndevelopment sets of the ASVspoof 2019 LA database. In sum-\\nmary, Stage 1 pre-training with V oxCeleb data, followed by\\nStage 2 training with CS data, and then fine-tuning with in-\\ndomain ASVspoof 2019 LA data generally leads to the best\\nTable 3: Comparison with previous works in the literature. With\\nStage 1 and Stages 1-2 denote the results with pre-training\\n(Stage 1) and sequential CS-training (Stages 1-2), respectively.\\nSystem Type SASV SV SPF\\nECAPA-TDNN [42] ASV 23.83 1.63 30.75\\nAASIST [43] CM 24.38 49.24 0.67\\nSASV Baseline [44] Score-level Fusion 1.71 1.66 1.76\\n[15] Emb. level fusion 0.28 0.28 0.28\\n[18] Joint-optimization 1.53 2.44 0.75\\n[17] Single system 4.86 8.06 0.50\\nOurs - Stage 3 only Single system 11.47 13.46 10.30\\nwith Stage 1 Single system 2.13 3.43 1.08\\nwith Stages 1-2 Single system 1.19 1.83 0.58\\nperformance.\\n4.2. Loss function comparison\\nTable 2 shows the results adopting Stages 1 and 3, with dif-\\nferent loss functions and their combination. When using only\\none of the three loss functions, the trends differ for each model.\\nThe use of contrastive loss (Lsasv\\ncont) with integrated identification\\nloss (Lsasv\\nid1 ), leads to the best performance for both models. The\\ncomparison of identification losses (Lsasv\\nid1 and Lsasv\\nid2 ) shows that\\nintegrated outperformed its counterpart in most cases.\\n4.3. Performance comparison\\nTable 3 shows a comparison of results for the proposed single\\nSASV system to those for competing systems reported in the lit-\\nerature. The first two rows show results for the individual ASV\\nand CM subsystems used by SASV challenge baselines [8].\\nRows 3 to 6 show results for the SASV B1 baseline 4, the best\\nembedding-level fusion system, the best jointly-optimized sys-\\ntem, and best single end-to-end system reported in the litera-\\nture.5 Results for our best system (bottom row) correspond to\\nan 84% relative improvement in SASV-EER compared to the\\nbest single system for which the SASV-EER is 4.86%. This\\npushes the state-of-the-art in single SASV approaches dramati-\\ncally. It also outperforms the best jointly-optimized solution.\\n5. Conclusions\\nWe present a new integrated SASV embedding extractor that\\nachieves an SASV-EER of 1.06%, significantly surpassing pre-\\nvious best results for such systems. This approach bridges the\\nperformance gap between single integrated SASV systems and\\nfusion-based methods that combine pre-trained ASV and CM\\nmodels. Our multi-stage training solution benefits from pre-\\ntraining with large amount of data also with CS, which ad-\\ndresses the scarcity of spoofed data and the imbalance with gen-\\nuine training data.\\n6. Acknowledgements\\nThis work is partially supported by the COMPA grant funded\\nby the Korea government (MSIT and Police) (No. RS-\\n2023-00235082), Academy of Finland (Decision No. 349605,\\nproject “SPEECHFAKES”), JST CREST (JPMJCR18A6, JP-\\nMJCR20D3), and MEXT KAKENHI (21K17775, 21H04906).\\n4The 2022 SASV challenge B1 baseline with softmax applied to the\\nCM output which improves results notably.\\n5Better results are reported in the literature, but these correspond\\nto ensemble systems. The comparisons presented here are restricted to\\nsystems which employ at most one CM and one ASV subsystem.'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-02T00:32:57+00:00', 'author': 'Author name(s) withheld', 'keywords': '', 'moddate': '2023-06-02T00:32:57+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Submitted to INTERSPEECH', 'trapped': '/False', 'source': '/content/Documents/Towards Single Integrated Spoofing-Aware Speaker Verification Embeddings.pdf', 'total_pages': 5, 'page': 4, 'page_label': '5'}, page_content='7. References\\n[1] T. Kinnunen and H. Li, “An overview of text-independent speaker\\nrecognition: From features to supervectors,” Speech communica-\\ntion, vol. 52, no. 1, 2010.\\n[2] Z. Bai and X.-L. Zhang, “Speaker recognition based on deep\\nlearning: An overview,” Neural Networks, vol. 140, 2021.\\n[3] Y . Zhang, Z. Lv, H. Wu et al., “Mfa-conformer: Multi-scale fea-\\nture aggregation conformer for automatic speaker verification,” in\\nProc. INTERSPEECH, 2022.\\n[4] Z. Chen, S. Chen, Y . Wu et al., “Large-scale self-supervised\\nspeech representation learning for automatic speaker verification,”\\nin Proc. ICASSP, 2022.\\n[5] T. Kinnunen, M. Sahidullah, H. Delgado et al., “The ASVspoof\\n2017 Challenge: Assessing the limits of replay spoofing attack\\ndetection,” in Proc. INTERSPEECH, 2017.\\n[6] X. Wang, J. Yamagishi, M. Todisco et al., “ASVspoof 2019: A\\nlarge-scale public database of synthesized, converted and replayed\\nspeech,” Computer Speech & Language, vol. 64, 2020.\\n[7] J. Yamagishi, X. Wang, M. Todisco et al., “ASVspoof 2021: ac-\\ncelerating progress in spoofed and deepfake speech detection,” in\\nProc. the Automatic Speaker Verification and Spoofing Counter-\\nmeasures Challenge, 2021.\\n[8] J.-w. Jung, H. Tak, H.-j. Shim et al., “SASV 2022: The first\\nspoofing-aware speaker verification challenge,” in Proc. INTER-\\nSPEECH, 2022.\\n[9] H.-j. Shim, J.-w. Jung, J.-h. Kim and H.-j. Yu, “Integrated replay\\nspoofing-aware text-independent speaker verification,” Applied\\nSciences, vol. 10, no. 18, 2020.\\n[10] J. Li, M. Sun, X. Zhang and Y . Wang, “Joint decision of anti-\\nspoofing and automatic speaker verification by multi-task learning\\nwith contrastive loss,” IEEE Access, vol. 8, 2020.\\n[11] M. Todisco, H. Delgado, K. A. Lee et al., “Integrated presenta-\\ntion attack detection and automatic speaker verification: Common\\nfeatures and gaussian back-end fusion,” inProc. INTERSPEECH,\\n2018.\\n[12] Y . Zhang, G. Zhu and Z. Duan, “A probabilistic fusion frame-\\nwork for spoofing aware speaker verification,” in Proc. Speaker\\nOdyssey, 2022.\\n[13] J. Lin, T. Chen, J. Huang et al., “The CLIPS system for 2022\\nspoofing-aware speaker verification challenge,” in Proc. INTER-\\nSPEECH, 2022.\\n[14] X. Wang, X. Qin, Y . Wang et al., “The DKU-OPPO system for\\nthe 2022 spoofing-aware speaker verification challenge,” in Proc.\\nINTERSPEECH, 2022.\\n[15] J.-H. Choi, J.-Y . Yang, Y . R. Jeoung and C. J.-H., “HYU sub-\\nmission for the SASV challenge 2022: Reforming speaker em-\\nbeddings with spoofing-aware conditioning,” in Proc. INTER-\\nSPEECH, 2022.\\n[16] W. H. Kang, J. Alam and A. Fathan, “End-to-end framework\\nfor spoof-aware speaker verification,” in Proc. INTERSPEECH,\\n2022.\\n[17] Z. Teng, Q. Fu, J. White et al., “SA-SASV: An end-to-end spoof-\\naggregated spoofing-aware speaker verification system,” in Proc.\\nINTERSPEECH, 2022.\\n[18] W. Ge, H. Tak, M. Todisco and N. Evans, “On the potential of\\njointly-optimised solutions to spoofing attack detection and auto-\\nmatic speaker verification,” in Proc. IberSPEECH, 2022.\\n[19] A. Alenin, N. Torgashov, A. Okhotnikov et al., “A subnetwork\\napproach for spoofing aware speaker verification,” in Proc. IN-\\nTERSPEECH, 2022.\\n[20] P. Zhang, P. Hu and X. Zhang, “Norm-constrained score-level\\nensemble for spoofing aware speaker verification,” in Proc. IN-\\nTERSPEECH, 2022.\\n[21] L. Zhang, Y . Li, H. Zhao and L. Xie, “Backend ensemble for\\nspeaker verification and spoofing countermeasure,” in Proc. IN-\\nTERSPEECH, 2022.\\n[22] J. S. Chung, A. Nagrani and A. Zisserman, “V oxceleb2: Deep\\nspeaker recognition,” in Proc. INTERSPEECH, 2018.\\n[23] X. Wang and J. Yamagishi, “Spoofed training data for speech\\nspoofing countermeasure can be efficiently created using neural\\nvocoders,” in Proc. ICASSP, 2023.\\n[24] S. H. Mun, J.-w. Jung, M. H. Han and N. S. Kim, “Frequency and\\nmulti-scale selective kernel attention for speaker verification,” in\\nProc. SLT, 2023.\\n[25] W. J. Holmes, “Copy synthesis of female speech using the JSRU\\nparallel formant synthesiser.,” in Proc. EUROSPEECH, 1989, pp.\\n2513–2516.\\n[26] J. Deng, J. Guo, N. Xue and S. Zafeiriou, “Arcface: Additive an-\\ngular margin loss for deep face recognition,” in Proceedings of\\nthe IEEE/CVF conference on computer vision and pattern recog-\\nnition, 2019.\\n[27] H. S. Heo, B.-J. Lee, J. Huh and J. S. Chung, “Clova baseline sys-\\ntem for the voxceleb speaker recognition challenge 2020,” arXiv\\npreprint arXiv:2009.14153, 2020.\\n[28] Y . Kwon, H. S. Heo, B.-J. Lee and J. S. Chung, “The ins and outs\\nof speaker recognition: lessons from V oxSRC 2020,” in Proc.\\nICASSP, 2021.\\n[29] M. Pal, D. Paul and G. Saha, “Synthetic speech detection using\\nfundamental frequency variation and spectral features,”Computer\\nSpeech & Language, vol. 48, 2018.\\n[30] J. Frank and L. Sch ¨onherr, “Wavefake: A data set to facilitate au-\\ndio deepfake detection,” in Proc. Neural Information Processing\\nSystems Datasets and Benchmarks, 2021.\\n[31] Z. Wu, X. Xiao, E. S. Chng and H. Li, “Synthetic speech detection\\nusing temporal modulation feature,” in Proc. ICASSP, 2013.\\n[32] A. Sizov, E. Khoury, T. Kinnunen et al., “Joint speaker verification\\nand antispoofing in the i-vector space,” IEEE Transactions on\\nInformation Forensics and Security, vol. 10, no. 4, 2015.\\n[33] I. Saratxaga, J. Sanchez, Z. Wu et al., “Synthetic speech detection\\nusing phase information,” Speech Communication, 2016.\\n[34] C. Sun, S. Jia, S. Hou et al., “Exposing ai-synthesized hu-\\nman voices using neural vocoder artifacts,” arXiv preprint\\narXiv:2302.09198, 2023.\\n[35] J. Kong, J. Kim and J. Bae, “HiFi-GAN: Generative adversarial\\nnetworks for efficient and high fidelity speech synthesis,” inProc.\\nNIPS, 2020, vol. 33.\\n[36] R. Prenger, R. Valle and B. Catanzaro, “WaveGlow: A flow-based\\ngenerative network for speech synthesis,” inProc. ICASSP, 2019.\\n[37] X. Wang, S. Takaki and J. Yamagishi, “Neural source-filter-based\\nwaveform model for statistical parametric speech synthesis,” in\\nProc. ICASSP, 2019.\\n[38] J. Engel, L. Hantrakul, C. Gu and A. Roberts, “DDSP: Differen-\\ntiable digital signal processing,” in Proc. ICLR, 2020.\\n[39] J. S. Chung, J. Huh, S. Mun et al., “In defence of metric learning\\nfor speaker recognition,” in Proc. INTERSPEECH, 2020.\\n[40] R. Caruana, Multitask learning, Springer, 1998.\\n[41] I. Loshchilov and F. Hutter, “Decoupled weight decay regulariza-\\ntion,” in Proc. ICLR, 2017.\\n[42] B. Desplanques, J. Thienpondt and K. Demuynck, “Ecapa-tdnn:\\nEmphasized channel attention, propagation and aggregation in\\ntdnn based speaker verification,” in Proc. INTERSPEECH, 2020.\\n[43] J.-w. Jung, H.-S. Heo, H. Tak et al., “Aasist: Audio anti-spoofing\\nusing integrated spectro-temporal graph attention networks,” in\\nProc. ICASSP, 2022.\\n[44] H.-j. Shim, H. Tak, X. Liu et al., “Baseline systems for the first\\nspoofing-aware speaker verification challenge: score and embed-\\nding fusion,” in Proc. Speaker Odyssey, 2022.'), Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-08-11T02:51:47+00:00', 'author': '', 'keywords': '', 'moddate': '2020-08-11T02:51:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/Documents/Ecapa-tdnn.pdf', 'total_pages': 5, 'page': 0, 'page_label': '1'}, page_content='ECAPA-TDNN: Emphasized Channel Attention, Propagation and Aggregation\\nin TDNN Based Speaker Veriﬁcation\\nBrecht Desplanques, Jenthe Thienpondt, Kris Demuynck\\nIDLab, Department of Electronics and Information Systems, imec - Ghent University, Belgium\\nbrecht.desplanques@ugent.be, jenthe.thienpondt@ugent.be\\nAbstract\\nCurrent speaker veriﬁcation techniques rely on a neural network\\nto extract speaker representations. The successful x-vector ar-\\nchitecture is a Time Delay Neural Network (TDNN) that ap-\\nplies statistics pooling to project variable-length utterances into\\nﬁxed-length speaker characterizing embeddings. In this paper,\\nwe propose multiple enhancements to this architecture based on\\nrecent trends in the related ﬁelds of face veriﬁcation and com-\\nputer vision. Firstly, the initial frame layers can be restructured\\ninto 1-dimensional Res2Net modules with impactful skip con-\\nnections. Similarly to SE-ResNet, we introduce Squeeze-and-\\nExcitation blocks in these modules to explicitly model channel\\ninterdependencies. The SE block expands the temporal con-\\ntext of the frame layer by rescaling the channels according to\\nglobal properties of the recording. Secondly, neural networks\\nare known to learn hierarchical features, with each layer operat-\\ning on a different level of complexity. To leverage this comple-\\nmentary information, we aggregate and propagate features of\\ndifferent hierarchical levels. Finally, we improve the statistics\\npooling module with channel-dependent frame attention. This\\nenables the network to focus on different subsets of frames dur-\\ning each of the channel’s statistics estimation. The proposed\\nECAPA-TDNN architecture signiﬁcantly outperforms state-of-\\nthe-art TDNN based systems on the V oxCeleb test sets and the\\n2019 V oxCeleb Speaker Recognition Challenge.\\nIndex Terms: speaker recognition, speaker veriﬁcation, deep\\nneural networks, x-vectors, channel attention\\n1. Introduction\\nIn recent years, x-vectors [1] and their subsequent improve-\\nments [2, 3, 4] have consistently provided state-of-the-art re-\\nsults on the task of speaker veriﬁcation. Improving upon the\\noriginal Time Delay Neural Network (TDNN) architecture is\\nan active area of research. Usually, the neural networks are\\ntrained on the speaker identiﬁcation task. After convergence,\\nlow dimensional speaker embeddings can be extracted from the\\nbottleneck layer preceding the output layer to characterize the\\nspeaker in the input recording. Speaker veriﬁcation can be ac-\\ncomplished by comparing the two embeddings corresponding\\nwith an enrollment and a test recording to accept or reject the\\nhypothesis that both recordings contain the same speaker. A\\nsimple cosine distance measurement can be used for this com-\\nparison. In addition, a more complicated scoring backend can\\nbe trained such as Probabilistic Linear Discriminant Analysis\\n(PLDA) [5].\\nThe rising popularity of the x-vector system has resulted\\nin signiﬁcant architectural improvements and optimized train-\\ning procedures [6] over the original approach. The topology of\\nthe system was improved by incorporating elements of the pop-\\nular ResNet [7] architecture. Adding residual connections be-\\ntween the frame-level layers has been shown to enhance the em-\\nbeddings [3, 4]. Additionally, residual connections enable the\\nback-propagation algorithm to converge faster and help avoid\\nthe vanishing gradient problem [7].\\nThe statistics pooling layer in the x-vector system projects\\nthe variable-length input into a ﬁxed-length representation by\\ngathering simple statistics of hidden node activations across\\ntime. The authors in [8, 9] introduce a temporal self-attention\\nsystem to this pooling layer which allows the network to only\\nfocus on frames it deems important. It can also be interpreted as\\na V oice Activity Detection (V AD) preprocessing step to detect\\nthe irrelevant non-speech frames.\\nIn this work we propose further architectural enhancements\\nto the TDNN architecture and statistics pooling layer. We in-\\ntroduce additional skip connections to propagate and aggregate\\nchannels throughout the system. Channel attention that uses a\\nglobal context is incorporated in the frame layers and statistics\\npooling layer to improve the results even further.\\nThe paper is organized as follows: Section 2 will describe\\nthe current state-of-the-art speaker recognition systems which\\nwill be used as a baseline. Section 3 will explain and motivate\\nthe novel components of our proposed architecture. Section 4\\nwill explain our experimental setup to test the impact of the indi-\\nvidual components in our architecture on the popular V oxCeleb\\ndatasets [10, 11, 12]. We discuss the results of these experi-\\nments in Section 5. In addition, a comparison between popular\\nstate-of-the-art baseline systems will be provided. Section 6\\nwill conclude with a brief overview of our ﬁndings.\\n2. DNN speaker recognition systems\\nTwo types of DNN-based speaker recognition architectures will\\nserve as strong baselines to measure the impact of our proposed\\narchitecture: an x-vector and a ResNet based system which both\\ncurrently provide state-of-the-art performance on speaker veri-\\nﬁcation tasks such as V oxSRC [12].\\n2.1. Extended-TDNN x-vector\\nThe ﬁrst baseline system is the Extended TDNN x-vector archi-\\ntecture [2, 3, 4] and improves upon the original x-vector sys-\\ntem introduced in [1]. The initial frame layers consist of 1-\\ndimensional dilated convolutional layers interleaved with dense\\nlayers. Every ﬁlter has access to all the features of the previous\\nlayer or input layer. The task of the dilated convolutional layers\\nis to gradually build up the temporal context. Residual connec-\\ntions are introduced in all frame-level layers. The frame layers\\nare followed by an attentive statistics pooling layer that calcu-\\nlates the mean and standard deviations of the ﬁnal frame-level\\nfeatures. The attention system [8] allows the model to select\\nthe frames it deems relevant. After the statistics pooling, two\\nfully-connected layers are introduced with the ﬁrst one acting\\nas a bottleneck layer to generate the low-dimensional speaker\\ncharacterizing embedding.\\narXiv:2005.07143v3  [eess.AS]  10 Aug 2020'), Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-08-11T02:51:47+00:00', 'author': '', 'keywords': '', 'moddate': '2020-08-11T02:51:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/Documents/Ecapa-tdnn.pdf', 'total_pages': 5, 'page': 1, 'page_label': '2'}, page_content='2.2. ResNet-based r-vector\\nThe second baseline system is the r-vector system proposed\\nin [4]. It is based on the ResNet18 and ResNet34 implemen-\\ntations of the successful ResNet architecture [7]. The convo-\\nlutional frame layers of this network process the features as a\\n2-dimensional signal before collecting the mean and standard\\ndeviation statistics in the pooling layer. See [4] for more details\\nabout the topology.\\n3. Proposed ECAPA-TDNN architecture\\nIn this section, we examine some of the limitations of the x-\\nvector architecture and incorporate potential solutions in our\\nECAPA-TDNN architecture. The following subsections will\\nfocus on frame-level and pooling-level enhancements. An\\noverview of the complete architecture is given by Figure 2. BN\\nstands for Batch Normalization [13] and the non-linearities are\\nRectiﬁed Linear Units (ReLU) unless speciﬁed otherwise.\\n3.1. Channel- and context-dependent statistics pooling\\nIn recent x-vector architectures, soft self-attention is used\\nfor calculating weighted statistics in the temporal pooling\\nlayer [8]. Success with multi-headed attention has shown that\\ncertain speaker properties can be extracted on different sets of\\nframes [9]. Due to these results, we argue that it might be bene-\\nﬁcial to extend this temporal attention mechanism even further\\nto the channel dimension. This enables the network to focus\\nmore on speaker characteristics that do not activate on identi-\\ncal or similar time instances, e.g. speaker-speciﬁc properties of\\nvowels versus speaker-speciﬁc properties of consonants.\\nWe implement the attention mechanism as described in [8]\\nand adapt it to be channel-dependent:\\net,c = vvvT\\nc f(WWWhhht + bbb) +kc, (1)\\nwhere hhht are the activations of the last frame layer at time step\\nt. The parameters WWW ∈ RR×C and bbb ∈ RR×1 project the\\ninformation for self-attention into a smallerR-dimensional rep-\\nresentation that is shared across allCchannels to reduce the pa-\\nrameter count and risk of overﬁtting. After a non-linearity f(·)\\nthis information is transformed to a channel-dependent self-\\nattention score through a linear layer with weights vvvc ∈RR×1\\nand bias kc. This scalar score et,c is then normalized over all\\nframes by applying the softmax function channel-wise across\\ntime:\\nαt,c = exp(et,c)∑T\\nτ exp(eτ,c)\\n. (2)\\nThe self-attention score αt,c represents the importance of\\neach frame given the channel and is used to calculate the\\nweighted statistics of channel c. For each utterance the chan-\\nnel component ˜µc of the weighted mean vector ˜µ˜µ˜µis estimated\\nas:\\n˜µc =\\nT∑\\nt\\nαt,cht,c. (3)\\nThe channel component ˜σc of the weighted standard deviation\\nvector ˜σ˜σ˜σis constructed as follows:\\n˜σc =\\n\\ued6a\\ued6b\\ued6b√\\nT∑\\nt\\nαt,ch2\\nt,c −˜µ2c. (4)\\nThe ﬁnal output of the pooling layer is given by concatenating\\nthe vectors of the weighted mean ˜µ˜µ˜µand weighted standard de-\\nviation ˜σ˜σ˜σ.\\nFurthermore, we expand the temporal context of the pool-\\ning layer by allowing the self-attention to look at global prop-\\nerties of the utterance. We concatenate the local input hhht in (1)\\nwith the global non-weighted mean and standard deviation of\\nhhht across the time domain. This context vector should allow the\\nattention mechanism to adapt itself to global properties of the\\nutterance such as noise or recording conditions.\\n3.2. 1-Dimensional Squeeze-Excitation Res2Blocks\\nThe temporal context of frame layers in the original x-vector\\nsystem is limited to 15 frames. As the network apparently ben-\\neﬁts from a wider temporal context [2, 4, 3] we argue it could be\\nbeneﬁcial to rescale the frame-level features given global prop-\\nerties of the recording, similar to the global context in the atten-\\ntion module described above. For this purpose we introduce 1-\\ndimensional Squeeze-Excitation (SE) blocks, as this computer\\nvision approach to model global channel interdependencies has\\nbeen proved successful [14, 15].\\nThe ﬁrst component of an SE-block is the squeeze operation\\nwhich generates a descriptor for each channel. The squeeze\\noperation simply consists of calculating the mean vector zzz of\\nthe frame-level features across the time domain:\\nzzz= 1\\nT\\nT∑\\nt\\nhhht. (5)\\nThe descriptors in zzzare then used in the excitation operation to\\ncalculate a weight for each channel. We deﬁne the subsequent\\nexcitation operation as:\\nsss= σ(WWW2f(WWW1zzz+ bbb1) +bbb2) (6)\\nwith σ(·) denoting the sigmoid function, f(·) a non-linearity,\\nWWW1 ∈RR×C and WWW2 ∈RC×R. This operation acts as a bot-\\ntleneck layer with C and R referring to the number of input\\nchannels and reduced dimensionality respectively. The result-\\ning vector ssscontains weights sc between zero and one, which\\nare applied to the original input through channel-wise multipli-\\ncation\\n˜hhhc = schhhc (7)\\nThe 1-dimensional SE-block can be integrated in the x-\\nvector architecture in various ways, with using them after each\\ndilated convolution being the most straightforward one. How-\\never, we want to combine them with the beneﬁts of residual\\nconnections [7]. Simultaneously, we do not want to increase\\nthe total amount of parameters too much compared to the base-\\nline systems. The SE-Res2Block shown in Figure 1 incorporates\\nthe requirements mentioned above. We contain the dilated con-\\nvolutions with a preceding and succeeding dense layer with a\\ncontext of 1 frame. The ﬁrst dense layer can be used to reduce\\nthe feature dimension, while the second dense layer restores the\\nnumber of features to the original dimension. This is followed\\nby an SE-block to scale each channel. The whole unit is covered\\nby a skip connection.\\nThe use of these traditional ResBlocks makes it easy to in-\\ncorporate advancements concerning this popular computer vi-\\nsion architecture. The recent Res2Net module [16], for exam-\\nple, enhances the central convolutional layer so that it can pro-\\ncess multi-scale features by constructing hierarchical residual-\\nlike connections within. The integration of this module im-\\nproves performance, while signiﬁcantly reducing the number\\nof model parameters.'), Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-08-11T02:51:47+00:00', 'author': '', 'keywords': '', 'moddate': '2020-08-11T02:51:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/Documents/Ecapa-tdnn.pdf', 'total_pages': 5, 'page': 2, 'page_label': '3'}, page_content='Figure 1: The SE-Res2Block of the ECAPA-TDNN architecture.\\nThe standard Conv1D layers have a kernel size of 1. The central\\nRes2Net [16] Conv1D with scale dimensions= 8expands the\\ntemporal context through kernel sizekand dilation spacingd.\\n3.3. Multi-layer feature aggregation and summation\\nThe original x-vector system only uses the feature map of the\\nlast frame-layer for calculating the pooled statistics. Given the\\nhierarchical nature of a TDNN, these deeper level features are\\nthe most complex ones and should be strongly correlated with\\nthe speaker identities. However, due to evidence in [17, 18] we\\nargue that the more shallow feature maps can also contribute\\ntowards more robust speaker embeddings. For each frame, our\\nproposed system concatenates the output feature maps of all\\nthe SE-Res2Blocks. After this Multi-layer Feature Aggregation\\n(MFA), a dense layer processes the concatenated information to\\ngenerate the features for the attentive statistics pooling.\\nAnother, complementary way to exploit multi-layer infor-\\nmation is to use the output of all preceding SE-Res2Blocks\\nand initial convolutional layer as input for each frame layer\\nblock [17, 19]. We implement this by deﬁning the residual con-\\nnection in each SE-Res2Block as the sum of the outputs of all\\nthe previous blocks. We opt for a summation of the feature maps\\ninstead of concatenation to restrain the model parameter count.\\nThe ﬁnal architecture without the summed residual connections\\nis shown in Figure 2.\\n4. Experimental setup\\n4.1. Training the speaker embedding extractors\\nWe apply the ﬁxed-condition V oxSRC 2019 training restric-\\ntions [12] and only use the development part of the V oxCeleb2\\ndataset [11] with 5994 speakers as training data. A small subset\\nof about 2% of the data is reserved as a validation set for hy-\\nperparameter optimization. It is a well known fact that neural\\nnetworks beneﬁt from data augmentation which generates ex-\\ntra training samples. We generate a total of 6 extra samples for\\neach utterance. The ﬁrst set of augmentations follow the Kaldi\\nrecipe [2] in combination with the publicly available MUSAN\\ndataset (babble, noise) [20] and the RIR dataset (reverb) pro-\\nvided in [21]. The remaining three augmentations are generated\\nwith the open-source SoX (tempo up, tempo down) and FFm-\\npeg (alternating opus or aac compression) libraries.\\nFigure 2: Network topology of the ECAPA-TDNN. We denotek\\nfor kernel size anddfor dilation spacing of the Conv1D layers\\nor SE-Res2Blocks.Cand T correspond to the channel and tem-\\nporal dimension of the intermediate feature-maps respectively.\\nSis the number of training speakers.\\nThe input features are 80-dimensional MFCCs from a\\n25 ms window with a 10 ms frame shift. Two second random\\ncrops of the MFCCs feature vectors are normalized through\\ncepstral mean subtraction and no voice activity detection is\\napplied. As a ﬁnal augmentation step, we apply SpecAug-\\nment [22] on the log mel spectrogram of the samples. The al-\\ngorithm randomly masks 0 to 5 frames in the time domain and\\n0 to 10 channels in the frequency domain.\\nAll models are trained with a cyclical learning rate varying\\nbetween 1e-8 and 1e-3 using thetriangular2 policy as described\\nin [23] in conjunction with the Adam optimizer [24]. The du-\\nration of one cycle is set to 130k iterations. All systems are\\ntrained using AAM-softmax [6, 25] with a margin of 0.2 and\\nsoftmax prescaling of 30 for 4 cycles. To prevent overﬁtting,\\nwe apply a weight decay on all weights in the model of 2e-5,\\nexcept for the AAM-softmax weights, which uses 2e-4. The\\nmini-batch size for training is 128.\\nWe study two setups of the proposed ECAPA-TDNN archi-\\ntecture with either 512 or 1024 channels in the convolutional\\nframe layers. The dimension of the bottleneck in the SE-Block\\nand the attention module is set to 128. The scale dimensionsin\\nthe Res2Block [16] is set to 8. The number of nodes in the ﬁnal\\nfully-connected layer is 192. The performance of this system\\nwill be compared to the baselines described in Section 2.\\n4.2. Speaker veriﬁcation\\nSpeaker embeddings are extracted from the ﬁnal fully-\\nconnected layer for all systems. Trial scores are produced us-\\ning the cosine distance between embeddings. Subsequently, all\\nthe scores are normalized using adaptive s-norm [26, 27]. The'), Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-08-11T02:51:47+00:00', 'author': '', 'keywords': '', 'moddate': '2020-08-11T02:51:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/Documents/Ecapa-tdnn.pdf', 'total_pages': 5, 'page': 3, 'page_label': '4'}, page_content='Table 1: EER and MinDCF performance of all systems on the standard VoxCeleb1 and VoxSRC 2019 test sets.\\nArchitecture # Params VoxCeleb1 VoxCeleb1-E VoxCeleb1-H VoxSRC19\\nEER(%) MinDCF EER(%) MinDCF EER(%) MinDCF EER(%)\\nE-TDNN 6.8M 1.49 0.1604 1.61 0.1712 2.69 0.2419 1.81\\nE-TDNN (large) 20.4M 1.26 0.1399 1.37 0.1487 2.35 0.2153 1.61\\nResNet18 13.8M 1.47 0.1772 1.60 0.1789 2.88 0.2672 1.97\\nResNet34 23.9M 1.19 0.1592 1.33 0.1560 2.46 0.2288 1.57\\nECAPA-TDNN (C=512) 6.2M 1.01 0.1274 1.24 0.1418 2.32 0.2181 1.32\\nECAPA-TDNN (C=1024) 14.7M 0.87 0.1066 1.12 0.1318 2.12 0.2101 1.22\\nimposter cohort consists of the speaker-wise averages of the\\nlength-normalized embeddings of all training utterances. The\\nsize of the imposter cohort was set to 1000 for the V oxCeleb\\ntest sets and to a more robust value of 50 for the cross-dataset\\nV oxSRC 2019 evaluation.\\n4.3. Evaluation protocol\\nThe system is evaluated on the popular V oxCeleb1 test sets [10]\\nand V oxSRC 2019 evaluation set [12]. Performance will be\\nmeasured by providing the Equal Error Rate (EER) and the min-\\nimum normalized detection cost MinDCF withPtarget = 10−2\\nand CFA = CMiss = 1. A concise ablation study is used to\\ngain a deeper understanding how each of the proposed improve-\\nments affects the performance.\\n5. Results\\nA performance overview of the baseline systems described in\\nSection 2 and our proposed ECAPA-TDNN system is given in\\nTable 1, together with the number of model parameters in the\\nembedding extractor. We implement two setups with the num-\\nber of ﬁlters C in the convolutional layers either set to 512\\nor 1024. Our proposed architecture signiﬁcantly outperforms\\nall baselines while using fewer model parameters. The larger\\nECAPA-TDNN system gives an average relative improvement\\nof 18.7% in EER and 12.5% in MinDCF over the best scor-\\ning baseline for each test set. We note that the performance\\nof the baselines supersedes the numbers reported in [3, 4] in\\nmost cases. We continue with an ablation study of the individ-\\nual components introduced in Section 3. An overview of these\\nresults is given in Table 2.\\nTable 2: Ablation study of the ECAPA-TDNN architecture.\\nSystems EER(%) MinDCF\\nECAPA-TDNN (C=512) 1.01 0.1274\\nA.1 Attentive Statistics [8] 1.12 0.1316\\nA.2 Channel Att. w/o Context 1.03 0.1288\\nB.1 No SE-Block 1.27 0.1446\\nB.2 No Res2Net-Block 1.07 0.1316\\nC.1 No MFA 1.10 0.1311\\nC.2 No Res. Connections 1.08 0.1310\\nC.3 No Sum Res. Connections 1.08 0.1217\\nTo measure the impact of our proposed attention mod-\\nule, we run an experiment A.1, that uses the attention module\\nfrom [8]. We also run a separate experiment A.2 that does\\nnot supply the context vector to the proposed attention. The\\nchannel- and context-dependent statistics pooling system im-\\nproves the EER and MinDCF metric with 9.8% and 3.2%, re-\\nspectively. This conﬁrms the beneﬁts of applying different tem-\\nporal attention to each channel. Addition of the context vector\\nresults in very small performance gains with the system rel-\\natively improving about 1.9% in EER and 1.1% in MinDCF.\\nNonetheless, this strengthens our belief that a TDNN-based ar-\\nchitecture should try to exploit global context information.\\nThis intuition is conﬁrmed with experiment B.1 that clearly\\nshows the importance of the SE-blocks described in Section 3.2.\\nIncorporating the SE-modules in the Res2Blocks results in rela-\\ntive improvements of 20.5% in EER and 11.9% in the MinDCF\\nmetric. This indicates that the limited temporal context of the\\nframe-level features is insufﬁcient and should be complemented\\nwith global utterance-based information. In experiment B.2 we\\nreplaced the multi-scale features of the Res2Blocks with the\\nstandard central dilated 1D convolutional of the ResNet coun-\\nterpart. Aside from a substantial 30% relative reduction in\\nmodel parameters, the multi-scale Res2Net approach also leads\\ntowards a relative improvement of 5.6% in EER and 3.2% in\\nMinDCF.\\nIn experiment C.1, we only use the output of the ﬁnal SE-\\nRes2Block instead of aggregating the information of all SE-\\nRes2Blocks. Aggregation of the outputs leads to relative im-\\nprovements of 8.2% in EER and 2.8% in the MinDCF value.\\nRemoving all residual connections (experiment C.2) shows a\\nsimilar rate of degradation. Replacing a standard ResNet skip\\nconnection in the SE-Res2Blocks by the sum of the outputs\\nof all previous SE-Res2Blocks improves the EER with 6.5%,\\nwhile slightly degrading the MinDCF score in experiment C.3.\\nHowever, experiments during the recently held Short-duration\\nSpeaker Veriﬁcation (SdSV) Challenge 2020 [28] convinced us\\nto incorporate summed residuals in the ﬁnal ECAPA-TDNN ar-\\nchitecture. The strong results in this challenge show the archi-\\ntecture generalizes well to other domains [29].\\n6. Conclusion\\nIn this paper we presented ECAPA-TDNN, a novel TDNN-\\nbased speaker embedding extractor for speaker veriﬁcation. We\\nbuilt further upon the original x-vector architecture and put\\nmore Emphasis on Channel Attention, Propagation and Aggre-\\ngation. The incorporation of Squeeze-Excitation blocks, multi-\\nscale Res2Net features, extra skip connections and channel-\\ndependent attentive statistics pooling, led to signiﬁcant relative\\nimprovements of 19% in EER on average over strong baseline\\nsystems on the V oxCeleb and V oxSRC 2019 evaluation sets.'), Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-08-11T02:51:47+00:00', 'author': '', 'keywords': '', 'moddate': '2020-08-11T02:51:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/Documents/Ecapa-tdnn.pdf', 'total_pages': 5, 'page': 4, 'page_label': '5'}, page_content='7. References\\n[1] D. Snyder, D. Garcia-Romero, G. Sell, D. Povey, and S. Khudan-\\npur, “X-vectors: Robust DNN embeddings for speaker recogni-\\ntion,” inProc. ICASSP, 2018, pp. 5329–5333.\\n[2] D. Snyder, D. Garcia-Romero, G. Sell, A. McCree, D. Povey, and\\nS. Khudanpur, “Speaker recognition for multi-speaker conversa-\\ntions using x-vectors,” inProc. ICASSP, 2019, pp. 5796–5800.\\n[3] D. Garcia-Romero, A. McCree, D. Snyder, and G. Sell, “JHU-\\nHLTCOE system for the V oxSRC speaker recognition challenge,”\\nin Proc. ICASSP, 2020, pp. 7559–7563.\\n[4] H. Zeinali, S. Wang, A. Silnova, P. Matjka, and O. Plchot, “BUT\\nsystem description to V oxCeleb speaker recognition challenge\\n2019,” 2019.\\n[5] S. Ioffe, “Probabilistic linear discriminant analysis,” in ECCV,\\n2006, pp. 531–542.\\n[6] J. Deng, J. Guo, N. Xue, and S. Zafeiriou, “ArcFace: Additive\\nangular margin loss for deep face recognition,” in2019 IEEE/CVF\\nCVPR, 2019, pp. 4685–4694.\\n[7] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for\\nimage recognition,” inIEEE/CVF CVPR, 2016, pp. 770–778.\\n[8] K. Okabe, T. Koshinaka, and K. Shinoda, “Attentive statistics\\npooling for deep speaker embedding,” inProc. Interspeech, 2018,\\npp. 2252–2256.\\n[9] Y . Zhu, T. Ko, D. Snyder, B. K.-W. Mak, and D. Povey, “Self-\\nattentive speaker embeddings for text-independent speaker veriﬁ-\\ncation,” inProc. Interspeech, 2018, pp. 3573–3577.\\n[10] A. Nagrani, J. S. Chung, and A. Zisserman, “V oxCeleb: A large-\\nscale speaker identiﬁcation dataset,” in Proc. Interspeech, 2017,\\npp. 2616–2620.\\n[11] J. S. Chung, A. Nagrani, and A. Zisserman, “V oxCeleb2: Deep\\nspeaker recognition,” inProc. Interspeech, 2018, pp. 1086–1090.\\n[12] J. S. Chung, A. Nagrani, E. Coto, W. Xie, M. McLaren, D. A.\\nReynolds, and A. Zisserman, “V oxSRC 2019: The ﬁrst V oxCeleb\\nspeaker recognition challenge,” 2019.\\n[13] S. Ioffe and C. Szegedy, “Batch normalization: Accelerating deep\\nnetwork training by reducing internal covariate shift,” in Proc.\\nICML, 2015, pp. 448–456.\\n[14] J. Hu, L. Shen, and G. Sun, “Squeeze-and-Excitation networks,”\\nin Proc. IEEE/CVF CVPR, 2018, pp. 7132–7141.\\n[15] J. Zhou, T. Jiang, Z. Li, L. Li, and Q. Hong, “Deep speaker em-\\nbedding extraction with channel-wise feature responses and ad-\\nditive supervision softmax loss function,” in Proc. Interspeech,\\n2019, pp. 2883–2887.\\n[16] S. Gao, M.-M. Cheng, K. Zhao, X. Zhang, M.-H. Yang, and\\nP. H. S. Torr, “Res2Net: A new multi-scale backbone architec-\\nture,”IEEE TPAMI, 2019.\\n[17] J. Lee and J. Nam, “Multi-level and multi-scale feature aggrega-\\ntion using sample-level deep convolutional neural networks for\\nmusic classiﬁcation,” 2017.\\n[18] Z. Gao, Y . Song, I. McLoughlin, P. Li, Y . Jiang, and L.-R. Dai,\\n“Improving Aggregation and Loss Function for Better Embedding\\nLearning in End-to-End Speaker Veriﬁcation System,” in Proc.\\nInterspeech, 2019, pp. 361–365.\\n[19] D. Povey, G. Cheng, Y . Wang, K. Li, H. Xu, M. Yarmohammadi,\\nand S. Khudanpur, “Semi-orthogonal low-rank matrix factoriza-\\ntion for deep neural networks,” in Proc. Interspeech, 2018, pp.\\n3743–3747.\\n[20] D. Snyder, G. Chen, and D. Povey, “MUSAN: A music, speech,\\nand noise corpus,” 2015.\\n[21] T. Ko, V . Peddinti, D. Povey, M. L. Seltzer, and S. Khudanpur,\\n“A study on data augmentation of reverberant speech for robust\\nspeech recognition,” inProc. ICASSP, 2017, pp. 5220–5224.\\n[22] D. S. Park, W. Chan, Y . Zhang, C.-C. Chiu, B. Zoph, E. D.\\nCubuk, and Q. V . Le, “SpecAugment: A simple data augmen-\\ntation method for automatic speech recognition,” in Proc. Inter-\\nspeech, 2019.\\n[23] L. N. Smith, “Cyclical learning rates for training neural net-\\nworks,” inIEEE WACV, 2017, pp. 464–472.\\n[24] D. Kingma and J. Ba, “Adam: A method for stochastic optimiza-\\ntion,”Proc. ICLR, 2014.\\n[25] X. Xiang, S. Wang, H. Huang, Y . Qian, and K. Yu, “Margin mat-\\nters: Towards more discriminative deep neural network embed-\\ndings for speaker recognition,” 2019.\\n[26] Z. N. Karam, W. M. Campbell, and N. Dehak, “Towards reduced\\nfalse-alarms using cohorts,” in 2011 IEEE International Confer-\\nence on Acoustics, Speech and Signal Processing (ICASSP), 2011,\\npp. 4512–4515.\\n[27] S. Cumani, P. Batzu, D. Colibro, C. Vair, P. Laface, and V . Vasi-\\nlakakis, “Comparison of speaker recognition approaches for real\\napplications,” inProc. Interspeech, 2011, pp. 2365–2368.\\n[28] H. Zeinali, K. A. Lee, J. Alam, and L. Burget, “Short-duration\\nspeaker veriﬁcation (SdSV) challenge 2020: the challenge evalu-\\nation plan,” 2019.\\n[29] J. Thienpondt, B. Desplanques, and K. Demuynck, “Cross-lingual\\nspeaker veriﬁcation with domain-balanced hard prototype min-\\ning and language-dependent score normalization,” in Proc. Inter-\\nspeech, 2020.'), Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-10-13T10:02:34+00:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-10-13T10:02:34+00:00', 'subject': '(unspecified)', 'title': '(anonymous)', 'trapped': '/False', 'source': '/content/Documents/InfoEdge_Data_Scientist_2025.pdf', 'total_pages': 6, 'page': 0, 'page_label': '1'}, page_content=\"InfoEdge Data Scientist 2025\\n1. A non-zero vector that can be changed at most by its scalar factor after the\\napplication of linear transformations is called:\\na) Eigenvector\\nb) Scalar vector\\nc) Unit vector\\nd) Orthogonal vector\\n2. X is the sum of the squares of k independent standard normal random variables.\\nWhat's the distribution of X?\\na) Normal distribution\\nb) Chi-squared distribution\\nc) t-distribution\\nd) Binomial distribution\\n3. In rolling a dice 300 times we get 54 ones, 46 twos, 54 threes, 46 fours, 50 fives\\nand 36 sixes. We wish to determine whether the dice is fair. What is the test\\nstatistic value for the goodness of fit test in this case?\\na) 6.8\\nb) 7.8\\nc) 8.0\\nd) 9.2\\n4. What is the correct formula for detecting outliers using the Inter Quartile Range\\n(IQR)?\\na) Q1 ± 1.5*IQR\\nb) Q1 - 1.5*IQR and Q3 + 1.5*IQR\\nc) Mean ± 2*IQR\\nd) Median ± 1.5*IQR\\n5. What is IQR?\\na) Difference between max and min\\nb) Difference between Q3 and Q1\\nc) Average of quartiles\\nd) Ratio of Q1 and Q3\\n6. In a normal distribution, the percentage of values that lie within one, two, three\\nstandard deviations of the mean respectively are:\\na) 68%, 95%, 99.7%\\nb) 70%, 90%, 99%\\nc) 65%, 93%, 99%\\nd) 68%, 96%, 100%\\n7. Which of the following is NOT an AXIOM of probability?\\na) P(Event) ≥ 0\\nb) P(Sample Space) = 1\\nc) P(A union B) = P(A) + P(B) if A and B are mutually exclusive\"), Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-10-13T10:02:34+00:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-10-13T10:02:34+00:00', 'subject': '(unspecified)', 'title': '(anonymous)', 'trapped': '/False', 'source': '/content/Documents/InfoEdge_Data_Scientist_2025.pdf', 'total_pages': 6, 'page': 1, 'page_label': '2'}, page_content=\"d) P(A and B) = P(A/B)*P(B)\\n8. Which of the following is NOT a property of the Poisson process?\\na) Events are independent of each other\\nb) Average rate of occurrence is constant\\nc) Events are uniformly spaced\\nd) Series of discrete events\\n9. What does Z score signify?\\na) How many variance away from mean\\nb) How many standard deviations away from mean\\nc) How many means away from std\\nd) Both b and c\\n10. A fair dice is thrown twice. What is the probability that '5' will occur at least\\nonce?\\na) 5/36\\nb) 11/36\\nc) 1/6\\nd) 25/36\\n11. Latent Dirichlet Allocation is a generative model.\\na) True\\nb) False\\n12. We are implementing decision tree, if there are very large number of classes in\\na categorical variable, which measure is more preferable?\\na) Information gain is preferred over gain ratio\\nb) Gain ratio is preferred over information gain\\nc) Both are equal\\nd) None\\n13. Which of the following is/are assumption(s) of the Naive Bayes Algorithm?\\na) All features are equally important\\nb) All features are independent\\nc) Both a and b\\nd) None\\n14. If you suspect your model to be overfitting, which of the following is NOT a\\npreferred way to try to reduce overfitting?\\na) Increase amount of training data\\nb) Opt for a more sophisticated optimization algorithm\\nc) Decrease model complexity\\nd) Reduce noise in the data\\n15. A deep decision tree with pure leaf nodes is not likely perform well on unseen\\ndata because of:\\na) High Variance\\nb) High Bias\\nc) Performs well\\nd) None\"), Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-10-13T10:02:34+00:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-10-13T10:02:34+00:00', 'subject': '(unspecified)', 'title': '(anonymous)', 'trapped': '/False', 'source': '/content/Documents/InfoEdge_Data_Scientist_2025.pdf', 'total_pages': 6, 'page': 2, 'page_label': '3'}, page_content='16. A decision tree is likely to be more prone to:\\na) High Bias\\nb) High Variance\\nc) Low Variance\\nd) Low Bias\\n17. Linear regression can be optimized using:\\na) Gradient Descent\\nb) Normal Equation\\nc) Both a and b\\nd) None\\n18. You are given 20 data points in a cartesian coordinate system. PCA gives\\nnearly identical eigen values. What could lead to this?\\na) Data symmetric about axes\\nb) Spread equally along axes\\nc) Uniformly spread along rim of circle centered at origin\\nd) All\\n19. Which of the following is NOT TRUE related to binary classification?\\na) MSE function is convex\\nb) Models output probabilities in (0,1)\\nc) MSE not best cost function\\nd) Log-loss is convex\\n20. Instead of trying to achieve absolute zero error, we set a metric called Bayes\\nerror. Which of the following options could be a good reason for using Bayes error?\\na) Input variables incomplete\\nb) System may be stochastic\\nc) Limited training data\\nd) All of the above\\n21. In which neural net architecture does weight sharing occur?\\na) CNN\\nb) ANN\\nc) DBN\\nd) Both a and b\\n22. Which of the following Neural network applies identity mapping (i.e., Input to\\nsome layer is passed directly or as shortcut to another layer)?\\na) GRNN\\nb) ResNet\\nc) Attention\\nd) AlexNet\\n23. While using Bidirectional RNN for text language translation, the input may be\\nbest passed as:\\na) Individual words\\nb) Pair of words\\nc) Whole sentence'), Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-10-13T10:02:34+00:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-10-13T10:02:34+00:00', 'subject': '(unspecified)', 'title': '(anonymous)', 'trapped': '/False', 'source': '/content/Documents/InfoEdge_Data_Scientist_2025.pdf', 'total_pages': 6, 'page': 3, 'page_label': '4'}, page_content=\"d) None\\n24. State whether the statement: 'Neural networks can approximate any function.'\\nis True or False.\\na) True\\nb) False\\n25. Which of the following architectures may be more suitable for learning\\ntime-series data compared to the rest?\\na) LSTM\\nb) CNN\\nc) ANN\\nd) All\\n26. Which of the following is NOT used for preventing overfitting in neural\\nnetworks?\\na) Dropout\\nb) Data augmentation\\nc) He Initialization\\nd) Early Stopping\\n27. Which of the following is FALSE about Radial Basis Function Neural Network?\\na) Resembles RNNs\\nb) Uses radial basis function\\nc) Considers distance to center\\nd) Output is absolute value\\n28. Which one of the following is not a standard pooling technique in CNNs?\\na) Average Pool\\nb) Max Pool\\nc) Median Pool\\nd) Min Pool\\n29. We are most likely to find a global minima of an objective function if the\\nobjective function is:\\na) Only convex\\nb) Only differentiable\\nc) Neither convex nor differentiable\\nd) Both convex and differentiable\\n30. Pandas dataframes of Python, Index values must be:\\na) Unique\\nb) Hashable\\nc) Both\\nd) None\\n31. Which of the following SQL command shall display all the unique values in a\\ncolumn called 'data1' in a SQL table MY_TABLE?\\na) select data1 from MY_TABLE\\nb) select unique data1 from MY_TABLE\\nc) select distinct data1 from MY_TABLE\"), Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-10-13T10:02:34+00:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-10-13T10:02:34+00:00', 'subject': '(unspecified)', 'title': '(anonymous)', 'trapped': '/False', 'source': '/content/Documents/InfoEdge_Data_Scientist_2025.pdf', 'total_pages': 6, 'page': 4, 'page_label': '5'}, page_content=\"d) select data1 from MY_TABLE where data1 is distinct\\n32. Which is the correct way to get the number of unique values in a column 'C' in a\\nPandas dataframe df?\\na) dfCcount unique()\\nb) df[C].nunique()\\nc) df.unique()\\nd) dfC find_unique()\\n33. Which of the following is FALSE about 'dictionary' data type in Python?\\na) Values accessed via keys\\nb) Keys accessed via values\\nc) Not ordered\\nd) Mutable\\n34. Data type 'Lists' in Python are:\\na) Mutable\\nb) Ordered\\nc) Iterable\\nd) All of the above\\n35. In Python command prompt, if you type 3**2**3, you may get which of the\\nbelow answers?\\na) 6561\\nb) 729\\nc) 18\\nd) None\\n36. Which of the following methods would compute the frequency of elements in a\\ngiven column of a Pandas dataframe?\\na) value count\\nb) Dcount values()\\nc) count value()\\nd) value_counts()\\n37. The Numpy slicing operator [:4] would do the following:\\na) Slice elements from beginning to index 3\\nb) Slice elements to index 4\\nc) Slice last 4 elements\\nd) Throw error\\n38. For the given three lists in Python: X = [1, 2, 3, 4], Y = [1, 3, 5, 7], and Z = [1, 3],\\nwhich of the below command may be used to compute Z from X and Y?\\na) X.intersection(Y)\\nb) list(x.common(Y))\\nc) np.common(set(X), set(Y))\\nd) list(set(X).intersection(set(Y)))\\n39. 10 men and 12 women images are there. A model classifies 14 women where\\n8 are actual women and 6 are men. Precision and recall?\\na) Precision = 4/7, Recall = 2/3\"), Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-10-13T10:02:34+00:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-10-13T10:02:34+00:00', 'subject': '(unspecified)', 'title': '(anonymous)', 'trapped': '/False', 'source': '/content/Documents/InfoEdge_Data_Scientist_2025.pdf', 'total_pages': 6, 'page': 5, 'page_label': '6'}, page_content='b) Precision = 2/3, Recall = 4/7\\nc) Precision = 3/5, Recall = 4/5\\nd) None of the above'), Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-13T01:06:09+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-13T01:06:09+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/Documents/SKA-TDNN.pdf', 'total_pages': 7, 'page': 0, 'page_label': '1'}, page_content='FREQUENCY AND MULTI-SCALE SELECTIVE KERNEL ATTENTION\\nFOR SPEAKER VERIFICATION\\nSung Hwan Mun∗1 Jee-weon Jung∗2 Min Hyun Han1 Nam Soo Kim1\\n1Department of ECE and INMC, Seoul National University, Seoul, South Korea\\n2Naver Corporation, South Korea\\nABSTRACT\\nThe majority of recent state-of-the-art speaker veriﬁcation\\narchitectures adopt multi-scale processing and frequency-\\nchannel attention mechanisms. Convolutional layers of these\\nmodels typically have a ﬁxed kernel size, e.g., 3 or 5. In this\\nstudy, we further contribute to this line of research utilising\\na selective kernel attention (SKA) mechanism. The SKA\\nmechanism allows each convolutional layer to adaptively\\nselect the kernel size in a data-driven fashion. It is based\\non an attention mechanism which exploits both frequency\\nand channel domain. We ﬁrst apply existing SKA module\\nto our baseline. Then we propose two SKA variants where\\nthe ﬁrst variant is applied in front of the ECAPA-TDNN\\nmodel and the other is combined with the Res2net backbone\\nblock. Through extensive experiments, we demonstrate that\\nour two proposed SKA variants consistently improves the\\nperformance and are complementary when tested on three\\ndifferent evaluation protocols.\\nIndex Terms— speaker veriﬁcation, selective kernel at-\\ntention, multi-scale module\\n1. INTRODUCTION\\nIn recent years, various deep neural network (DNN) archi-\\ntectures for speaker veriﬁcation (SV) systems have been pro-\\nposed [1–6]. Current state-of-the-art architectures typically\\nutilise 1-dimensional convolutional neural networks (1D-\\nCNNs) such as x-vector, RawNet3, or ECAPA-TDNN [7–9].\\nAmong these, ECAPA-TDNN [9] is widely adopted, demon-\\nstrating stable yet competitive performance across a wide\\nrange of studies. It involves Res2net backbone blocks with a\\nsqueeze-excitation (SE) layer at the end of each block, where\\nthe Res2net incorporates multi-scale modelling and the SE\\nefﬁciently recalibrates the channel (ﬁlter) axis of a CNN\\nfeature map [10, 11].\\nSeveral architectures that extend ECAPA-TDNN have\\nalso been proposed [12, 13]. Authors of [12] extended\\n* Equal contribution.\\nThis work was supported by Institute of Information & communications\\nTechnology Planning & Evaluation (IITP) grant funded by the Korea govern-\\nment (MSIT) (No.2021-0-00456, Development of Ultra-high Speech Quality\\nTechnology for Remote Multi-speaker Conference System).\\nECAPA-TDNN and proposed ECAPA-CNN-TDNN, by\\nadding a 2D-CNN-based front-end with frequency-wise SE\\nlayers to incorporate frequency translational invariance. Sim-\\nilarly, MFA-TDNN [13] applied a 2D-CNN-based module in\\nfront of the original ECAPA-TDNN identical to the ECAPA-\\nCNN-TDNN; however, it proposed to replace the 2D-CNN-\\nbased module with a multi-scale frequency-channel attention\\nmodule. Leveraging the multi-scale processing capability\\nand the attention module, which resembles the SE layer,\\nMFA-TDNN demonstrates competitive performance across\\ntest scenarios involving diverse duration.\\nTo this end, we explore to further push this line of re-\\nsearch. We adapt the selective kernel attention (SKA) mech-\\nanism more effectively to speaker veriﬁcation, inspired by\\n[14–18]. Speech signals have multi-scale and hierarchical lin-\\nguistic structures (e.g., phoneme, syllable, and word) and dif-\\nferent time-frequency responses [19]. The SKA module is\\nexpected to adaptively emphasise the local and global infor-\\nmation required for extracting robust speaker-discriminative\\nrepresentations. Hence, our model architecture, which in-\\nvolves several different-sized kernels can choose which ker-\\nnel to concentrate on in a data-driven fashion.\\nWe further propose two modules, which are variants of\\nthe SKA. First, we propose multi-scale SKA (msSKA) which\\nincorporates the SKA approach with the Res2net-based back-\\nbone modules. The objective is to develop a backbone mod-\\nule which can better model utterances with diverse durations.\\nSecond, we propose frequency-wise SKA (fwSKA) which\\nadapts the SKA module to operate upon the frequency axis\\nof a feature map. It is designed to inject global frequency\\ninformation across the intermediate feature representations,\\nsimilarly to [12].\\nExperiments conducted with three different evaluation\\nprotocols consistently demonstrate the effectiveness of our\\nproposed approaches over the baseline systems. We also ob-\\nserve the identical tendency across three different durations.\\nThe rest of this paper is organised as follows: Section 2\\ndescribes the selective kernel attention module, Section 3 in-\\ntroduces the proposed SKA-variants, and Sections 4 presents\\nthe proposed architectures. Then the experimental settings\\nand results are addressed in Sections 5 and 6, respectively.\\nFinally, we conclude in Sections 7.\\n978-1-6654-7189-3/22$31.00 © 2023 IEEE\\narXiv:2204.01005v4  [eess.AS]  12 Oct 2022'), Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-13T01:06:09+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-13T01:06:09+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/Documents/SKA-TDNN.pdf', 'total_pages': 7, 'page': 1, 'page_label': '2'}, page_content='softmax\\ns\\nz\\na𝒌𝒌𝟏𝟏\\nX\\n𝐶𝐶\\n𝐹𝐹\\n𝑇𝑇\\nU\\n𝐔𝐔𝒌𝒌𝟏𝟏\\nV\\na𝒌𝒌𝟏𝟏𝐔𝐔𝒌𝒌𝟏𝟏\\n𝓕𝓕𝒌𝒌𝟏𝟏\\n𝓕𝓕𝒌𝒌𝑵𝑵\\n𝐔𝐔𝒌𝒌𝑵𝑵 a𝒌𝒌𝑵𝑵𝐔𝐔𝒌𝒌𝑵𝑵\\na𝒌𝒌𝑵𝑵\\nFig. 1: Frequency-wise selective kernel attention (fwSKA)\\n2. SELECTIVE KERNEL ATTENTION MODULE\\nThis Section describes the selective kernel attention (SKA)\\nmechanism [14] which can select the kernel size adaptive in a\\ndata-driven fashion.\\nFor a given X ∈ RC′×F′×T′\\n, let Fki : X → Uki ∈\\nRC×F×T be a convolution operator with kernel sizeki. First,\\nthe input feature map X is split into N branches. Each con-\\nvolution layer, Fki , generates Uki , where each of the {ki}N\\ni=1\\nhas a pre-deﬁned different kernel size. To integrate differ-\\nent scales of information into the next layer, N branches are\\nfused by an element-wise summation, i.e., U = ∑N\\ni=1 Uki .\\nThen 2D global average pooling (GAP) embeds the global\\ninformation into the channel-wise feature vector s ∈RC as\\nfollow:\\ns = 1\\nF ×T\\nF∑\\nf=1\\nT∑\\nt=1\\nU(f, t). (1)\\nThe fully-connected (FC), batch normalisation (BN) and\\nReLU layers are sequentially passed to squeeze the channel-\\nwise compact feature z ∈Rd:\\nz = ReLU(BN(Ws)), (2)\\nwhere W ∈Rd×C denotes the weight matrix of a FC layer\\nand d is the dimensionality of z. Next, soft attention weights\\nacross channels aki = [aki;1, ..., aki;C]T ∈RC are calculated\\nvia a softmax function as follows:\\naki;j = exp(Aki;jz)∑N\\nl=1 exp(Akl;jz)\\n, (3)\\nwhere Aki;j ∈Rd is the j-th FC weight row vector of Aki =\\n[AT\\nki;1, ..., AT\\nki;C]T ∈RC×d.\\nFinally, the output feature map V ∈ RC×F×T is com-\\nputed as the weighed summation over the different branches:\\nVj =\\nN∑\\ni=1\\naki;jUki;j,\\nN∑\\ni=1\\naki;j = 1. (4)\\nwhere Vj and Uki;j ∈ RF×T are the j-th components of\\nV and Uki , respectively. We note the conventional SKA as\\nchannel-wise SKA (cwSKA) throughout this paper.\\nmsSKA3\\nmsSKA4\\nx .. x .. x .. x..\\nmsSKA2\\ny.. y.. y.. y..\\n(1) (2) (3) (4)\\n(1) (2) (3) (4)\\n𝐕𝐕(𝑗𝑗)\\n𝐱𝐱 𝑗𝑗\\n𝐔𝐔(𝑗𝑗)\\n𝐕𝐕𝑘𝑘1\\n𝑗𝑗 𝐕𝐕𝑘𝑘𝑁𝑁\\n𝑗𝑗\\n𝐕𝐕𝑘𝑘2\\n𝑗𝑗\\n𝐬𝐬 𝑗𝑗\\n𝐳𝐳 𝑗𝑗\\n𝐚𝐚𝑘𝑘1\\n𝑗𝑗 𝐚𝐚𝑘𝑘2\\n𝑗𝑗\\n𝐚𝐚𝑘𝑘𝑁𝑁\\n𝑗𝑗\\n𝐔𝐔𝑘𝑘1\\n𝑗𝑗\\n 𝐔𝐔𝑘𝑘2\\n𝑗𝑗\\n 𝐔𝐔𝑘𝑘𝑁𝑁\\n𝑗𝑗\\nFig. 2: Multi-scale selective kernel attention (msSKA)\\n3. PROPOSED SKA-V ARIANTS\\n3.1. Frequency-wise SKA (fwSKA)\\nThe conventional SKA method, cwSKA, extracts the global\\ninformation regarding the channel importance by using\\n2D GAP on the F ×T dimension. However, speaker-\\ndiscriminative information may also exist in the frequency\\nor temporal domain, which the channel-wise recalibration\\ncan not effectively capture. Thus, we propose frequency-wise\\nSKA (fwSKA) to aggregate global frequency information to\\nthe attention weights using the SKA framework. It adopts the\\nsame SKA technique, however, s is a frequency-wise feature\\nvector rather than a channel-wise feature vector:\\ns = 1\\nC ×T\\nC∑\\nc=1\\nT∑\\nt=1\\nU(c, t). (5)\\nCompact feature z, attention weights a, and the output\\nfeature map V is derived in the same manner with cwSKA.\\nHowever, note that their dimensionalities are F, not C be-\\ncause they operate upon the frequency dimension.\\n3.2. Multi-scale SKA (msSKA)\\nWe apply both the conventional cwSKA and our proposed\\nfwSKA to the 2D-CNN module that places in front of the\\nECAPA-TDNN. However, we believe that the SKA tech-\\nnique can be also complementary with the Res2net backbone\\nblock of the ECAPA-TDNN, which also focuses on process-\\ning data in a multi-scale fashion. Hence, the multi-scale\\nSKA (msSKA) module is proposed. The msSKA module\\nreplaces the single kernel 1D-CNN in Res2net block of the\\nECAPA-TDNN architecture.\\nIn the msSKA module, a 2D input feature map X ∈\\nRC′×T′\\nis ﬁrst evenly divided into s feature map subsets\\n{x(j)}s\\nj=1 ∈ R(C′/s)×T′\\nwhere s denotes the number of\\nscales. Let Gki : x(j) →U(j)\\nki\\n∈R(C/s)×T be 1D convolution\\noperator with kernel size ki. The 1D convolution Gki is ap-\\nplied to the each feature map subset{x(j)}s\\nj=1, and the output'), Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-13T01:06:09+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-13T01:06:09+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/Documents/SKA-TDNN.pdf', 'total_pages': 7, 'page': 2, 'page_label': '3'}, page_content='Conv2D\\nfcwSKA Block\\nfcwSKA Block\\nConv2D\\nConv1D\\nmsSKA Block\\nmsSKA Block\\nConv1D\\nmsSKA Block\\ninput: 1×80×𝑇𝑇\\n128×40×𝑇𝑇\\n128×40×𝑇𝑇\\n128×40×𝑇𝑇\\n64×20× ⁄𝑇𝑇 2\\n(64×20)×1× ⁄𝑇𝑇 2\\nflatten\\n1,024×1× ⁄𝑇𝑇 2\\n1,024×1× ⁄𝑇𝑇 2\\n1,024×1× ⁄𝑇𝑇 2\\n3,072×1× ⁄𝑇𝑇 2\\n1,536×1× ⁄𝑇𝑇 2\\nConv1D\\nSE\\nConv1D\\nmsSKA3\\nmsSKA4\\nx1 x2 x3 x4\\nmsSKA2\\ny1 y2 y3 y4\\nConv2D\\nfwSKA\\ncwSKA\\nSE\\nFig. 3: The overall proposed architecture: The frequency-channel-wise SKA block-based front network (left) and the multi-\\nscale SKA block-based TDNN network (right). This architecture is referred to SKA-TDNN.\\nbrunches are integrated by the an element-wise summation:\\ns(j) = 1\\nT\\nT∑\\nt=1\\nU(j)(t), U(j) =\\nN∑\\ni=1\\nU(j)\\nki\\n, (6)\\nwhere U(j) ∈R(C/s)×T is the j-th scale’s fused feature map\\nobtained via 1D-CNN with different kernel sizes. Also, the\\nj-th compact feature z(j), attention weights a(j), and output\\nfeature map V(j) are calculated as in the Section 2 and 3.1.\\nFinally, all j feature maps are concatenated in the channel\\naxis.\\n4. MODEL ARCHITECTURES\\nFigure 3 illustrates the overall scheme of the proposed archi-\\ntecture. We propose three blocks leveraging the SKA mech-\\nanism described in the previous Section, namely, fcwSKA,\\nfwSKA, and msSKA blocks. The fcwSKA stands for apply-\\ning fwSKA and cwSKA in sequence. Except for the proposed\\nmsSKA block, all blocks exist within the 2D-CNN block in\\nfront of the ECAPA-TDNN architecture. The msSKA block\\nreplaces the Res2net block within the ECAPA-TDNN.\\nThe fcwSKA block.comprises a 2D-CNN, fwSKA, cwSKA,\\nSE layers sequentially with the residual connection. Each\\nfcwSKA block is included in front of the ECAPA-TDNN.\\nThe fwSKA block.has the same architecture as the fcwSKA\\nblock, but only consists of the fwSKA layer between the 2D-\\nCNN and SE layers.\\nThe msSKA block. resembles a typical Res2net backbone\\nblock within the ECAPA-TDNN. However, we adopt msSKA\\nto each scale except one.\\nBy applying above blocks to the ECAPA-TDNN or\\nECAPA-CNN-TDNN architecture, we propose four systems:\\n• ECAPA-TDNN with msSKA. does not employ a 2D\\nCNN-based block in front of the ECAPA-TDNN. It re-\\nplaces the backbone blocks with the msSKA-based blocks\\n(Figure 3, right). In each msSKA block, we use N = 2\\nfor the 1D-CNNs with kernel sizes, where their sizes are\\n3 and 5. Both the dilation and group size are set to 1, and\\nthe reduction ratio C/d is 8. We adopt a channel of 1,024\\nand a scale of 8.\\n• ECAPA-CNN-TDNN with fcwSKA.places the proposed\\nfcwSKA-based blocks instead of the front network of stan-\\ndard ECAPA-CNN-TDNN (Figure 3 left). In each fcw\\nSKA block, the 2D-CNNs with the kernel sizes of 3×3 and\\n5×5 are exploited. The dilation, the group size, and the re-\\nduction ratio are set to the same values as in the ECAPA-\\nTDNN with msSKA. For the multi-scale TDNN networks,\\na channel of 1,024 and a scale of 8 are used.\\n• ECAPA-CNN-TDNN with fwSKA.has the same struc-\\nture as the ECAPA-CNN-TDNN with fcwSKA, except for\\nonly containing the fwSKA layer in the SKA blocks.\\n• SKA-TDNN. consists of both the fcwSKA block-based\\nfront and the msSKA block-based TDNN networks (Fig-\\nure 3). We set the hyper-parameters to the same values\\nused in the ECAPA-TDNN with msSKA and the ECAPA-\\nCNN-TDNN with fcwSKA.\\nWe adopt the channel and context-dependent statistic\\npooling [9] to aggregate the frame-level output features in\\nall systems. We adopt the equal-weighted summation of the\\nadditive angular margin (AAM) softmax [20] and the angular\\nprototypical (AP) [5] objective functions to train all networks.'), Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-13T01:06:09+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-13T01:06:09+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/Documents/SKA-TDNN.pdf', 'total_pages': 7, 'page': 3, 'page_label': '4'}, page_content='Table 1: The experimental results on the V oxCeleb1-O, V oxCeleb-E and V oxCeleb-H evaluation protocols. COS: Vanilla cosine\\nsimilarity. TTA: Test time augmentation. SN: Adaptive score normalisation. †: Our implementation.\\nVoxCeleb1-O VoxCeleb1-E VoxCeleb1-H\\nModel Params EER(%) MinDCF EER(%) MinDCF EER(%) MinDCF\\nFull 3.0s 1.5s Full 3.0s 1.5s Full 3.0s 1.5s Full 3.0s 1.5s Full 3.0s 1.5s Full 3.0s 1.5s\\nResNet34 Q/SAP [6]\\nCOS 2.27 2.74 4.70 0.169 0.217 0.336 2.33 2.90 4.60 0.169 0.216 0.334 4.50 5.55 8.43 0.281 0.352 0.499\\n1.4M TTA 2.23 - - 0.167 - - 2.27 - - 0.166 - - 4.37 - - 0.283 - -\\nSN 2.08 2.65 4.51 0.163 0.210 0.320 2.18 2.79 4.49 0.151 0.201 0.313 4.23 5.42 8.35 0.248 0.321 0.465\\nResNet34 H/ASP [6]\\nCOS 1.09 1.47 2.67 0.091 0.112 0.200 1.28 1.59 2.64 0.094 0.114 0.184 2.29 2.95 4.68 0.167 0.200 0.293\\n7.7M TTA 1.06 - - 0.083 - - 1.23 - - 0.087 - - 2.23 - - 0.155 - -\\nSN 1.01 1.31 2.55 0.080 0.108 0.193 1.14 1.46 2.51 0.080 0.103 0.171 2.55 2.96 4.78 0.133 0.178 0.272\\nECAPA-TDNN† [9]\\nCOS 1.01 1.43 2.77 0.081 0.110 0.197 1.21 1.52 2.75 0.088 0.105 0.183 2.23 2.89 4.72 0.156 0.201 0.296\\n14.7M TTA 0.97 - - 0.078 - - 1.19 - - 0.086 - - 2.14 - - 0.150 - -\\nSN 0.96 1.35 2.59 0.077 0.105 0.188 1.16 1.48 2.60 0.079 0.101 0.179 2.10 2.79 4.59 0.135 0.181 0.274\\nECAPA-CNN-TDNN† [12]\\nCOS 0.94 1.21 2.32 0.063 0.095 0.172 1.07 1.39 2.36 0.074 0.096 0.159 2.03 2.64 4.34 0.129 0.169 0.255\\n27.6M TTA 0.91 - - 0.063 - - 1.06 - - 0.071 - - 2.04 - - 0.123 - -\\nSN 0.88 1.20 2.26 0.060 0.094 0.168 1.01 1.35 2.31 0.069 0.092 0.154 1.93 2.52 4.11 0.115 0.160 0.247\\nMFA-TDNN† [13]\\nCOS 0.90 1.18 2.28 0.064 0.096 0.169 1.05 1.36 2.33 0.073 0.097 0.161 2.00 2.62 4.29 0.132 0.165 0.252\\n24.9M TTA 0.86 - - 0.068 - - 1.03 - - 0.070 - - 2.02 - - 0.130 - -\\nSN 0.84 1.16 2.20 0.059 0.092 0.161 0.98 1.30 2.27 0.066 0.093 0.152 1.89 2.48 3.98 0.119 0.158 0.243\\nECAPA-CNN-TDNN\\nwith cwSKA\\nCOS 0.91 1.19 2.26 0.067 0.094 0.162 1.05 1.34 2.30 0.072 0.095 0.155 1.97 2.52 4.15 0.124 0.162 0.253\\n28.3M TTA 0.83 - - 0.060 - - 0.99 - - 0.068 - - 1.95 - - 0.120 - -\\nSN 0.83 1.15 2.20 0.061 0.089 0.155 0.97 1.29 2.23 0.066 0.086 0.149 1.91 2.45 3.95 0.117 0.154 0.245\\nECAPA-TDNN\\nwith msSKA\\nCOS 0.97 1.29 2.45 0.074 0.107 0.181 1.13 1.47 2.52 0.076 0.099 0.168 2.12 2.76 4.49 0.153 0.186 0.268\\n16.7M TTA 0.95 - - 0.076 - - 1.12 - - 0.078 - - 2.10 - - 0.148 - -\\nSN 0.92 1.28 2.41 0.072 0.099 0.175 1.09 1.44 2.48 0.074 0.096 0.164 2.01 2.65 4.38 0.130 0.175 0.265\\nECAPA-CNN-TDNN\\nwith fwSKA\\nCOS 0.90 1.19 2.19 0.060 0.088 0.163 1.01 1.31 2.25 0.073 0.095 0.153 1.93 2.49 4.05 0.122 0.161 0.248\\n28.3M TTA 0.82 - - 0.060 - - 0.97 - - 0.069 - - 1.90 - - 0.115 - -\\nSN 0.80 1.11 2.09 0.057 0.086 0.151 0.96 1.25 2.15 0.063 0.085 0.147 1.86 2.38 3.87 0.111 0.148 0.239\\nECAPA-CNN-TDNN\\nwith fcwSKA\\nCOS 0.87 1.18 2.20 0.059 0.084 0.160 0.99 1.29 2.19 0.069 0.091 0.148 1.90 2.44 4.00 0.118 0.154 0.246\\n29.4M TTA 0.84 - - 0.055 - - 0.98 - - 0.067 - - 1.89 - - 0.114 - -\\nSN 0.80 1.14 2.07 0.057 0.080 0.152 0.93 1.20 2.06 0.061 0.084 0.137 1.77 2.31 3.79 0.104 0.143 0.232\\nSKA-TDNN\\nCOS 0.85 1.14 2.14 0.054 0.082 0.154 0.97 1.25 2.12 0.065 0.087 0.144 1.87 2.41 3.95 0.114 0.150 0.241\\n34.9M TTA 0.83 - - 0.053 - - 0.94 - - 0.063 - - 1.85 - - 0.111 - -\\nSN 0.78 1.10 2.05 0.047 0.078 0.147 0.90 1.18 2.03 0.059 0.081 0.134 1.74 2.28 3.77 0.102 0.138 0.224\\n5. EXPERIMENTS\\n5.1. Baseline model architectures\\nWe utilise the following six baselines: ResNet34 Q/SAP,\\nH/ASP [6], ECAPA-TDNN [9], ECAPA-CNN-TDNN [12],\\nMFA-TDNN [13], and the ECAPA-CNN-TDNN with cwSKA.\\nAmong the baselines MFA-TDNN and ECAPA-CNN-TDNN\\nwith cwSKA would be the most competitive systems. MFA-\\nTDNN is the most recently proposed system in this line\\nof speaker veriﬁcation research and we designed ECAPA-\\nCNN-TDNN with cwSKA to validate how well the model\\nperforms when the conventional cwSKA is applied. Except\\nfor ResNet34 Q/SAP and H/ASP baselines where we used\\nthe pre-trained weight parameters, our own implementations\\nwere carried out for the other models.\\n5.2. Dataset and evaluation protocol\\nWe use the development set of V oxCeleb2 dataset [4] for\\ntraining the models, which consists of 1,092,009 utter-\\nances from 5,994 speakers. The evaluation is performed\\nusing V oxCeleb1 dataset [3] where we report the equal er-\\nror rate (EER) and the minimum detection cost function\\n(MinDCF) for three different evaluation protocols, namely,\\nV oxCeleb1-O, V oxCeleb1-E and V oxCeleb1-H.Ptarget=0.05\\nand Cmiss=Cfa =1 are used to calculate the MinDCF metric.\\n5.3. Back-end approaches for the scoring\\nWe report the performance of each model using three differ-\\nent back-end methods: (1) vanilla cosine similarity (COS),\\n(2) test time augmentation (TTA), and (3) score normali-\\nsation (SN). For the vanilla COS, the whole utterance is\\nused as input to extract an embedding. For the TTA [4], we\\nﬁrst segment each utterance into ten 4-second segments with\\noverlaps. The score for a given trial is derived by averaging\\ncosine similarity values between all pairs of segments (i.e.,\\n10×10=100). Finally, for the SN [21], we normalise the\\ncomputed vanilla COS scores. We adopt the V oxCeleb2 de-\\nvelopment set as the cohort set and then select the top 50,000\\nvanilla COS scores among cohort impostors to calculate the\\nstatistics for SN.\\n5.4. Implementation details\\nWe implement models with the PyTorch library and conduct\\nexperiments using 4 NVIDIA GeForce RTX 3090 GPUs in'), Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-13T01:06:09+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-13T01:06:09+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/Documents/SKA-TDNN.pdf', 'total_pages': 7, 'page': 4, 'page_label': '5'}, page_content='0 20 40 60 80 100 120\\nChannel index\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8Attention weights of 5x5 kernel\\nx3.0\\nx2.0\\nx1.0\\n0 20 40 60 80 100 120\\nChannel index\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0Attention weights of 3x3 kernel\\nx3.0\\nx2.0\\nx1.0\\n32,000 samples (2 seconds)\\n×2.0\\n×3.0\\n×1.0\\nUp-sampling\\nUp-sampling\\nFig. 4: The attention weights of 5 ×5 (left) and 3 ×3 (middle) kernels for each 128 channel index. The 3 in-\\nput utterances with different resolutions (right). The input utterance is randomly sampled from V oxCeleb1 test set\\n(id10272/dkN2DIBrXqQ/00002.wav).\\nparallel1 During training, we randomly crop an input ut-\\nterance to a 2-second segment and then augment it with\\neither MUSAN noises [22] or the simulated room impulse\\nresponses (RIRs) [23]. Input features to the models are 80-\\ndimensional log mel-ﬁlterbanks derived with a hamming\\nwindow length of 25ms and hop-size of 10ms with 512-size\\nFFT bins. We apply mean and variance normalisation to the\\nlog mel-ﬁlterbanks [24].\\nThe AAM-softmax objective function [20] adopts a mar-\\ngin of 0.2 and a scale of 30. The AP objective function [5]\\nuses one utterance for the prototype. For training, all models\\nare trained with a batch size of 200 and optimised using an\\nAdam optimiser [25] with a weight decay of 2e-5. The learn-\\ning rate was scheduled via the cosine annealing with warm-\\nup restart [26] with a cycle size of 25 epochs, the maximum\\nlearning rate of 1e-3 and the decreasing rate of 0.8 for two\\ncycles.\\n6. RESULTS\\n6.1. Main results\\nTable 1 describes the main experiments where we report the\\nperformances of several baselines and the proposed SKA-\\nbased models. We additionally report the evaluation result on\\nshort duration scenario. Speaker embedding extracted from\\na full duration of an enrolment utterance is compared with\\neither a test utterance with 3-second or 1.5-second duration.\\nWe crop the middle part of an utterance to generate a short\\nsegment and if the utterance length is shorter than the target\\nduration, we ﬁrst duplicate and then perform cropping, fol-\\nlowing the protocol in [2,27]. We also report the results using\\nthe three scoring approaches, i.e., the vanilla COS, TTA, SN,\\ndescribed in Section 5.3.\\nBy comparing rows 3 and 7 of Table 1, we observe\\nmarginal improvement by applying the proposed msSKA\\n1Implementation is available at https://github.com/msh9184/\\nska-tdnn.git.\\nmodule in the backbone module (ECAPA-TDNN vs ECAPA-\\nTDNN with msSKA). Both models outperform the ResNet-\\nbased models (ResNet34 Q/SAP and H/ASP), which are\\ncommonly used in the SV ﬁeld. Next, we show that the use\\nof frequency-wise SKA in the front module (rows 8 and 9\\nof Table 1) helps improve the performance compared to the\\nsystem using only conventional channel-wise SKA (rows 6 of\\nTable 1). In addition, SKA-TDNN, including both fcwSKA\\nblock-based front module and msSKA block-based TDNN\\nnetwork, obtains the best performing result, achieving EER\\nof 0.78% and MinDCF of 0.047 on the V oxCeleb-O test set,\\nrespectively. Although the SKA-based models introduce ad-\\nditional parameters, they effectively improve the performance\\nwithout severely slowing down the inference process.\\nWe also investigate the effect of SKA-based models on\\ntest utterances with different duration. The proposed SKA-\\nbased models show consistent relative improvement under\\nall test scenarios on different duration. Compared to the\\nSN results of best performing baseline, ECAPA-TDNN with\\ncwSKA, on the V oxCeleb1-O test set, the SKA-TDNN ob-\\ntains relative improvements of 4.35% and 6.82% in terms of\\nEER on 3.0-second and 1.5-second test utterances, showing\\nmore improvement with shorter utterances.\\nAcross all models including the proposed architectures,\\nTTA and SN results show improved performance than those\\nof typical cosine similarity (COS) where SN consistently\\nshowed the best performance on all evaluation sets.\\n6.2. Analysis and interpretation\\nWe further design an additional experiment to gain insights\\non the SKA module’s working mechanism. For this purpose,\\nwe observe the values of attention vectors ( a3×3, a5×5) that\\ndecide on which kernel, either 3 ×3 or 5×5, is more utilised\\nwhen utterances with different resolutions are input. Utter-\\nances with different resolutions are generated using upsam-\\npling with interpolation as illustrated in the right side of Fig-\\nure 4.'), Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-13T01:06:09+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-13T01:06:09+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/Documents/SKA-TDNN.pdf', 'total_pages': 7, 'page': 5, 'page_label': '6'}, page_content='The left and the middle sides of Figure 4 illustrate atten-\\ntion weights of 5×5 kernel and 3×3 kernel using an utterance\\nrandomly sampled from the V oxCeleb1 test set. Through vi-\\nsualisation, we observe that for the 5×5 kernel, attention val-\\nues tend to increase as the input is upsampled more. In con-\\ntrast, for the 3×3 kernel, attention values are the lowest when\\nit is upsampled the most (yellow line). We hence conﬁrm that\\nthe SKA module adaptively selects the kernel size, thereby\\nselecting the receptive ﬁeld size, adjusted in a data-driven\\nfashion.\\n7. CONCLUSION\\nThis paper explored a selective kernel attention (SKA) mod-\\nule, allowing each convolutional layer to adaptively adjust\\nkernel size based on an attention mechanism applied to both\\nfrequency and channel domain. In addition, we proposed\\narchitectures by integrating the frequency-channel-wise SKA\\nblock-based front and the multi-scale SKA block-based\\nTDNN networks. Vast experiments conducted using three\\ndifferent evaluation protocols demonstrate that both proposed\\nSKA-based modules boost the veriﬁcation performance and\\napplying both modules simultaneously performed the best.\\nThe SKA modules are relatively robust to short duration\\nscenarios.\\n8. ACKNOWLEDGEMENTS\\nWe thank Bong-Jin Lee, Hee-Soo Heo, Young-ki Kwon, and\\nYou Jin Kim at Naver Corporation for valuable discussions.\\n9. REFERENCES\\n[1] Li Wan, Quan Wang, Alan Papir, and Ignacio Lopez\\nMoreno, “Generalized End-to-End Loss for Speaker\\nVeriﬁcation,” in Proc. IEEE International Conference\\non Acoustics, Speech and Signal Processing (ICASSP).\\nIEEE, 2018, pp. 4879–4883.\\n[2] Jee-weon Jung, Hee-soo Heo, ju-ho Kim, Hye-jin Shim,\\nand Ha-jin Yu, “RawNet: Advanced End-to-End\\nDeep Neural Network using Raw Waveforms for Text-\\nIndependent Speaker Veriﬁcation,” in Proc. INTER-\\nSPEECH, 2019, pp. 1268–1272.\\n[3] Arsha Nagrani, Joon Son Chung, and Andrew Zisser-\\nman, “V oxCeleb: A Large-Scale Speaker Identiﬁcation\\nDataset,” in Proc. INTERSPEECH, 2017, pp. 2616–\\n2620.\\n[4] Joon Son Chung, Arsha Nagrani, and Andrew Zisser-\\nman, “V oxCeleb2: Deep Speaker Recognition,” inProc.\\nINTERSPEECH, 2018, pp. 1086–1090.\\n[5] Joon Son Chung, Jaesung Huh, Seongkyu Mun, Minjae\\nLee, Hee Soo Heo, Soyeon Choe, Chiheon Ham, Sungh-\\nwan Jung, Bong-Jin Lee, and Icksang Han, “In Defence\\nof Metric Learning for Speaker Recognition,” in Proc.\\nINTERSPEECH, 2020.\\n[6] Hee Soo Heo, Bong-Jin Lee, Jaesung Huh, and Joon Son\\nChung, “Clova Baseline System for the V oxCeleb\\nSpeaker Recognition Challenge 2020,” arXiv preprint\\narXiv:2009.14153, 2020.\\n[7] David Snyder, Daniel Garcia-Romero, Gregory Sell,\\nDaniel Povey, and Sanjeev Khudanpur, “X-vectors:\\nRobust DNN Embeddings for Speaker Recognition,”\\nin Proc. IEEE International Conference on Acoustics,\\nSpeech and Signal Processing (ICASSP). IEEE, 2018,\\npp. 5329–5333.\\n[8] Jee-weon Jung, You Jin Kim, Hee-Soo Heo, Bong-Jin\\nLee, Youngki Kwon, and Joon Son Chung, “Pushing\\nthe Limits of Raw Waveform Speaker Recognition,” in\\nProc. INTERSPEECH, 2022, pp. 2228–2232.\\n[9] Brecht Desplanques, Jenthe Thienpondt, and Kris De-\\nmuynck, “ECAPA-TDNN: Emphasized Channel At-\\ntention, Propagation and Aggregation in TDNN based\\nSpeaker Veriﬁcation,” in Proc. INTERSPEECH, 2020,\\npp. 3830–3834.\\n[10] Jie Hu, Li Shen, and Gang Sun, “Squeeze-and-\\nExcitation Networks,” in Proc. IEEE/CVF Conference\\non Computer Vision and Pattern Recognition (CVPR),\\n2018, pp. 7132–7141.\\n[11] Shang-Hua Gao, Ming-Ming Cheng, Kai Zhao, Xin-Yu\\nZhang, Ming-Hsuan Yang, and Philip Torr, “Res2Net: A\\nNew Multi-Scale Backbone Architecture,” IEEE Trans-\\nactions on Pattern Analysis and Machine Intelligence,\\nvol. 43, no. 2, pp. 652–662, 2019.\\n[12] Jenthe Thienpondt, Brecht Desplanques, and Kris De-\\nmuynck, “Integrating Frequency Translational Invari-\\nance in TDNNs and Frequency Positional Information\\nin 2D ResNets to Enhance Speaker Veriﬁcation,” in\\nProc. INTERSPEECH, 2021, pp. 2302–2306.\\n[13] Tianchi Liu, Rohan Kumar Das, Kong Aik Lee,\\nand Haizhou Li, “MFA: TDNN with Multi-Scale\\nFrequency-Channel Attention for Text-independent\\nSpeaker Veriﬁcation with Short Utterances,” in Proc.\\nIEEE International Conference on Acoustics, Speech\\nand Signal Processing (ICASSP) . IEEE, 2022, pp.\\n7517–7521.\\n[14] Xiang Li, Wenhai Wang, Xiaolin Hu, and Jian Yang,\\n“Selective Kernel Networks,” in Proc IEEE/CVF Con-\\nference on Computer Vision and Pattern Recognition\\n(CVPR), 2019, pp. 510–519.'), Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-13T01:06:09+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-13T01:06:09+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/Documents/SKA-TDNN.pdf', 'total_pages': 7, 'page': 6, 'page_label': '7'}, page_content='[15] Hang Zhang, Chongruo Wu, Zhongyue Zhang, Yi Zhu,\\nHaibin Lin, Zhi Zhang, Yue Sun, Tong He, Jonas\\nMueller, R Manmatha, et al., “ResNeSt: Split-Attention\\nNetworks,” arXiv preprint arXiv:2004.08955, 2020.\\n[16] Yinpeng Chen, Xiyang Dai, Mengchen Liu, Dongdong\\nChen, Lu Yuan, and Zicheng Liu, “Dynamic Convo-\\nlution: Attention over Convolution Kernels,” in Proc.\\nIEEE/CVF Conference on Computer Vision and Pattern\\nRecognition (CVPR), 2020, pp. 11030–11039.\\n[17] Yanfeng Wu, Chenkai Guo, Junan Zhao, Xiao Jin, and\\nJing Xu, “RSKNet-MTSP: Effective and Portable Deep\\nArchitecture for Speaker Veriﬁcation,” Neurocomput-\\ning, 2022.\\n[18] Seong-Hu Kim, Hyeonuk Nam, and Yong-Hwa Park,\\n“Decomposed Temporal Dynamic CNN: Efﬁcient\\nTime-Adaptive Network for Text-Independent Speaker\\nVeriﬁcation Explained with Speaker Activation Map,”\\narXiv preprint arXiv:2203.15277, 2022.\\n[19] Ruiteng Zhang, Jianguo Wei, Xugang Lu, Wenhuan\\nLu, Di Jin, Junhai Xu, Lin Zhang, Yantao Ji, and\\nJianwu Dang, “TMS: A Temporal Multi-Scale Back-\\nbone Design for Speaker Embedding,” arXiv preprint\\narXiv:2203.09098, 2022.\\n[20] Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos\\nZafeiriou, “ArcFace: Additive Angular Margin Loss\\nfor Deep Face Recognition,” in Proc. IEEE/CVF Con-\\nference on Computer Vision and Pattern Recognition\\n(CVPR), 2019, pp. 4690–4699.\\n[21] Pavel Matejka, Ondrej Novotn `y, Oldrich Plchot, Lukas\\nBurget, Mireia Diez S´anchez, and Jan Cernock`y, “Anal-\\nysis of Score Normalization in Multilingual Speaker\\nRecognition,” in Proc. INTERSPEECH, 2017, pp.\\n1567–1571.\\n[22] David Snyder, Guoguo Chen, and Daniel Povey, “MU-\\nSAN: A Music, Speech, and Noise Corpus,” arXiv\\npreprint arXiv:1510.08484, 2015.\\n[23] Tom Ko, Vijayaditya Peddinti, Daniel Povey, Michael L\\nSeltzer, and Sanjeev Khudanpur, “A Study on Data\\nAugmentation of Reverberant Speech for Robust Speech\\nRecognition,” in Proc. IEEE International Conference\\non Acoustics, Speech and Signal Processing (ICASSP).\\nIEEE, 2017, pp. 5220–5224.\\n[24] Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempit-\\nsky, “Instance Normalization: The Missing Ingredient\\nfor Fast Stylization,” in Proc. IEEE/CVF Conference\\non Computer Vision and Pattern Recognition (CVPR),\\n2016, pp. 1–6.\\n[25] Diederik P Kingma and Jimmy Ba, “Adam: A method\\nfor stochastic optimization,” inProc. International Con-\\nference on Learning Representations (ICLR), 2015, pp.\\n1–15.\\n[26] Ilya Loshchilov and Frank Hutter, “SGDR: Stochas-\\ntic Gradient Descent with Warm Restarts,” in Proc.\\nInternational Conference on Learning Representations\\n(ICLR), 2017, pp. 1–13.\\n[27] Ju-ho Kim, Hye-jin Shim, Jungwoo Heo, and Ha-jin Yu,\\n“RawNeXt: Speaker Veriﬁcation System for Variable-\\nDuration Utterances with Deep Layer Aggregation and\\nExtended Dynamic Scaling Policies,” in Proc. IEEE In-\\nternational Conference on Acoustics, Speech and Signal\\nProcessing (ICASSP). IEEE, 2022, pp. 7647–7651.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create embeddings for the text chunks\n",
        "embedding = HuggingFaceEmbeddings(model_name=\"nomic-ai/nomic-embed-text-v1.5\", model_kwargs = {'trust_remote_code': True})\n",
        "\n",
        "# Initialize a FAISS (Vector Store) retriever with the text embeddings\n",
        "retriever = FAISS.from_documents(texts, embedding).as_retriever(search_kwargs={\"k\": 20})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 679,
          "referenced_widgets": [
            "f824d00eb0274e29b99616d1866eec73",
            "3d286dd471804058a8ec92ec3eb4a9a7",
            "1c4ce3fa903c48919d2ba1f204094e4f",
            "62cec4a858e64b0ea5ff2f66000509b0",
            "a92f080a7c49485ab02cb4160eb4cb28",
            "4c2cdfc51e6a4a9485275f81b84fac31",
            "b48f89bfa20f442d87eb4729f7f81bdc",
            "403bee13a3b94d2ab553c48640fee635",
            "af656d5a6adb45c7a3b9fc3fd9b1ba9e",
            "4385dd78e1e940a4859ca4d976a919fe",
            "5a4d88a8d41949148b414b92ef5f7b62",
            "a0400180db5c4415afa51a31eacd0fd6",
            "25c01da6ea004ace81bdadc19248d745",
            "899f222440d44e629b08dba2743de590",
            "52d32bf370f04b239948b648acb50ae6",
            "153347bc34324bbcaf156044233d51e4",
            "9bd678c150524691b05fac9194eba6fa",
            "3ca8cd8126cc408f9c4136536962af2b",
            "827154e17fc14be3a47944712bcbc640",
            "0318ad513b834e0490a190ad5024807d",
            "2c7be21e2b6d41ec8a2974edc4ad8602",
            "b3b8dc34d40b4dd2a89190635b8caea5",
            "1cdd64fe5e4c4721a9f7b864eeb359b5",
            "7a37a20adf094c9980a2068aa9645e49",
            "2086f6e4b1b1440ea46785f7aa4c2cdf",
            "a403ee6cf9ea4cf79d7267de2f4ee042",
            "fa936923a58046a8abcb3fbf11b33c9e",
            "f1c318375fbc42ee8fcee3b8bec48404",
            "f72af3f02e4d4b1c960d3a7c739561d3",
            "eeea5d041a674d57a9ead96e58f488f0",
            "c19b620ab80b46e7b410e36fcf7269ee",
            "2e994a89ad7845adaee92184cf6b370e",
            "6adac25fec014e079c0b9f6c056c6ae0",
            "c93487298a764d9f841ffc9aa84db7e5",
            "b0859c7f7c8248a7b4c50b23beb6866a",
            "ad6d9ca3c7d44a63a789c62b516a2eea",
            "8a44ae29e4d4499da456da70039a2e7c",
            "bacf3bf1b1f94e4f94ecd07323c3cfbd",
            "c42c0ea0998449399f27fe6a770aca89",
            "b215aded4035426ebb36e1c4245fe69b",
            "cb16ee3ce40243ac9e4baca9a87a45af",
            "7de66071e42f498e86d67d83745bb74d",
            "b064165dbebf4e828995162e7c337fa7",
            "cc298aeb72cc4e6ea4a3fb4f5f1b6498",
            "a4b7ec057bce41529558411a2f2e3de2",
            "96f00a020b594331b7151cbb7b7adf80",
            "628f48973f4c497d961b4bb4351b0d56",
            "4e0af9f6b7e841b0818d82ed2f33fb5b",
            "fc3afdbcd59e470e9106c4b86a4ff23b",
            "bdb1bfcec5954e4cbaf38fd712f8f430",
            "a617692c9ebc497db11bc708e3e409ed",
            "19e443e0d3c44a1e8137454af48131cc",
            "16a0d57ea4044878a171e6c0d4512d1e",
            "0267a25f378f44589042a674873ae185",
            "f70c7a69bf89437a8253c914e09b1cbd",
            "016f071b914a41c9a9b108f4df63b90e",
            "39ae7e24d84345528d42fc42458b50f9",
            "0a2868f6fa9242ea857953e28d947be6",
            "16d27ad3860a4943b2c945078b881cf2",
            "00b5a8bb7baa44e983bc3563439b8d3e",
            "c60904536f134aeaa972bada6c634602",
            "184db2a996514115b8a52dfa130adcff",
            "0de0e480e539402c9374f32a2fd9cef7",
            "7b8d346ee9ca452c8b66b5a300db47e5",
            "96a2580b6eb64f858bcba70fa54ab485",
            "930bdd694a7f452c8c7b97d9b1bebe5e",
            "f071c9dcbfe14a8e8603165e48e0791f",
            "880d23d0845741098b4189bca5414759",
            "068901942c30418aae76e5ac65595dbb",
            "721ab22448b94230b18cf579b767d3af",
            "653badcba90f474a8ee5d57eaf028ac4",
            "80e554035bfc4628b0deddabcb1d1739",
            "fcaf72e3159e4325b1452f1506ad29b9",
            "ebb614b9e9a44885a40dabbd6c808c7b",
            "992e8f7513d844009638c8862de40a10",
            "d3a2eda188b440a1806ef9881e6ecb35",
            "efe7533e63774879a1259e0eccde647f",
            "1e8bbbcec5684c3fb88b46dfee733d54",
            "dc4ebff916f84bba98c84de7afbee852",
            "277d8207ff5f49178cbee2a3363b97df",
            "f2d021e50e5f4aaca9907fb6a82d23bc",
            "b1da9bb3292c4b41b13dca5faaf0f89e",
            "59a2282225eb43e4a5134c7f28e0f0ff",
            "fa924fbf3f7f4f4dbcd6a43674562440",
            "ea090cde7e7540218187c4308ab464aa",
            "2c137a3fb71e4ff39d8644f9577a0a4a",
            "b9c594dbd4b44fd6a0cea5cb187cdb00",
            "613ee9bee03e4ac9b5c87e382925cc02",
            "3c311f0293004dd1a19b6c73b581d68d",
            "99e75e7bbbd34471880b17b18ba7fc20",
            "2235b116df1f494c9fce73a7a38a19a6",
            "83c28460467c433fbd8eb5ceb67743a8",
            "3ed99862a3a048e1a3e8544bf4fe0cdc",
            "e48f1097ff414f3794008996669439f7",
            "5b03c9a460294597a58e44433588392b",
            "3855765c295148a89cc56e101557e860",
            "82b5ea5c95794b8d98aafcc65b619bf1",
            "0e2c290792754c658b3d45199f8b2333",
            "ce47345ba07d42718d0280d3ebdcd655",
            "5b9bff89d2234c78b4656e31cc6ed354",
            "61de5aa2756343eea6f0caa9aed67dc9",
            "ff48fa24654649be910302be350005a5",
            "e33a748795bb4868b61863dad673e67e",
            "a1cd1d028992418e9939cf2bebb14bae",
            "259fe087fb9742cfa2f3d4885329eb6d",
            "0bb60e26366d4e048ade4189782a3527",
            "9e90cf5a553c46fdaa1e2e8d9b2d6d67",
            "d2ce16d7a2a443d1ac6771cd8a39b27e",
            "64c16938a20a4ba08f85a96b428350e2",
            "100a11b0c270445683b89a995de361e9",
            "a2cd8444035c4eddb710ec217df3b7a4",
            "eb2a5521a47a447790ce7dfc069a14f8",
            "e189fe1bf2f04198a411213fa01114f8",
            "fc2998fbc37647459d29dc8dee9bf445",
            "1fe209a0b91845cca00840dc98dbef12",
            "2c712478359c4334bd1b846427a71ab5",
            "ed2a766723c6482ea2ca06046cf51c51",
            "2d52815bb96741fa893e760adb8abe31",
            "f9ac908adbd549dca2e8282bfdc6dab9",
            "4cc8270b531742678db83a4851ef8289",
            "a79a353c7bac411c9e481cce7c98c345",
            "ae80e3224eea44c2a3b51f084d42efe8",
            "de5fd94152d844e3bb13a0c1c58caca1",
            "de3cd9ee49cf4b86b4d3cf1ec085c4ff",
            "f18e44d69bf24c64965d2a22473cb05e",
            "7c466d1b4cfd4d55892390a204085695",
            "0099939376db41018f5b6d2c0264b8b7",
            "c9e2673428ca4d3992b36d89e9171d42",
            "626e753e3d394ad5a8aaf3538e56fba2",
            "b9085a75972b46c885975523f12c8365",
            "55e677ac79ae4584bd075a1b9a10d342",
            "981dad94e9cc4e20a6dff47cc22a7346",
            "160a740f12f143d1a29aec4dbb6196b9",
            "a5b9510692314346858d2a59d2636694",
            "bf5b10dcec4c475e9455aa6e6fe19eef",
            "20b01b9183d54e288b4c7c5152479730",
            "546edc99466c4592a41ccf129cd1d2eb",
            "c24853989d5d46eca8a115ba09129275",
            "293f5ff4498e4e00a12a06eb40f5fe47",
            "d795a28b1ca5458a9c62afb9b2e93f1d",
            "f767a22290e94c08a31992617af258c1",
            "d920b9419375496ebf12c54f9025d1ca",
            "e79d768fed964348b139fd7759790a8b"
          ]
        },
        "id": "iGqbrDvBiZYY",
        "outputId": "f9a7faca-293b-496c-bcae-1b47f968c2f8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "modules.json:   0%|          | 0.00/255 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f824d00eb0274e29b99616d1866eec73"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/140 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a0400180db5c4415afa51a31eacd0fd6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1cdd64fe5e4c4721a9f7b864eeb359b5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/120 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c93487298a764d9f841ffc9aa84db7e5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a4b7ec057bce41529558411a2f2e3de2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "configuration_hf_nomic_bert.py: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "016f071b914a41c9a9b108f4df63b90e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "A new version of the following files was downloaded from https://huggingface.co/nomic-ai/nomic-bert-2048:\n",
            "- configuration_hf_nomic_bert.py\n",
            ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "modeling_hf_nomic_bert.py: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f071c9dcbfe14a8e8603165e48e0791f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "A new version of the following files was downloaded from https://huggingface.co/nomic-ai/nomic-bert-2048:\n",
            "- modeling_hf_nomic_bert.py\n",
            ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/547M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1e8bbbcec5684c3fb88b46dfee733d54"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:transformers_modules.nomic_hyphen_ai.nomic_hyphen_bert_hyphen_2048.7710840340a098cfb869c4f65e87cf2b1b70caca.modeling_hf_nomic_bert:<All keys matched successfully>\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3c311f0293004dd1a19b6c73b581d68d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5b9bff89d2234c78b4656e31cc6ed354"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a2cd8444035c4eddb710ec217df3b7a4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/695 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ae80e3224eea44c2a3b51f084d42efe8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/286 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "160a740f12f143d1a29aec4dbb6196b9"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the RetrievalQA chain for question-answering tasks\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "# Define a query and retrieve relevant documents\n",
        "query = \"give me summary of this?\"\n",
        "\n",
        "llm = ChatGroq(\n",
        "    model=\"openai/gpt-oss-120b\",\n",
        "    temperature=0.9\n",
        ")\n",
        "\n",
        "# Create a RetrievalQA chain using the language model and the retriever\n",
        "chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)\n",
        "\n",
        "# Invoke the chain with a specific query to get a response\n",
        "reponse = chain.invoke(query)\n",
        "\n",
        "# Print the result of the response\n",
        "print(reponse[\"result\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-iPCSzQjQ4p",
        "outputId": "c23cc4a5-b2a7-4254-8896-1412bebcaaab"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**Summary of the ECAPA‑TDNN speaker‑verification architecture (as described in the excerpt)**  \n",
            "\n",
            "---  \n",
            "\n",
            "### 1. Goal  \n",
            "Build a more powerful TDNN‑based speaker‑embedding extractor (x‑vector) that outperforms previous systems on VoxCeleb and other benchmarks.\n",
            "\n",
            "### 2. Core Architectural Innovations  \n",
            "\n",
            "| Innovation | What it does | Why it helps |\n",
            "|-----------|--------------|--------------|\n",
            "| **1‑D Res2Net blocks (SE‑Res2Blocks)** | Replace the original 1‑D Conv1D layers with a multi‑scale Res2Net module (scale = 8) that expands temporal context using larger kernels and dilation. | Captures fine‑ and coarse‑grained temporal patterns without a huge parameter increase. |\n",
            "| **Squeeze‑Excitation (SE) on each block** | “Squeeze” → compute per‑channel descriptor *z* = mean of frame‑level features across time. “Excitation” → two FC layers (bottleneck) generate channel‑wise weights *s* via sigmoid. | Learns global channel inter‑dependencies, allowing the network to emphasize the most informative channels. |\n",
            "| **Multi‑layer Feature Aggregation (MFA)** | The residual connection of each SE‑Res2Block is the **sum** of the outputs of *all* preceding blocks (instead of only the immediate one). | Provides the block with hierarchical information from shallow to deep layers while keeping the parameter count low. |\n",
            "| **Channel‑wise Self‑Attention in Statistics Pooling** | For each channel *c*, compute a frame‑level score *eₜ,₍c₎* → softmax → attention weight *αₜ,₍c₎*. Weighted mean µ̃₍c₎ and weighted std σ̃₍c₎ are then pooled. | Lets each channel decide which frames are most relevant for its own statistics, improving robustness to noise, reverberation, etc. |\n",
            "| **Attentive Statistics Pooling with Global Context** | Concatenate the local frame vector *hₜ* with the non‑weighted global mean and std before feeding it to the attention mechanism. | Gives the attention module a view of utterance‑level properties (e.g., overall noise level) so it can adapt its focus. |\n",
            "| **Bottleneck & Embedding Layer** | After pooling, two FC layers are applied; the first is a bottleneck that produces the low‑dimensional speaker embedding. | Reduces dimensionality while preserving discriminative information. |\n",
            "\n",
            "### 3. Training & Data  \n",
            "\n",
            "* **Dataset:** VoxCeleb2 development set (≈ 5 k speakers) under the fixed‑condition VoxSRC‑19 protocol.  \n",
            "* **Augmentation:** MUSAN (babble, noise), RIR (reverb), plus speed‑perturbation (tempo up/down) and codec compression (opus/aac) via SoX/FFmpeg.  \n",
            "* **Loss & Optimisation:** Standard cross‑entropy with additive margin softmax (or similar) and back‑propagation; SE blocks help gradient flow and avoid vanishing gradients.  \n",
            "\n",
            "### 4. Ablation Study (Table 2) – Impact of Each Component  \n",
            "\n",
            "| System / Removed Component | EER % | Min‑DCF |\n",
            "|----------------------------|------|---------|\n",
            "| **Full ECAPA‑TDNN (C = 512)** | **1.01** | **0.1274** |\n",
            "| A.1 – Replace attentive stats pooling with plain stats | 1.12 | 0.1316 |\n",
            "| A.2 – Channel attention **without** global context | 1.03 | 0.1288 |\n",
            "| B.1 – **No SE‑block** | 1.27 | 0.1446 |\n",
            "| B.2 – **No Res2Net‑block** | 1.07 | 0.1316 |\n",
            "| C.1 – **No MFA** (no multi‑layer aggregation) | 1.10 | 0.1311 |\n",
            "| C.2 – **No residual connections** | 1.08 | 0.1310 |\n",
            "| C.3 – **No summed residual connections** (use concatenation) | 1.08 | **0.1217** (slightly better DCF) |\n",
            "\n",
            "*Key take‑aways:*  \n",
            "* SE blocks give the biggest gain (removing them hurts EER by ~0.26 %).  \n",
            "* Multi‑scale Res2Net and MFA also contribute noticeable improvements.  \n",
            "* The summed residual scheme is more parameter‑efficient than concatenation, with comparable performance.\n",
            "\n",
            "### 5. Overall Results  \n",
            "\n",
            "* The ECAPA‑TDNN consistently outperforms earlier TDNN‑based x‑vector systems on VoxCeleb test sets.  \n",
            "* By combining multi‑scale temporal modeling (Res2Net), channel‑wise attention, and global‑context‑aware pooling, the network learns embeddings that are more robust to acoustic variability (noise, reverberation, codec artifacts).  \n",
            "\n",
            "---  \n",
            "\n",
            "### TL;DR  \n",
            "\n",
            "The ECAPA‑TDNN improves classic x‑vector speaker verification by (1) replacing ordinary 1‑D convolutions with multi‑scale Res2Net blocks, (2) adding squeeze‑excitation to model channel inter‑dependencies, (3) aggregating features from **all** previous layers via summed residual connections, and (4) using channel‑wise self‑attention (with utterance‑level context) in the statistics‑pooling layer. These changes yield a ~1 % EER and a lower Min‑DCF, showing a clear advantage over prior TDNN‑based models.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Explain the role fo infoedge _data_scientist\"\n",
        "response = chain.invoke(query)\n",
        "print(response[\"result\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FvtEYJ2njrKA",
        "outputId": "a514b71b-1c83-421d-e0cc-bd1f9ce4e52d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**InfoEdge — Data Scientist Role Overview**\n",
            "\n",
            "InfoEdge (the parent company behind platforms such as Naukri.com, 99acres, Jeevansathi, and Shiksha) relies heavily on data‑driven decision‑making to improve its products, attract and retain users, and stay ahead of the competition. A Data Scientist at InfoEdge is a key member of the analytics & machine learning (ML) team whose work spans the entire data pipeline—from raw data collection to delivering actionable insights and production‑grade models. Below is a concise description of the typical responsibilities, skills, and day‑to‑day activities you can expect in this role.\n",
            "\n",
            "---\n",
            "\n",
            "## 1. Core Responsibilities\n",
            "\n",
            "| Area | What the Data Scientist Does |\n",
            "|------|------------------------------|\n",
            "| **Business Problem Framing** | • Work with product managers, engineers, and business stakeholders to translate high‑level goals (e.g., “increase job‑application conversion” or “improve property‑search relevance”) into concrete, measurable analytics problems. <br>• Define success metrics (CTR, conversion rate, churn, etc.) and design experiments. |\n",
            "| **Data Acquisition & Preparation** | • Extract data from diverse sources (web logs, click‑stream, CRM, external APIs, partner data). <br>• Build/maintain ETL pipelines using tools like **Spark, Airflow, AWS Glue, or GCP Dataflow**. <br>• Clean, transform, and feature‑engineer large, often noisy, semi‑structured datasets (e.g., resumes, job postings, real‑estate listings). |\n",
            "| **Exploratory Data Analysis (EDA)** | • Perform statistical analysis and visualisations (Python → pandas, seaborn, plotly; R → ggplot2). <br>• Identify patterns, anomalies, and data quality issues that affect downstream models. |\n",
            "| **Model Development** | • Design, prototype, and validate ML models (ranking, recommendation, classification, time‑series forecasting, NLP). <br>• Use frameworks such as **scikit‑learn, XGBoost, LightGBM, TensorFlow, PyTorch**. <br>• Apply advanced techniques like deep learning for text (BERT, RoBERTa) for resume/job‑description matching, or graph neural networks for network‑based recommendations. |\n",
            "| **Model Evaluation & Experimentation** | • Set up rigorous A/B or multivariate tests, track lift in production metrics. <br>• Use cross‑validation, hold‑out sets, and statistical significance testing. |\n",
            "| **Productionisation & Monitoring** | • Deploy models as APIs or batch jobs (Docker, Kubernetes, AWS SageMaker, GCP AI Platform). <br>• Implement monitoring for data drift, performance decay, and latency. <br>• Build alerting dashboards (Grafana, Looker, Tableau). |\n",
            "| **Insight Communication** | • Translate technical results into clear business recommendations. <br>• Create dashboards, reports, and storytelling visualisations for non‑technical audiences. |\n",
            "| **Research & Innovation** | • Stay up‑to‑date with the latest research (e.g., large language models, self‑supervised learning, causal inference) and evaluate their applicability to InfoEdge products. <br>• Publish internal white‑papers or present at tech talks. |\n",
            "\n",
            "---\n",
            "\n",
            "## 2. Typical Projects at InfoEdge\n",
            "\n",
            "| Project Type | Example Use‑Cases |\n",
            "|--------------|-------------------|\n",
            "| **Job‑Search Ranking** | Build a learning‑to‑rank model that orders job listings based on relevance to a candidate’s profile, using features from resume parsing, past click‑stream, and employer metadata. |\n",
            "| **Resume‑Job Matching** | Deploy NLP pipelines (named‑entity extraction, skill‑taxonomy mapping) and similarity models (Siamese networks, transformer‑based encoders) to suggest best‑fit jobs to job‑seekers and vice‑versa. |\n",
            "| **User‑Retention & Churn Prediction** | Use time‑series and survival analysis to predict which users are likely to drop out, enabling targeted re‑engagement campaigns. |\n",
            "| **Property Price Forecasting** | Combine macro‑economic indicators, historical transaction data, and geo‑spatial features to predict real‑estate price trends on 99acres. |\n",
            "| **Personalised Content Recommendation** | Implement collaborative‑filtering or hybrid recommendation systems for courses on Shiksha or matches on Jeevansathi. |\n",
            "| **A/B Test Optimisation** | Design experiments to evaluate UI changes, new recommendation algorithms, or pricing strategies, ensuring statistically sound conclusions. |\n",
            "| **Fraud Detection** | Build anomaly‑detection models to flag fake job postings, fraudulent user accounts, or suspicious payment activity. |\n",
            "\n",
            "---\n",
            "\n",
            "## 3. Required Skill Set\n",
            "\n",
            "| Category | Key Tools / Techniques |\n",
            "|----------|------------------------|\n",
            "| **Programming** | Python (pandas, NumPy, scikit‑learn, PyTorch/TensorFlow), SQL, Bash, Spark (PySpark/Scala). |\n",
            "| **Data Engineering** | Airflow, Kafka, Hadoop, Snowflake/Redshift/BigQuery, Docker, Kubernetes. |\n",
            "| **Machine Learning** | Supervised & unsupervised learning, deep learning, NLP (transformers, spaCy), recommendation systems, time‑series, causal inference. |\n",
            "| **Statistical Foundations** | Hypothesis testing, experiment design, Bayesian methods, survival analysis. |\n",
            "| **Visualization & Reporting** | Tableau, Looker, PowerBI, matplotlib/seaborn/plotly, Jupyter/Colab notebooks. |\n",
            "| **Software Engineering Practices** | Version control (Git), CI/CD pipelines, code reviews, unit testing, documentation. |\n",
            "| **Domain Knowledge** | Understanding of recruitment, real‑estate, education, or matrimonial markets; familiarity with user behaviour metrics (CTR, CVR, DAU/MAU). |\n",
            "\n",
            "---\n",
            "\n",
            "## 4. Typical Day‑to‑Day Workflow\n",
            "\n",
            "1. **Morning stand‑up** – Sync with product, engineering, and analytics teammates; discuss progress on current experiments, blockers, and upcoming priorities.  \n",
            "2. **Data exploration** – Pull fresh logs or datasets, run quick sanity checks, visualise feature distributions.  \n",
            "3. **Model prototyping** – Experiment with a new algorithm (e.g., a transformer‑based encoder for resume parsing) in a Jupyter notebook; log results with MLflow or Weights & Biases.  \n",
            "4. **Code review / collaboration** – Review peers’ PRs, discuss model architecture decisions, share best practices.  \n",
            "5. **Experiment design** – Draft an A/B test plan, define metrics, write analysis scripts.  \n",
            "6. **Production hand‑off** – Containerise the model, write deployment scripts, add monitoring hooks.  \n",
            "7. **Stakeholder demo** – Present findings to product managers, illustrate impact on key KPIs, gather feedback for next iteration.  \n",
            "8. **Learning** – Read recent papers (e.g., on large‑scale retrieval or self‑supervised learning) and assess whether they can improve existing pipelines.\n",
            "\n",
            "---\n",
            "\n",
            "## 5. Impact on the Business\n",
            "\n",
            "- **Higher relevance → more applications**: Better matching and ranking models directly increase the number of successful job applications, boosting revenue from premium employer services.  \n",
            "- **Improved user experience → retention**: Personalised recommendations keep users engaged longer across all InfoEdge platforms.  \n",
            "- **Cost optimisation**: Accurate demand forecasting for real‑estate listings or education courses helps with inventory planning and ad‑spend allocation.  \n",
            "- **Risk mitigation**: Fraud‑detection models protect the brand and reduce financial losses.  \n",
            "\n",
            "---\n",
            "\n",
            "### Bottom Line\n",
            "\n",
            "A **Data Scientist at InfoEdge** is a hybrid role that blends **data engineering**, **statistical analysis**, **machine‑learning research**, and **product thinking**. The goal is to turn massive, multi‑domain data streams into **actionable intelligence** and **scalable AI solutions** that power the company’s core consumer‑facing products and drive measurable business growth.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MhoCvfqWmlId"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}